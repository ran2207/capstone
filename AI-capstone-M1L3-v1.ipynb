{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6187c689-cf12-4ee8-9937-6c3cf7174a1b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b699ef4-d67e-4a1e-a221-d46e1c55abaf",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Data Loading and Augmentation Using PyTorch</font></h1>\n",
    "\n",
    "<h1 align=left><font size = 5.5>Building efficient data loaders with PyTorch</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7eed9-9cbf-495e-ba94-b0050ce39ecc",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 60 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d69fa-4ad0-4bf7-9a62-9ad1ca7bea09",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "- Load the data using Pytorch data loader\n",
    "- Augment the image dataset using PyTorch\n",
    "- Visualize the augmented data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e31c0-eb69-4916-b60b-c759ce4e0b1f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the PyTorch version of the data loading lab! You have explored the core concept of sequential loading by manually handling file paths and opening images one by one. This memory-efficient method is the foundation for how all modern deep learning frameworks handle large datasets.\n",
    "\n",
    "In this notebook, you will implement the **PyTorch** utilities for the same. You will discover PyTorch's modular approach, which separates *data representation* from *data iteration*:\n",
    "\n",
    "1.  **Custom `Dataset` Class:** You'll build our own dataset class from scratch by inheriting from `torch.utils.data.Dataset`. This gives you full control and a deep understanding of the data pipeline.\n",
    "2.  **In-built `ImageFolder` Utility:** You'll use the convenient `torchvision.datasets.ImageFolder` class, which automatically handles data from a standard directory structure.\n",
    "3.  **The `DataLoader`:** You'll wrap both of our datasets in a `DataLoader`, PyTorch's engine for efficient batching, shuffling, and parallelized data loading.\n",
    "\n",
    "Finally, you'll compare these methods to understand why this modular design is so powerful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a2a2c-297d-4b35-8cb5-e402e15c1ec6",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "<div>\n",
    "<font size = 3>    \n",
    "\n",
    "0.  [Setup](#Setup)\n",
    "1.  [Custom PyTorch `Dataset`](#Custom-PyTorch-Dataset)\n",
    "2.  [In-built `ImageFolder` utility](#In-built-ImageFolder-utility)\n",
    "3.  [The `DataLoader` - Bringing it all together](#The-DataLoader---Bringing-it-all-together)\n",
    "4.  [Comparison and analysis](#Comparison-and-analysis)\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e61ba0-24c8-48f7-93ae-04c2ff093406",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's begin by importing our libraries and defining the data directories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f867db9-3959-4186-a771-a533f9b5e217",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following __cell__ to install them; it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed74d7e-2fe7-4607-9b63-23b847259e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:38.757099Z",
     "iopub.status.busy": "2025-10-27T07:47:38.756197Z",
     "iopub.status.idle": "2025-10-27T07:47:39.320307Z",
     "shell.execute_reply": "2025-10-27T07:47:39.319544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.18 ms, sys: 9.23 ms, total: 16.4 ms\n",
      "Wall time: 554 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26 matplotlib==3.9.2 pandas==2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c8b48-3357-471f-ae4a-018b6ff22aec",
   "metadata": {},
   "source": [
    "Now let's check if the above libraries are installed properly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef91746-4342-49d5-9ed4-a486fbb4a899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:39.341686Z",
     "iopub.status.busy": "2025-10-27T07:47:39.341548Z",
     "iopub.status.idle": "2025-10-27T07:47:39.344296Z",
     "shell.execute_reply": "2025-10-27T07:47:39.343866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Library installation failed!\n",
      "--- Error Details ---\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.26.0 Requires-Python >=3.9,<3.13; 1.26.1 Requires-Python >=3.9,<3.13\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.26 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.26\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where we try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"❌ Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"✅ Library installation was successful, let's proceed ahead\")    \n",
    "\n",
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee2590-b304-4805-974c-47c9875e7069",
   "metadata": {},
   "source": [
    "### `PyTorch (torch)` and `torchvision`library installation\n",
    "\n",
    "Next, install the `PyTorch (torch)` and `torchvision` library using the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e527fb-5afd-46fe-a289-d49a8fc0edcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:39.345568Z",
     "iopub.status.busy": "2025-10-27T07:47:39.345472Z",
     "iopub.status.idle": "2025-10-27T07:47:41.085395Z",
     "shell.execute_reply": "2025-10-27T07:47:41.084572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.8.0+cpu (from versions: 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No matching distribution found for torch==2.8.0+cpu\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 20.9 ms, sys: 22.9 ms, total: 43.7 ms\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26db088-00c6-448a-9b3a-13cdd09e8bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:41.087366Z",
     "iopub.status.busy": "2025-10-27T07:47:41.087196Z",
     "iopub.status.idle": "2025-10-27T07:47:55.117039Z",
     "shell.execute_reply": "2025-10-27T07:47:55.116564Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skillsnetwork\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443ebeb-6968-40c4-91bb-2f66c537fd33",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf604bd1-2120-489f-ac7f-111f9b6b2124",
   "metadata": {},
   "source": [
    "First, set up the data extraction directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a1a181-1adb-4c53-ab84-f00120168a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:55.118543Z",
     "iopub.status.busy": "2025-10-27T07:47:55.118426Z",
     "iopub.status.idle": "2025-10-27T07:47:55.120096Z",
     "shell.execute_reply": "2025-10-27T07:47:55.119723Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abab318-2e93-4d57-b87b-0df45b28e566",
   "metadata": {},
   "source": [
    "### Data acquisition and preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8d781-2352-46ac-b343-238e685cdac2",
   "metadata": {},
   "source": [
    "### Define the dataset URL\n",
    "We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9784b6-3658-4919-9e9e-bd46b2364d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:55.121107Z",
     "iopub.status.busy": "2025-10-27T07:47:55.121045Z",
     "iopub.status.idle": "2025-10-27T07:47:55.122437Z",
     "shell.execute_reply": "2025-10-27T07:47:55.122130Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bc750-d4fa-43aa-a18d-03b06588833b",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255f255b-9789-4367-85c6-8efed8db83f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:55.123456Z",
     "iopub.status.busy": "2025-10-27T07:47:55.123382Z",
     "iopub.status.idle": "2025-10-27T07:47:55.126080Z",
     "shell.execute_reply": "2025-10-27T07:47:55.125798Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"✅ Successfully extracted to '{extract_dir}'.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c485360-4606-4f71-8d93-05599e2d39c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:47:55.127188Z",
     "iopub.status.busy": "2025-10-27T07:47:55.127122Z",
     "iopub.status.idle": "2025-10-27T07:48:01.947109Z",
     "shell.execute_reply": "2025-10-27T07:48:01.946669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e566d87219248f5a2039848d827a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a97a79ddc24a76b2a6a471e8f4e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"❌ Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347fde4-a40a-43d6-a2f6-7b268cf6f762",
   "metadata": {},
   "source": [
    "### Define the directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3c5d0b-7f38-4f1e-85c3-66426e207fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.948258Z",
     "iopub.status.busy": "2025-10-27T07:48:01.948175Z",
     "iopub.status.idle": "2025-10-27T07:48:01.949984Z",
     "shell.execute_reply": "2025-10-27T07:48:01.949615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir = os.path.join(extract_dir, 'images_dataSAT')\n",
    "dir_non_agri = os.path.join(base_dir, 'class_0_non_agri')\n",
    "dir_agri = os.path.join(base_dir, 'class_1_agri')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ef883-0ff7-415f-9d16-f798cf8f6d96",
   "metadata": {},
   "source": [
    "## Custom PyTorch `Dataset`\n",
    "\n",
    "In PyTorch, \n",
    "- A `Dataset` object is an iterable (gives one sample at a time).\n",
    "\n",
    "- A `DataLoader` is an iterator that returns batches (adds a batch dimension: `[B, C, H, W]`).\n",
    "\n",
    "- To create a custom dataset, we will create a class that inherits from `torch.utils.data.Dataset` and implements three essential methods:\n",
    "- `__init__(self)`: This method is run once when the dataset is instantiated. It's where you perform initial setup, such as loading file paths and labels.\n",
    "- `__len__(self)`: This returns the total number of samples in the dataset.\n",
    "- `__getitem__(self, idx)`: This method is responsible for loading and returning a *single* sample from the dataset at the given index `idx`. This is the core of sequential loading; it fetches one data point from storage \"just-in-time\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "523cbaa4-8e7e-49ca-8dde-911b1dc658a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.950893Z",
     "iopub.status.busy": "2025-10-27T07:48:01.950829Z",
     "iopub.status.idle": "2025-10-27T07:48:01.953407Z",
     "shell.execute_reply": "2025-10-27T07:48:01.953010Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomBinaryClassDataset(Dataset):\n",
    "    \"\"\"A custom dataset for our agricultural land classification task.\"\"\"\n",
    "    def __init__(self, non_agri_dir, agri_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            non_agri_dir (string): Directory with all the non-agricultural images.\n",
    "            agri_dir (string): Directory with all the agricultural images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load non-agri paths and assign label 0\n",
    "        for fname in os.listdir(non_agri_dir):\n",
    "            self.image_paths.append(os.path.join(non_agri_dir, fname))\n",
    "            self.labels.append(0)\n",
    "            \n",
    "        # Load agri paths and assign label 1\n",
    "        for fname in os.listdir(agri_dir):\n",
    "            self.image_paths.append(os.path.join(agri_dir, fname))\n",
    "            self.labels.append(1)\n",
    "\n",
    "        temp = list(zip(self.image_paths, self.labels))\n",
    "        np.random.shuffle(temp)\n",
    "        self.image_paths, self.labels = zip(*temp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\") # Ensure image is in RGB format\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformations if they exist\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa0f6a-113c-4c8c-890b-e5064b31b911",
   "metadata": {},
   "source": [
    "### Define transformations\n",
    "\n",
    "- `torchvision.transforms` provides a suite of common image transformations. These are used for preprocessing (such as `resizing` and `converting to a tensor`) and data augmentation (such as `random flips` and `rotations`).\n",
    "- `transforms.Compose` chains these operations together into a single, callable pipeline. Data augmentation is a crucial regularization technique that helps prevent overfitting by showing the model slightly different versions of the same image at each epoch.\n",
    "- Transforms like `RandomFlip` and `Rotation` work on PIL images and must be applied before `ToTensor()`, which converts images to tensors.\n",
    "- `Normalize()` scales tensor values (e.g., from `[0, 1]` to `[-1, 1]`) using `(x - mean) / std`  or can be based on model you are fine tuining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a035de-93bc-468f-baf5-a0f56060bfeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.954443Z",
     "iopub.status.busy": "2025-10-27T07:48:01.954382Z",
     "iopub.status.idle": "2025-10-27T07:48:01.956303Z",
     "shell.execute_reply": "2025-10-27T07:48:01.956016Z"
    }
   },
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomRotation(20),\n",
    "                                     transforms.ToTensor(), # Converts PIL Image to a tensor and scales values to [0, 1]\n",
    "                                     transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e534d-1b32-4521-95c2-d40c752eb450",
   "metadata": {},
   "source": [
    "Now that you know how a data augmentation pipeline works in PyTorch, let’s create a transformation pipeline `custom_transform`for the given specifications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862efc85-dc43-48f4-bc17-8ae5e165ecdd",
   "metadata": {},
   "source": [
    "### **Task 1**: Create a transformation pipeline `custom_transform` for: \n",
    "    1. image size = 64 x 64 pixels\n",
    "    2. RandomHorizontalFlip probability 0.5\n",
    "    3. RandomVerticalFlip probability 0.2\n",
    "    4. RandomRotation of 45 degrees\n",
    "\n",
    "Then, transform it to tensor and normalize it to [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98be49f-c7d6-40de-b637-ab9a58210bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.957451Z",
     "iopub.status.busy": "2025-10-27T07:48:01.957389Z",
     "iopub.status.idle": "2025-10-27T07:48:01.958763Z",
     "shell.execute_reply": "2025-10-27T07:48:01.958383Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478e635-3fbe-4d6a-a185-11bdc1b127aa",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomVerticalFlip(p=0.2),\n",
    "                                       transforms.RandomRotation(45),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "                                      ])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aa410-79e2-42b5-b069-9f7bfa2ff895",
   "metadata": {},
   "source": [
    "## In-built `ImageFolder` utility\n",
    "\n",
    "For datasets that follow a standard `root/class_name/xxx.ext` directory structure, PyTorch provides `torchvision.datasets.ImageFolder`. This is a specialized `Dataset` class that does all the work of finding image paths and inferring class labels for you. It's the most convenient way to load image datasets organized in folders, where each subfolder represents a different class.\n",
    "\n",
    "**ImageFolder** acts as a quick bridge between raw image files and PyTorch models. You can point the `root` argument at the top-level dataset folder, and the class automatically walks through every subfolder, using each subfolder’s name as a label. You can attach a `transform pipeline` with common choices including `transforms.Resize`, `transforms.CenterCrop`, `transforms.ToTensor`, and `transforms.Normalize` to pre-process every image on the fly. Calling `len(dataset)` reveals total images, while indexing `dataset[i]` returns an `(image, label)` pair. You can then combine it with `DataLoader` to batch, shuffle, and parallel-load data effortlessly.\n",
    "\n",
    "**ImageFolder** works on the `dataset folder` structured as represented below:\n",
    "\n",
    "    dataset/\n",
    "    ├── class1/\n",
    "    │   ├── image1.jpg\n",
    "    │   └── image2.jpg\n",
    "    └── class2/\n",
    "        ├── image3.jpg\n",
    "        └── image4.jpg\n",
    "    \n",
    "\n",
    "From this dataset, you can create a `dataset object` like this: \n",
    "\n",
    "```\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root='path/to/dataset',\n",
    "    transform=transform\n",
    "\n",
    "```\n",
    "\n",
    "Then, you can use this `dataset object` to create a `dataloader` like this:\n",
    "```\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    # Your training code here\n",
    "    .\n",
    "    .\n",
    "    ...\n",
    "\n",
    "```\n",
    "This `dataloader` can then be used for creating batches of images for training / testing of the model.\n",
    "\n",
    "**`ImageFolder`** automatically assigns numerical labels to classes based on folder names alphabetically. Access class names via `dataset.classes` and class-to-index mapping via `dataset.class_to_idx`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bbe13-ac9f-43e3-a0f6-56a2074dd528",
   "metadata": {},
   "source": [
    "Your task is to get the dataset and apply custom_transform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b9cd1-408d-46a9-bb9e-66a698359014",
   "metadata": {},
   "source": [
    "### **Task 2**: Get the dataset **imagefolder_dataset** using `datasets.Imagefolder` method and applying **custom_transform**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db0dad9-a5b8-4bdd-99f7-5b61715cafe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.959736Z",
     "iopub.status.busy": "2025-10-27T07:48:01.959678Z",
     "iopub.status.idle": "2025-10-27T07:48:01.961055Z",
     "shell.execute_reply": "2025-10-27T07:48:01.960697Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b41bab-43f9-4f81-916b-e31ad84082e3",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "imagefolder_dataset = datasets.ImageFolder(root=base_dir, transform=custom_transform)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29e55d-1c4e-4e96-8dad-bd57a41f9a2d",
   "metadata": {},
   "source": [
    "Next, print the name and the class index. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505eaf75-e1a1-48cd-8b53-8a2a8fa5bd6e",
   "metadata": {},
   "source": [
    "### **Task 3**: Print the name and the class index from the **imagefolder_dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e4df0a-2b0f-4b42-9e93-d6a6608eab36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.962099Z",
     "iopub.status.busy": "2025-10-27T07:48:01.962027Z",
     "iopub.status.idle": "2025-10-27T07:48:01.963402Z",
     "shell.execute_reply": "2025-10-27T07:48:01.963062Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c42ba2-5426-4bb7-b602-b8a230599fd7",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "print(f\"Classes found by ImageFolder: {imagefolder_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {imagefolder_dataset.class_to_idx}\")\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ec27b-9df5-43f6-971b-c606b70cdad2",
   "metadata": {},
   "source": [
    "## The `DataLoader` - Bringing it all together\n",
    "\n",
    "The `DataLoader` is an iterator that returns batches, based on the `batch_size`. It wraps a `Dataset` object and provides an iterable to access the data. Its key responsibilities are:\n",
    "- **Batching:** This involves grouping individual samples into mini-batches.\n",
    "- **Shuffling:** This involves randomly shuffling the data every epoch to prevent the model from learning the order of samples. The data loading process is parallelized, preventing the CPU from becoming a bottleneck for the GPU.\n",
    "\n",
    "We can use the `DataLoader` with **both** our `CustomBinaryClassDataset` and the `ImageFolder` dataset.\n",
    "\n",
    "Linear and convolution layers in a neural network can work **with or without** the batch dimension \n",
    "- e.g., with shapes `[C, H, W]` or  `[B, C, H, W]`, where:\n",
    "    - `B`: `batch_size`\n",
    "    - `C`: `number of channels`\n",
    "    - `H`: `image height`\n",
    "    - `W`: `image width`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6106cec5-2594-4ca6-8435-7fa2010ec80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:01.964306Z",
     "iopub.status.busy": "2025-10-27T07:48:01.964246Z",
     "iopub.status.idle": "2025-10-27T07:48:02.023328Z",
     "shell.execute_reply": "2025-10-27T07:48:02.022934Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m BATCH_SIZE = \u001b[32m8\u001b[39m \u001b[38;5;66;03m#  the number of images in a batch (batch size)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --- Using your Custom Dataset ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m custom_dataset = CustomBinaryClassDataset(dir_non_agri, dir_agri, transform=\u001b[43mcustom_transform\u001b[49m)\n\u001b[32m      5\u001b[39m custom_loader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m2\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- Using the ImageFolder Dataset ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'custom_transform' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8 #  the number of images in a batch (batch size)\n",
    "\n",
    "# --- Using your Custom Dataset ---\n",
    "custom_dataset = CustomBinaryClassDataset(dir_non_agri, dir_agri, transform=custom_transform)\n",
    "custom_loader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# --- Using the ImageFolder Dataset ---\n",
    "imagefolder_loader = DataLoader(imagefolder_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4da1c8-ad7f-4891-95ff-30d23ecfb4f2",
   "metadata": {},
   "source": [
    "### Visualize a batch\n",
    "\n",
    "Let's inspect a batch from our `DataLoader`. You can use `iter()` and `next()` to pull a single batch. This is exactly what a training loop does under the hood. Visualizing the images and their labels confirms that our entire pipeline (from path loading and transformation to batching) is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a2f50e3-4128-4f0b-8fac-ae2c9e8caadd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.024371Z",
     "iopub.status.busy": "2025-10-27T07:48:02.024311Z",
     "iopub.status.idle": "2025-10-27T07:48:02.033277Z",
     "shell.execute_reply": "2025-10-27T07:48:02.032914Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get one batch from the Custom loader\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m images_custom, labels_custom = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mcustom_loader\u001b[49m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImages batch shape (Custom loader): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_custom.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabels batch shape (Custom loader): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_custom.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'custom_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Get one batch from the Custom loader\n",
    "images_custom, labels_custom = next(iter(custom_loader))\n",
    "\n",
    "print(f\"Images batch shape (Custom loader): {images_custom.shape}\") \n",
    "print(f\"Labels batch shape (Custom loader): {labels_custom.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed2a-d20c-40a0-b11a-5dc9c99d6bdd",
   "metadata": {},
   "source": [
    "Your next task is to get the images and labels from the loader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b2d27-5512-4e12-8020-1109c30f8dd7",
   "metadata": {},
   "source": [
    "### **Task 4**: Get a batch of images and labels from the `imagefolder_loader` and print their shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e24487-620e-400f-b123-b03502f598a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.034472Z",
     "iopub.status.busy": "2025-10-27T07:48:02.034408Z",
     "iopub.status.idle": "2025-10-27T07:48:02.035818Z",
     "shell.execute_reply": "2025-10-27T07:48:02.035456Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553149d-ce38-4bcd-b7d8-757d32b34fd5",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "images_inbuilt, labels_inbuilt = next(iter(imagefolder_loader))\n",
    "\n",
    "print(f\"Images batch shape (PyTorch in-built loader): {images_inbuilt.shape}\") \n",
    "print(f\"Labels batch shape (PyTorch in-built loader): {labels_inbuilt.shape}\")\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92ba82-6aea-4291-a023-0d1cdc52b9cc",
   "metadata": {},
   "source": [
    "Next, define a function to display an image from the batch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1221144c-ec9b-475c-94f0-5f602a429075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.036782Z",
     "iopub.status.busy": "2025-10-27T07:48:02.036702Z",
     "iopub.status.idle": "2025-10-27T07:48:02.038358Z",
     "shell.execute_reply": "2025-10-27T07:48:02.038022Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"Helper function to un-normalize and display an image\"\"\"\n",
    "    img = img / 2 + 0.5  # Un-normalize from [-1, 1] to [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from C,H,W to H,W,C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22abb19-59ed-4d69-a8c5-4352d3ca25a5",
   "metadata": {},
   "source": [
    "Then, display the images in the PyTorch in-built loader batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ccee5a9-cf29-4838-b08e-773f536cc6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.039322Z",
     "iopub.status.busy": "2025-10-27T07:48:02.039246Z",
     "iopub.status.idle": "2025-10-27T07:48:02.079609Z",
     "shell.execute_reply": "2025-10-27T07:48:02.079211Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_inbuilt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE):\n\u001b[32m      3\u001b[39m     ax = plt.subplot(\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, i + \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     imshow(\u001b[43mimages_inbuilt\u001b[49m[i])\n\u001b[32m      5\u001b[39m     plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch_loader Label:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_inbuilt[i].item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     plt.axis(\u001b[33m\"\u001b[39m\u001b[33moff\u001b[39m\u001b[33m\"\u001b[39m)    \n",
      "\u001b[31mNameError\u001b[39m: name 'images_inbuilt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "for i in range(BATCH_SIZE):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    imshow(images_inbuilt[i])\n",
    "    plt.title(f\"PyTorch_loader Label:{labels_inbuilt[i].item()}\")\n",
    "    plt.axis(\"off\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2694ca-4eeb-4a4b-b5e2-3d2ea58ad238",
   "metadata": {},
   "source": [
    "### **Task 5**: Display the images in the Custom loader batch\n",
    "Similar to the code cell above, display the images stored in `images_custom` generated using `custom_loader`. \n",
    "\n",
    "The title of the images should be **`Custom_loader Label: `** similar to the images seen in the above cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d6ad49-8d5b-4ba5-9a4f-f5497d5f1829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.080725Z",
     "iopub.status.busy": "2025-10-27T07:48:02.080655Z",
     "iopub.status.idle": "2025-10-27T07:48:02.082050Z",
     "shell.execute_reply": "2025-10-27T07:48:02.081735Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8b3f2-5809-4d70-b276-638896d30f87",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(BATCH_SIZE):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    imshow(images_custom[i])\n",
    "    plt.title(f\"Custom_loader Label:{labels_custom[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc8f3f-dd89-43b9-9074-266109e82a46",
   "metadata": {},
   "source": [
    "## Comparison and analysis\n",
    "\n",
    "In PyTorch, both methods lead to a `DataLoader`. The key difference lies in how the underlying `Dataset` is created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df05a9-a89f-4fea-bdc4-ca24441b0890",
   "metadata": {},
   "source": [
    "| Feature                  | Custom `Dataset`                                       | In-built `ImageFolder`     |\n",
    "|--------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Ease of use**          | Low. It requires writing a custom class with three specific methods. | High. It requires only a single, intuitive class instantiation. |\n",
    "| **Code complexity**      | High. You manually manage file paths, labels, and the loading logic inside `__getitem__`. | Low. The class handles all path and label discovery automatically.          |\n",
    "| **Flexibility**          | Very high. It is essential for non-standard directory structures, loading labels from a CSV/JSON, or complex loading logic (for example, from a database). | Moderate. It strictly requires the `root/class/image.ext` directory structure. |\n",
    "| **Performance**          | Potentially high. Performance depends on the efficiency of your `__getitem__` method. When wrapped in a `DataLoader` with `num_workers`, it can be highly performant. | High. `ImageFolder` is optimized and, when wrapped in a `DataLoader`, provides excellent performance out-of-the-box. |\n",
    "| **Recommended for**      | It is recommended for complex, non-standard datasets or when you need fine-grained control over every aspect of data loading. | It is recommended for almost all standard image classification tasks.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b487ff3-a058-4518-b0c2-58ba70acc072",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99094c4a-9340-493d-9f61-4e773fc7a6fe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab has demonstrated the power and elegance of PyTorch's data loading API. The key takeaway is the **separation of concerns**:\n",
    "- The `Dataset` class defines *what* the data is and *how to get a single item*.\n",
    "- The `DataLoader` class defines *how to iterate over* the data, handling batching, shuffling, and performance optimization.\n",
    "\n",
    "While building a custom `Dataset` is an excellent way to understand the underlying mechanics and is necessary for complex scenarios, `ImageFolder` provides a simple and robust solution for the vast majority of image classification problems. In either case, the `DataLoader` is the essential tool that makes the entire pipeline efficient and ready for training a deep learning model.\n",
    "\n",
    "You are now equipped with standard, production-ready methods for loading image data in PyTorch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cffe6c-d616-429c-8ef2-a90009e571ef",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6146f1-68c4-43a2-810e-3ffc1dd9e2fb",
   "metadata": {},
   "source": [
    "\n",
    "<!--\n",
    "## Change Log\n",
    "\n",
    "'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "```\n",
    "```|---|---|---|---|\n",
    "```\n",
    "```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n",
    "```\n",
    "```| 2025-06-28  | 2.0  | Sangeeta |  ID review |\n",
    "```\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298845d-a658-4e9b-96e8-e0857fd32c56",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.083338Z",
     "iopub.status.busy": "2025-10-27T07:48:02.083273Z",
     "iopub.status.idle": "2025-10-27T07:48:02.086508Z",
     "shell.execute_reply": "2025-10-27T07:48:02.086094Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1764543401.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 1 Lab 3: Data Loading and Augmentation Using PyTorchSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:AI-capstone-M1L3-v1.ipynb\"\"\"import osimport torchfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# TASK 1: Define transformation pipeline custom_transform# =============================================================================custom_transform = transforms.Compose([    transforms.Resize((64, 64)),                    # Resize to 64x64 pixels    transforms.RandomHorizontalFlip(p=0.5),         # Random horizontal flip with 50% probability    transforms.RandomVerticalFlip(p=0.2),           # Random vertical flip with 20% probability    transforms.RandomRotation(45),                  # Random rotation up to 45 degrees    transforms.ToTensor(),                          # Convert PIL image to tensor    transforms.Normalize(                           # Normalize using ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"Custom transform pipeline created with:\")print(\"  - Resize to 64x64\")print(\"  - RandomHorizontalFlip (p=0.5)\")print(\"  - RandomVerticalFlip (p=0.2)\")print(\"  - RandomRotation (45 degrees)\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet stats)\")# =============================================================================# TASK 2: Load dataset using datasets.ImageFolder with custom_transform# =============================================================================# Define dataset pathdataset_path = './images_dataSAT'# Load dataset using ImageFolderimagefolder_dataset = datasets.ImageFolder(    root=dataset_path,    transform=custom_transform)print(f\"\\nDataset loaded from: {dataset_path}\")print(f\"Total images in dataset: {len(imagefolder_dataset)}\")# =============================================================================# TASK 3: Print class names and indices from imagefolder_dataset# =============================================================================# Get class names and their corresponding indicesclass_names = imagefolder_dataset.classesclass_to_idx = imagefolder_dataset.class_to_idxprint(\"\\nClass names:\")for idx, class_name in enumerate(class_names):    print(f\"  {idx}: {class_name}\")print(\"\\nClass to index mapping:\")for class_name, idx in class_to_idx.items():    print(f\"  '{class_name}' -> {idx}\")# =============================================================================# TASK 4: Retrieve and display image shapes from a batch in imagefolder_loader# =============================================================================# Create DataLoader with batch size 8imagefolder_loader = DataLoader(    imagefolder_dataset,    batch_size=8,    shuffle=True,    num_workers=0  # Set to 0 for compatibility, increase for performance)# Get one batchbatch_images, batch_labels = next(iter(imagefolder_loader))print(f\"\\nBatch information:\")print(f\"  Batch images shape: {batch_images.shape}\")print(f\"  Batch labels shape: {batch_labels.shape}\")print(f\"  Batch labels: {batch_labels.tolist()}\")# Print individual image shapesprint(f\"\\nIndividual image shapes:\")for idx in range(batch_images.shape[0]):    print(f\"  Image {idx + 1}: {batch_images[idx].shape}\")# =============================================================================# TASK 5: Display images in the custom loader batch# =============================================================================def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):    \"\"\"    Denormalize tensor for visualization    \"\"\"    tensor = tensor.clone()    for t, m, s in zip(tensor, mean, std):        t.mul_(s).add_(m)    return tensordef imshow(img, title=None):    \"\"\"    Display tensor as image    \"\"\"    img = denormalize(img)    img = img.numpy().transpose((1, 2, 0))    img = np.clip(img, 0, 1)    plt.imshow(img)    if title:        plt.title(title)    plt.axis('off')# Display the batchprint(\"\\nDisplaying batch of images...\")fig, axes = plt.subplots(2, 4, figsize=(16, 8))axes = axes.flatten()for idx in range(8):    img = batch_images[idx]    label = batch_labels[idx].item()    class_name = class_names[label]    plt.subplot(2, 4, idx + 1)    imshow(img, title=f\"{class_name} (Label: {label})\")plt.suptitle(\"Batch from PyTorch DataLoader with Custom Transforms\")plt.tight_layout()plt.show()# =============================================================================# ADDITIONAL: Create separate train/val dataloaders (common pattern)# =============================================================================# Define transforms without augmentation for validationval_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=custom_transform)# Split into train and validation (80/20)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = torch.utils.data.random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42)  # For reproducibility)# Update validation dataset transformval_dataset.dataset.transform = val_transform# Create DataLoaderstrain_loader = DataLoader(    train_dataset,    batch_size=32,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=32,    shuffle=False,    num_workers=0)print(f\"\\nTrain/Val split:\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")# =============================================================================# BONUS: Show effect of augmentation# =============================================================================# Get a single image from datasetimg, label = imagefolder_dataset[0]# Create transform without augmentation for comparisonno_aug_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load same image without augmentationdataset_no_aug = datasets.ImageFolder(    root=dataset_path,    transform=no_aug_transform)# Show original vs augmentedfig, axes = plt.subplots(1, 2, figsize=(12, 6))# Original (no augmentation)img_orig, _ = dataset_no_aug[0]axes[0].imshow(denormalize(img_orig).numpy().transpose(1, 2, 0))axes[0].set_title(\"Original (No Augmentation)\")axes[0].axis('off')# With augmentationaxes[1].imshow(denormalize(img).numpy().transpose(1, 2, 0))axes[1].set_title(\"With Random Augmentation\")axes[1].axis('off')plt.suptitle(\"Effect of Data Augmentation\")plt.tight_layout()plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*60)print(\"SUMMARY: Module 1 Lab 3 - All Tasks Completed\")print(\"=\"*60)print(f\"Task 1: Created custom_transform with augmentation\")print(f\"Task 2: Loaded dataset with {len(imagefolder_dataset)} images\")print(f\"Task 3: Class names: {class_names}\")print(f\"        Class indices: {class_to_idx}\")print(f\"Task 4: Batch shape: {batch_images.shape}\")print(f\"Task 5: Displayed {batch_images.shape[0]} images from batch\")print(\"=\"*60)\u001b[39m\n                                                                                                                                                                                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 1 Lab 3: Data Loading and Augmentation Using PyTorchSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:AI-capstone-M1L3-v1.ipynb\"\"\"import osimport torchfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# TASK 1: Define transformation pipeline custom_transform# =============================================================================custom_transform = transforms.Compose([    transforms.Resize((64, 64)),                    # Resize to 64x64 pixels    transforms.RandomHorizontalFlip(p=0.5),         # Random horizontal flip with 50% probability    transforms.RandomVerticalFlip(p=0.2),           # Random vertical flip with 20% probability    transforms.RandomRotation(45),                  # Random rotation up to 45 degrees    transforms.ToTensor(),                          # Convert PIL image to tensor    transforms.Normalize(                           # Normalize using ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"Custom transform pipeline created with:\")print(\"  - Resize to 64x64\")print(\"  - RandomHorizontalFlip (p=0.5)\")print(\"  - RandomVerticalFlip (p=0.2)\")print(\"  - RandomRotation (45 degrees)\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet stats)\")# =============================================================================# TASK 2: Load dataset using datasets.ImageFolder with custom_transform# =============================================================================# Define dataset pathdataset_path = './images_dataSAT'# Load dataset using ImageFolderimagefolder_dataset = datasets.ImageFolder(    root=dataset_path,    transform=custom_transform)print(f\"\\nDataset loaded from: {dataset_path}\")print(f\"Total images in dataset: {len(imagefolder_dataset)}\")# =============================================================================# TASK 3: Print class names and indices from imagefolder_dataset# =============================================================================# Get class names and their corresponding indicesclass_names = imagefolder_dataset.classesclass_to_idx = imagefolder_dataset.class_to_idxprint(\"\\nClass names:\")for idx, class_name in enumerate(class_names):    print(f\"  {idx}: {class_name}\")print(\"\\nClass to index mapping:\")for class_name, idx in class_to_idx.items():    print(f\"  '{class_name}' -> {idx}\")# =============================================================================# TASK 4: Retrieve and display image shapes from a batch in imagefolder_loader# =============================================================================# Create DataLoader with batch size 8imagefolder_loader = DataLoader(    imagefolder_dataset,    batch_size=8,    shuffle=True,    num_workers=0  # Set to 0 for compatibility, increase for performance)# Get one batchbatch_images, batch_labels = next(iter(imagefolder_loader))print(f\"\\nBatch information:\")print(f\"  Batch images shape: {batch_images.shape}\")print(f\"  Batch labels shape: {batch_labels.shape}\")print(f\"  Batch labels: {batch_labels.tolist()}\")# Print individual image shapesprint(f\"\\nIndividual image shapes:\")for idx in range(batch_images.shape[0]):    print(f\"  Image {idx + 1}: {batch_images[idx].shape}\")# =============================================================================# TASK 5: Display images in the custom loader batch# =============================================================================def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):    \"\"\"    Denormalize tensor for visualization    \"\"\"    tensor = tensor.clone()    for t, m, s in zip(tensor, mean, std):        t.mul_(s).add_(m)    return tensordef imshow(img, title=None):    \"\"\"    Display tensor as image    \"\"\"    img = denormalize(img)    img = img.numpy().transpose((1, 2, 0))    img = np.clip(img, 0, 1)    plt.imshow(img)    if title:        plt.title(title)    plt.axis('off')# Display the batchprint(\"\\nDisplaying batch of images...\")fig, axes = plt.subplots(2, 4, figsize=(16, 8))axes = axes.flatten()for idx in range(8):    img = batch_images[idx]    label = batch_labels[idx].item()    class_name = class_names[label]    plt.subplot(2, 4, idx + 1)    imshow(img, title=f\"{class_name} (Label: {label})\")plt.suptitle(\"Batch from PyTorch DataLoader with Custom Transforms\")plt.tight_layout()plt.show()# =============================================================================# ADDITIONAL: Create separate train/val dataloaders (common pattern)# =============================================================================# Define transforms without augmentation for validationval_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=custom_transform)# Split into train and validation (80/20)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = torch.utils.data.random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42)  # For reproducibility)# Update validation dataset transformval_dataset.dataset.transform = val_transform# Create DataLoaderstrain_loader = DataLoader(    train_dataset,    batch_size=32,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=32,    shuffle=False,    num_workers=0)print(f\"\\nTrain/Val split:\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")# =============================================================================# BONUS: Show effect of augmentation# =============================================================================# Get a single image from datasetimg, label = imagefolder_dataset[0]# Create transform without augmentation for comparisonno_aug_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load same image without augmentationdataset_no_aug = datasets.ImageFolder(    root=dataset_path,    transform=no_aug_transform)# Show original vs augmentedfig, axes = plt.subplots(1, 2, figsize=(12, 6))# Original (no augmentation)img_orig, _ = dataset_no_aug[0]axes[0].imshow(denormalize(img_orig).numpy().transpose(1, 2, 0))axes[0].set_title(\"Original (No Augmentation)\")axes[0].axis('off')# With augmentationaxes[1].imshow(denormalize(img).numpy().transpose(1, 2, 0))axes[1].set_title(\"With Random Augmentation\")axes[1].axis('off')plt.suptitle(\"Effect of Data Augmentation\")plt.tight_layout()plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*60)print(\"SUMMARY: Module 1 Lab 3 - All Tasks Completed\")print(\"=\"*60)print(f\"Task 1: Created custom_transform with augmentation\")print(f\"Task 2: Loaded dataset with {len(imagefolder_dataset)} images\")print(f\"Task 3: Class names: {class_names}\")print(f\"        Class indices: {class_to_idx}\")print(f\"Task 4: Batch shape: {batch_images.shape}\")print(f\"Task 5: Displayed {batch_images.shape[0]} images from batch\")print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:02.087585Z",
     "iopub.status.busy": "2025-10-27T07:48:02.087523Z",
     "iopub.status.idle": "2025-10-27T07:48:02.090535Z",
     "shell.execute_reply": "2025-10-27T07:48:02.090208Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1764543401.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 1 Lab 3: Data Loading and Augmentation Using PyTorchSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:AI-capstone-M1L3-v1.ipynb\"\"\"import osimport torchfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# TASK 1: Define transformation pipeline custom_transform# =============================================================================custom_transform = transforms.Compose([    transforms.Resize((64, 64)),                    # Resize to 64x64 pixels    transforms.RandomHorizontalFlip(p=0.5),         # Random horizontal flip with 50% probability    transforms.RandomVerticalFlip(p=0.2),           # Random vertical flip with 20% probability    transforms.RandomRotation(45),                  # Random rotation up to 45 degrees    transforms.ToTensor(),                          # Convert PIL image to tensor    transforms.Normalize(                           # Normalize using ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"Custom transform pipeline created with:\")print(\"  - Resize to 64x64\")print(\"  - RandomHorizontalFlip (p=0.5)\")print(\"  - RandomVerticalFlip (p=0.2)\")print(\"  - RandomRotation (45 degrees)\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet stats)\")# =============================================================================# TASK 2: Load dataset using datasets.ImageFolder with custom_transform# =============================================================================# Define dataset pathdataset_path = './images_dataSAT'# Load dataset using ImageFolderimagefolder_dataset = datasets.ImageFolder(    root=dataset_path,    transform=custom_transform)print(f\"\\nDataset loaded from: {dataset_path}\")print(f\"Total images in dataset: {len(imagefolder_dataset)}\")# =============================================================================# TASK 3: Print class names and indices from imagefolder_dataset# =============================================================================# Get class names and their corresponding indicesclass_names = imagefolder_dataset.classesclass_to_idx = imagefolder_dataset.class_to_idxprint(\"\\nClass names:\")for idx, class_name in enumerate(class_names):    print(f\"  {idx}: {class_name}\")print(\"\\nClass to index mapping:\")for class_name, idx in class_to_idx.items():    print(f\"  '{class_name}' -> {idx}\")# =============================================================================# TASK 4: Retrieve and display image shapes from a batch in imagefolder_loader# =============================================================================# Create DataLoader with batch size 8imagefolder_loader = DataLoader(    imagefolder_dataset,    batch_size=8,    shuffle=True,    num_workers=0  # Set to 0 for compatibility, increase for performance)# Get one batchbatch_images, batch_labels = next(iter(imagefolder_loader))print(f\"\\nBatch information:\")print(f\"  Batch images shape: {batch_images.shape}\")print(f\"  Batch labels shape: {batch_labels.shape}\")print(f\"  Batch labels: {batch_labels.tolist()}\")# Print individual image shapesprint(f\"\\nIndividual image shapes:\")for idx in range(batch_images.shape[0]):    print(f\"  Image {idx + 1}: {batch_images[idx].shape}\")# =============================================================================# TASK 5: Display images in the custom loader batch# =============================================================================def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):    \"\"\"    Denormalize tensor for visualization    \"\"\"    tensor = tensor.clone()    for t, m, s in zip(tensor, mean, std):        t.mul_(s).add_(m)    return tensordef imshow(img, title=None):    \"\"\"    Display tensor as image    \"\"\"    img = denormalize(img)    img = img.numpy().transpose((1, 2, 0))    img = np.clip(img, 0, 1)    plt.imshow(img)    if title:        plt.title(title)    plt.axis('off')# Display the batchprint(\"\\nDisplaying batch of images...\")fig, axes = plt.subplots(2, 4, figsize=(16, 8))axes = axes.flatten()for idx in range(8):    img = batch_images[idx]    label = batch_labels[idx].item()    class_name = class_names[label]    plt.subplot(2, 4, idx + 1)    imshow(img, title=f\"{class_name} (Label: {label})\")plt.suptitle(\"Batch from PyTorch DataLoader with Custom Transforms\")plt.tight_layout()plt.show()# =============================================================================# ADDITIONAL: Create separate train/val dataloaders (common pattern)# =============================================================================# Define transforms without augmentation for validationval_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=custom_transform)# Split into train and validation (80/20)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = torch.utils.data.random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42)  # For reproducibility)# Update validation dataset transformval_dataset.dataset.transform = val_transform# Create DataLoaderstrain_loader = DataLoader(    train_dataset,    batch_size=32,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=32,    shuffle=False,    num_workers=0)print(f\"\\nTrain/Val split:\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")# =============================================================================# BONUS: Show effect of augmentation# =============================================================================# Get a single image from datasetimg, label = imagefolder_dataset[0]# Create transform without augmentation for comparisonno_aug_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load same image without augmentationdataset_no_aug = datasets.ImageFolder(    root=dataset_path,    transform=no_aug_transform)# Show original vs augmentedfig, axes = plt.subplots(1, 2, figsize=(12, 6))# Original (no augmentation)img_orig, _ = dataset_no_aug[0]axes[0].imshow(denormalize(img_orig).numpy().transpose(1, 2, 0))axes[0].set_title(\"Original (No Augmentation)\")axes[0].axis('off')# With augmentationaxes[1].imshow(denormalize(img).numpy().transpose(1, 2, 0))axes[1].set_title(\"With Random Augmentation\")axes[1].axis('off')plt.suptitle(\"Effect of Data Augmentation\")plt.tight_layout()plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*60)print(\"SUMMARY: Module 1 Lab 3 - All Tasks Completed\")print(\"=\"*60)print(f\"Task 1: Created custom_transform with augmentation\")print(f\"Task 2: Loaded dataset with {len(imagefolder_dataset)} images\")print(f\"Task 3: Class names: {class_names}\")print(f\"        Class indices: {class_to_idx}\")print(f\"Task 4: Batch shape: {batch_images.shape}\")print(f\"Task 5: Displayed {batch_images.shape[0]} images from batch\")print(\"=\"*60)\u001b[39m\n                                                                                                                                                                                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 1 Lab 3: Data Loading and Augmentation Using PyTorchSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:AI-capstone-M1L3-v1.ipynb\"\"\"import osimport torchfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# TASK 1: Define transformation pipeline custom_transform# =============================================================================custom_transform = transforms.Compose([    transforms.Resize((64, 64)),                    # Resize to 64x64 pixels    transforms.RandomHorizontalFlip(p=0.5),         # Random horizontal flip with 50% probability    transforms.RandomVerticalFlip(p=0.2),           # Random vertical flip with 20% probability    transforms.RandomRotation(45),                  # Random rotation up to 45 degrees    transforms.ToTensor(),                          # Convert PIL image to tensor    transforms.Normalize(                           # Normalize using ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"Custom transform pipeline created with:\")print(\"  - Resize to 64x64\")print(\"  - RandomHorizontalFlip (p=0.5)\")print(\"  - RandomVerticalFlip (p=0.2)\")print(\"  - RandomRotation (45 degrees)\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet stats)\")# =============================================================================# TASK 2: Load dataset using datasets.ImageFolder with custom_transform# =============================================================================# Define dataset pathdataset_path = './images_dataSAT'# Load dataset using ImageFolderimagefolder_dataset = datasets.ImageFolder(    root=dataset_path,    transform=custom_transform)print(f\"\\nDataset loaded from: {dataset_path}\")print(f\"Total images in dataset: {len(imagefolder_dataset)}\")# =============================================================================# TASK 3: Print class names and indices from imagefolder_dataset# =============================================================================# Get class names and their corresponding indicesclass_names = imagefolder_dataset.classesclass_to_idx = imagefolder_dataset.class_to_idxprint(\"\\nClass names:\")for idx, class_name in enumerate(class_names):    print(f\"  {idx}: {class_name}\")print(\"\\nClass to index mapping:\")for class_name, idx in class_to_idx.items():    print(f\"  '{class_name}' -> {idx}\")# =============================================================================# TASK 4: Retrieve and display image shapes from a batch in imagefolder_loader# =============================================================================# Create DataLoader with batch size 8imagefolder_loader = DataLoader(    imagefolder_dataset,    batch_size=8,    shuffle=True,    num_workers=0  # Set to 0 for compatibility, increase for performance)# Get one batchbatch_images, batch_labels = next(iter(imagefolder_loader))print(f\"\\nBatch information:\")print(f\"  Batch images shape: {batch_images.shape}\")print(f\"  Batch labels shape: {batch_labels.shape}\")print(f\"  Batch labels: {batch_labels.tolist()}\")# Print individual image shapesprint(f\"\\nIndividual image shapes:\")for idx in range(batch_images.shape[0]):    print(f\"  Image {idx + 1}: {batch_images[idx].shape}\")# =============================================================================# TASK 5: Display images in the custom loader batch# =============================================================================def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):    \"\"\"    Denormalize tensor for visualization    \"\"\"    tensor = tensor.clone()    for t, m, s in zip(tensor, mean, std):        t.mul_(s).add_(m)    return tensordef imshow(img, title=None):    \"\"\"    Display tensor as image    \"\"\"    img = denormalize(img)    img = img.numpy().transpose((1, 2, 0))    img = np.clip(img, 0, 1)    plt.imshow(img)    if title:        plt.title(title)    plt.axis('off')# Display the batchprint(\"\\nDisplaying batch of images...\")fig, axes = plt.subplots(2, 4, figsize=(16, 8))axes = axes.flatten()for idx in range(8):    img = batch_images[idx]    label = batch_labels[idx].item()    class_name = class_names[label]    plt.subplot(2, 4, idx + 1)    imshow(img, title=f\"{class_name} (Label: {label})\")plt.suptitle(\"Batch from PyTorch DataLoader with Custom Transforms\")plt.tight_layout()plt.show()# =============================================================================# ADDITIONAL: Create separate train/val dataloaders (common pattern)# =============================================================================# Define transforms without augmentation for validationval_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=custom_transform)# Split into train and validation (80/20)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = torch.utils.data.random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42)  # For reproducibility)# Update validation dataset transformval_dataset.dataset.transform = val_transform# Create DataLoaderstrain_loader = DataLoader(    train_dataset,    batch_size=32,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=32,    shuffle=False,    num_workers=0)print(f\"\\nTrain/Val split:\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")# =============================================================================# BONUS: Show effect of augmentation# =============================================================================# Get a single image from datasetimg, label = imagefolder_dataset[0]# Create transform without augmentation for comparisonno_aug_transform = transforms.Compose([    transforms.Resize((64, 64)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])# Load same image without augmentationdataset_no_aug = datasets.ImageFolder(    root=dataset_path,    transform=no_aug_transform)# Show original vs augmentedfig, axes = plt.subplots(1, 2, figsize=(12, 6))# Original (no augmentation)img_orig, _ = dataset_no_aug[0]axes[0].imshow(denormalize(img_orig).numpy().transpose(1, 2, 0))axes[0].set_title(\"Original (No Augmentation)\")axes[0].axis('off')# With augmentationaxes[1].imshow(denormalize(img).numpy().transpose(1, 2, 0))axes[1].set_title(\"With Random Augmentation\")axes[1].axis('off')plt.suptitle(\"Effect of Data Augmentation\")plt.tight_layout()plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*60)print(\"SUMMARY: Module 1 Lab 3 - All Tasks Completed\")print(\"=\"*60)print(f\"Task 1: Created custom_transform with augmentation\")print(f\"Task 2: Loaded dataset with {len(imagefolder_dataset)} images\")print(f\"Task 3: Class names: {class_names}\")print(f\"        Class indices: {class_to_idx}\")print(f\"Task 4: Batch shape: {batch_images.shape}\")print(f\"Task 5: Displayed {batch_images.shape[0]} images from batch\")print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "a9b616f4ff403ce9d3ce1a5ea8de817489ad693f314b66f4e6e42f6649669fac",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0041c6238a924c8c8730f2f600de2ed1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "03a97a79ddc24a76b2a6a471e8f4e924": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d27e26f2d0df468ba20f68bd4a415116",
        "IPY_MODEL_e781a3f94b8543159b91f6d30dab1e3c",
        "IPY_MODEL_836bf3f1b7d94fc9a927cf4b578491c8"
       ],
       "layout": "IPY_MODEL_0dc003c99ed343b2bccd16324e84edbd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0dc003c99ed343b2bccd16324e84edbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b05356fecd344f3a53e1ea9d27a826e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e566d87219248f5a2039848d827a0c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d468dc0a907b40ba9bb99b5b7affdcf8",
        "IPY_MODEL_9e397936e9944cff8966a49fdeb6352a",
        "IPY_MODEL_b210fc8877a844519eb42cc516250498"
       ],
       "layout": "IPY_MODEL_633985b8bc924cd8bc2d81cdda6cdc2d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2452f3d33b224335a932908e957c7dae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "330fc2cfd9944f1bade9adaa5b085757": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b5d5f5f54bc4a248c42d1f34d1a063e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "633985b8bc924cd8bc2d81cdda6cdc2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "836bf3f1b7d94fc9a927cf4b578491c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6937cd2d9fc4393b4e45ae52516491e",
       "placeholder": "​",
       "style": "IPY_MODEL_2452f3d33b224335a932908e957c7dae",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 10976.46it/s]"
      }
     },
     "9e397936e9944cff8966a49fdeb6352a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ccee7cf9a84d4e999c5c9c70b8ffe212",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0041c6238a924c8c8730f2f600de2ed1",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "afe3a346896e46cf97ea4f0d9940455b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b210fc8877a844519eb42cc516250498": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd9d6c526a974d1581928e21e40d60fc",
       "placeholder": "​",
       "style": "IPY_MODEL_1b05356fecd344f3a53e1ea9d27a826e",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 9000039.89it/s]"
      }
     },
     "bd9d6c526a974d1581928e21e40d60fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ccee7cf9a84d4e999c5c9c70b8ffe212": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d27e26f2d0df468ba20f68bd4a415116": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3b5d5f5f54bc4a248c42d1f34d1a063e",
       "placeholder": "​",
       "style": "IPY_MODEL_afe3a346896e46cf97ea4f0d9940455b",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "d417e37552f84420b90f7e9e9550ea70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d468dc0a907b40ba9bb99b5b7affdcf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d417e37552f84420b90f7e9e9550ea70",
       "placeholder": "​",
       "style": "IPY_MODEL_db44f7abde994e2d918d0b291424b02a",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "db44f7abde994e2d918d0b291424b02a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e6937cd2d9fc4393b4e45ae52516491e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e781a3f94b8543159b91f6d30dab1e3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_330fc2cfd9944f1bade9adaa5b085757",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ebec604f31b245f8855774514194d9ec",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     },
     "ebec604f31b245f8855774514194d9ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
