{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eacbb2-0438-460a-b84d-365d2c52df68",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70d918-efbe-499a-bed2-588256d4e68c",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Land Classification: CNN-Transformer Integration evaluation </font></h1>\n",
    "    \n",
    "<h2 align=left><font size = 5>Vision Transformer (ViT) Model Evaluation </font></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc7c85-29e0-4e24-abe5-20db41e18761",
   "metadata": {},
   "source": [
    "Estimated time: 90 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e3aff-3d2d-4265-8cbf-bd90661a8b31",
   "metadata": {},
   "source": [
    "# Introdution\n",
    "\n",
    "This notebook presents an end-to-end workflow for importing, testing, and evaluating two Vision Transformer (ViT) models developed in Keras and PyTorch, respectively. \n",
    "The self-attention mechanism in the ViTs allows these models to learn complex and broad spatial dependencies, providing improved performance on a variety of vision tasks compared to traditional convolutional neural networks. However, the CNNs are adept in learning the local features very well and can be trained using relatively smaller datasets and is generally much more efficient in utilizing the computational resources, as compared to ViTs. A CNN-ViT hybrid architecture gains from both CNN and ViT model strengths, by getting local features extracted using CNNs, while the transformer part of the hybrid architecture can determine the global dependencies.\n",
    "\n",
    "This lab focuses on model loading, prediction on sample data, and quantitative evaluation of the ViT models created using KEras and PyTorch. You'll explore the details of the framework-specific implementations, test the consistency of results, and gain practical experience comparing deep learning models across different Python ecosystems. \n",
    "\n",
    "Upon completion, you will have a thorough understanding of loading the CNN-ViT hybrid models testing workflow, key evaluation metrics, and how high-level architectural concepts translate into practical model evaluation using both Keras and PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25356adc-873d-42c7-8cb2-454447373e9d",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Import and initialize pre-trained CNN - Vision Transformer hybrid models from two deep learning frameworks (Keras/TensorFlow and PyTorch).\n",
    "- Prepare and preprocess sample image data for inference.\n",
    "- Perform model inference and obtain prediction results from both models.\n",
    "- Compute and compare core evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49cc54-91d6-4cbd-9be0-169392c8cf6c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Dataset download, extraction and paths](#Dataset-download,-extraction-and-paths)\n",
    "2. [Pre-trained model download](#Pre-trained-model-download)\n",
    "3. [Package installation](#Package-installation)\n",
    "4. [Library imports and setup](#Library-imports-and-setup)\n",
    "5. [Fix random seed for reproducibility](#Fix-random-seed-for-reproducibility)\n",
    "6. [Defining PyTorch model architecture](#Defining-PyTorch-model-architecture)\n",
    "7. [Dataset path and hyperparameters](#Dataset-path-and-hyperparameters)\n",
    "8. [PyTorch Dataloader](#PyTorch-Dataloader)\n",
    "9. [--- PyTorch pre-trained ViT model loading ---](#----PyTorch-pre-trained-ViT-model-loading----)\n",
    "10. [PyTorch model inference metrics](#PyTorch-model-inference-metrics)\n",
    "11. [Keras model loading](#Keras-model-loading)\n",
    "12. [Keras pre-trained ViT model loading](#----Keras-pre-trained-ViT-model-loading----)\n",
    "13. [Define dataloader](#Define-dataloader)\n",
    "14. [Collecting metrics for Keras-based CNN-ViT hybrid model](#Collecting-metrics-for-Keras-based-CNN-ViT-hybrid-model)\n",
    "15. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "16. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "17. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "18. [ROC curve plotting](#ROC-curve-plotting)\n",
    "19. [Comparing model performance](#Comparing-model-performance)\n",
    "20. [Summary and discussion](#Summary-and-discussion)\n",
    "21. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6eaf-8d5d-4b61-b985-10539a611ba6",
   "metadata": {},
   "source": [
    "## Dataset download, extraction and paths\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18af01f-74b1-426a-a652-7aba04c7bdf9",
   "metadata": {},
   "source": [
    "### Define root directory and download `url`\n",
    "\n",
    "First, you define the root directory where all the data would be downloaded and extracted.\n",
    "Here, the `dataset_url` is assigned a direct link to a tar archive hosted on IBM's cloud storage. This URL points to the satellite image dataset used for land classification tasks. Using a cloud-based URL ensures accessibility without local storage dependencies. This setup facilitates automated downloads later in the notebook. \n",
    "\n",
    "If dealing with large datasets, monitor download times and implement retry mechanisms for robustness. This variable is key for the subsequent download functions, linking external data to the local workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0848e51-87e4-4374-a3dd-81b4db9e6b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:20.144821Z",
     "iopub.status.busy": "2025-10-27T07:51:20.144375Z",
     "iopub.status.idle": "2025-10-27T07:51:20.152882Z",
     "shell.execute_reply": "2025-10-27T07:51:20.151530Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2b3cd7-14e4-4567-ad34-34c90583558c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:20.158544Z",
     "iopub.status.busy": "2025-10-27T07:51:20.158186Z",
     "iopub.status.idle": "2025-10-27T07:51:20.163062Z",
     "shell.execute_reply": "2025-10-27T07:51:20.161749Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6aabf-caaa-45b7-b493-1c0b6c73f872",
   "metadata": {},
   "source": [
    "### Data download\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "2. The dataset fallback downloading function, based on regular `http` downloading functions.\n",
    "3. Download the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a352373-3515-4862-9cc4-15694768557d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:20.166241Z",
     "iopub.status.busy": "2025-10-27T07:51:20.166048Z",
     "iopub.status.idle": "2025-10-27T07:51:25.845940Z",
     "shell.execute_reply": "2025-10-27T07:51:25.845511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8318277024fe79008ac9b6d7f5d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca06f43504a84787b37e3c0eeb78df20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67a2f8-04a5-42af-a08a-81a5c9ff7c95",
   "metadata": {},
   "source": [
    "## Pre-trained model download \n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b48ebfb-061d-4cbd-bf02-2f4b2836625b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:25.847149Z",
     "iopub.status.busy": "2025-10-27T07:51:25.847052Z",
     "iopub.status.idle": "2025-10-27T07:51:25.849354Z",
     "shell.execute_reply": "2025-10-27T07:51:25.849090Z"
    }
   },
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3efac-9b6e-423f-bde9-10ca9daf7d7b",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed82fcc-d12e-41c4-944e-ed3b7fc68d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:25.850422Z",
     "iopub.status.busy": "2025-10-27T07:51:25.850345Z",
     "iopub.status.idle": "2025-10-27T07:52:18.302049Z",
     "shell.execute_reply": "2025-10-27T07:52:18.301576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded './keras_cnn_vit_ai_capstone.keras'.\n",
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded './pytorch_cnn_vit_ai_capstone_model_state_dict.pth'.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras\"\n",
    "keras_model_name = \"keras_cnn_vit_ai_capstone.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth\"\n",
    "pytorch_state_dict_name = \"pytorch_cnn_vit_ai_capstone_model_state_dict.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58480296-6234-48c3-8511-765d9b7b89df",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "### Install PyTorch, SkLearn library and Python Packages for evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "194d3d65-da47-4267-b6ae-fd84c253d11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:18.303505Z",
     "iopub.status.busy": "2025-10-27T07:52:18.303401Z",
     "iopub.status.idle": "2025-10-27T07:52:20.529717Z",
     "shell.execute_reply": "2025-10-27T07:52:20.528771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.13 ms, sys: 40.2 ms, total: 49.4 ms\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install scikit-learn==1.7.0 tensorflow==2.19 numpy==1.26 matplotlib==3.9.2 skillsnetwork\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae46e9b-24c2-46ee-92be-5817bb7b83c5",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d14b41b-8928-40eb-953e-d326618b876c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:20.531991Z",
     "iopub.status.busy": "2025-10-27T07:52:20.531801Z",
     "iopub.status.idle": "2025-10-27T07:52:20.697163Z",
     "shell.execute_reply": "2025-10-27T07:52:20.696755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 18.3 ms, total: 163 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e51d50-c950-41e0-8332-7ab4a00e969c",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2660f2dd-ae38-4e87-9695-0053d4f642cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:20.698424Z",
     "iopub.status.busy": "2025-10-27T07:52:20.698312Z",
     "iopub.status.idle": "2025-10-27T07:52:23.549160Z",
     "shell.execute_reply": "2025-10-27T07:52:23.548737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.20.0  |  GPUs found: []\n",
      "CPU times: user 1.99 s, sys: 272 ms, total: 2.27 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"TensorFlow {tf.__version__}  |  GPUs found: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0959e-20ee-433d-8866-0de289f6de7b",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05054c3b-a127-4a81-977d-ec3495e4121f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:23.550442Z",
     "iopub.status.busy": "2025-10-27T07:52:23.550296Z",
     "iopub.status.idle": "2025-10-27T07:52:24.752233Z",
     "shell.execute_reply": "2025-10-27T07:52:24.751734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 788 ms, sys: 188 ms, total: 976 ms\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import  random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b005d-7d48-423d-a0e2-d3b9db962417",
   "metadata": {},
   "source": [
    "## Fix random seed for reproducibility\n",
    "\n",
    "Define `set_seed` to ensure reproducibility across Python, NumPy, TensorFlow, and PyTorch by seeding random generators and enabling deterministic cuDNN. \n",
    "\n",
    "Set `SEED` to 7331. This is useful for consistent results in stochastic processes like training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83e3e30-18af-4f05-b3b2-f13a2b2c2030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.753592Z",
     "iopub.status.busy": "2025-10-27T07:52:24.753428Z",
     "iopub.status.idle": "2025-10-27T07:52:24.759450Z",
     "shell.execute_reply": "2025-10-27T07:52:24.759119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 7331 - Processes are now deterministic.\n"
     ]
    }
   ],
   "source": [
    "#====================\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, tensorflow, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- Tensorflow -------------------------------------------------\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False \n",
    "\n",
    "#====================\n",
    "SEED = 7331\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} - Processes are now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d734414-8a58-42be-971d-9d91a9e3992d",
   "metadata": {},
   "source": [
    "## Model paths \n",
    "Check for the existence of the Keras and PyTorch model files. This ensures models are accessible before loading, preventing runtime errors. Use absolute paths for reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ced8314-8f71-47bc-8f35-d93d07f7d98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.760503Z",
     "iopub.status.busy": "2025-10-27T07:52:24.760440Z",
     "iopub.status.idle": "2025-10-27T07:52:24.762250Z",
     "shell.execute_reply": "2025-10-27T07:52:24.761912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the pre-trained Keras model:\n",
      "keras_cnn_vit_ai_capstone.keras --at------> ./keras_cnn_vit_ai_capstone.keras\n",
      "Found the pre-trained PyTorch model:\n",
      "pytorch_cnn_vit_ai_capstone_model_state_dict.pth --at------> ./pytorch_cnn_vit_ai_capstone_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(keras_model_path):\n",
    "    print(\"Unable to find the Keras model at give path. Please check...\")\n",
    "else:\n",
    "    print(f\"Found the pre-trained Keras model:\\n{keras_model_name} --at------> {keras_model_path}\")\n",
    "\n",
    "if not os.path.exists(pytorch_state_dict_path):\n",
    "    print(\"Unable to find the PyTorch model at give path. Please check...\")\n",
    "else:\n",
    "    print(f\"Found the pre-trained PyTorch model:\\n{pytorch_state_dict_name} --at------> {pytorch_state_dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dd630-e8b0-48d4-9a57-734f750ad045",
   "metadata": {},
   "source": [
    "## Defining PyTorch model architecture\n",
    "In this cell, you will define the PyTorch CNN-ViT model architegcture, exactly as defined during the model training. You define the classes for CNN feature extractor, patch embedding, multi-head self-attention, transformer block, ViT, and CNN-ViT hybrid. \n",
    "\n",
    "The `evaluate` function computes loss and accuracy. This architecture combines CNN local features with ViT global attention. \n",
    "\n",
    "Parameters like depth and heads are configurable, and defined same as during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c0f3de-83af-4bee-8385-cf4cf61f0dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.763221Z",
     "iopub.status.busy": "2025-10-27T07:52:24.763158Z",
     "iopub.status.idle": "2025-10-27T07:52:24.770161Z",
     "shell.execute_reply": "2025-10-27T07:52:24.769769Z"
    }
   },
   "outputs": [],
   "source": [
    "#====================\n",
    "class ConvNet(nn.Module):\n",
    "    ''' \n",
    "    Class to define the architecture same as the imported pre-trained CNN model\n",
    "    '''\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # -------- convolutional feature extractor --------\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32,  kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64,  kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "        )\n",
    "\n",
    "        # -------- global pooling + classifier head --------\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(nn.Flatten(),                           # flatten feature map of dimensions (1024 × 1 × 1) to 1024\n",
    "                                        nn.Linear(1024, 2048), nn.ReLU(inplace=True), nn.BatchNorm1d(2048), nn.Dropout(0.4), \n",
    "                                        nn.Linear(2048, num_classes)\n",
    "                                       )\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.features(x)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.forward_features(x)   # features, dimensions:(B, 1024, H', W')\n",
    "        x = self.pool(x)               # global-average-pooling, dimensions: (B, 1024, 1, 1)\n",
    "        x = self.classifier(x)         # classifier, dimensions: (B, num_classes)\n",
    "        return x\n",
    "\n",
    "#====================\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, input_channel=1024, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)  # 1×1 conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)\n",
    "        return x\n",
    "\n",
    "#====================\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)  # (B, heads, N, d)\n",
    "        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attn_drop(attn.softmax(dim=-1))\n",
    "        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)\n",
    "        return self.proj_drop(self.proj(x))\n",
    "\n",
    "#====================\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = MHSA(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "                                    nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "                                    nn.GELU(), nn.Dropout(dropout),\n",
    "                                    nn.Linear(int(dim * mlp_ratio), dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "#====================\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_ch=1024, num_classes=2,\n",
    "                 embed_dim=768, depth=6, heads=8,\n",
    "                 mlp_ratio=4., dropout=0.1, max_tokens=50):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(in_ch, embed_dim)           # 1×1 conv\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                          # x: (B,C,H,W)\n",
    "        x = self.patch(x)                          # (B,L,D)\n",
    "        B, L, _ = x.shape\n",
    "        cls = self.cls.expand(B, -1, -1)           # (B,1,D)\n",
    "        x = torch.cat((cls, x), 1)                 # (B,L+1,D)\n",
    "        x = x + self.pos[:, :L + 1]                # match seq-len\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(self.norm(x)[:, 0])       # CLS token\n",
    "\n",
    "#====================\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvNet(num_classes)            # load weights later\n",
    "        self.vit = ViT(num_classes=num_classes,\n",
    "                       embed_dim=embed_dim,\n",
    "                       depth=depth,\n",
    "                       heads=heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vit(self.cnn.forward_features(x))\n",
    "\n",
    "#====================\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_sum, correct = 0, 0\n",
    "        for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Validation\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)# Set device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a174ab-eb20-4fb8-a5b1-398e9d35dc50",
   "metadata": {},
   "source": [
    "## Dataset path and hyperparameters\n",
    "Here, you set the dataset path and hyperparameters like image size, channels, batch size, classes, and labels. These are used for data loading and model configuration. Consistent dimensions ensure compatibility with model inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70984ebd-84aa-4399-8346-3250d7986835",
   "metadata": {},
   "source": [
    "## Task 1: Define the dataset directory, dataloader and model hyperparameters. The dataloader and model hyperparameters should be same as used during training \n",
    "\n",
    "- Define the `dataset_path`\n",
    "\n",
    "- Define **hyperparameters common dataloader**\n",
    "    - `img_w`, `img_h = 64, 64`\n",
    "    - `batch_size = 128`\n",
    "    - `num_classes = 2`\n",
    "    - `agri_class_labels = [\"non-agri\", \"agri\"]`\n",
    "\n",
    "  \n",
    "- Define **hyperparameters for PyTorch CNN-Vit Hybrid model**. The values have to same as those used while training the hybrid model. \n",
    "    - `depth = 3`\n",
    "    - `attn_heads = 6`\n",
    "    - `embed_dim = 768`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5330b470-40ab-4238-aee2-f19b9d7fd6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.771192Z",
     "iopub.status.busy": "2025-10-27T07:52:24.771129Z",
     "iopub.status.idle": "2025-10-27T07:52:24.772555Z",
     "shell.execute_reply": "2025-10-27T07:52:24.772216Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d99f0-be6d-4d40-a516-2adf8ab0f9c4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "\n",
    "# hyperparameters common dataloader\n",
    "img_w, img_h = 64, 64\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n",
    "\n",
    "# hyperparameters for PyTorch CNN-Vit Hybrid model\n",
    "depth = 3\n",
    "attn_heads = 6\n",
    "embed_dim = 768\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea803c85-963a-43be-b5a2-399dd2905804",
   "metadata": {},
   "source": [
    "### PyTorch Dataloader\n",
    "Defines transforms for resizing, tensor conversion, and normalization (ImageNet means/std). \n",
    "\n",
    "Loads dataset with ImageFolder and creates DataLoader for batching without shuffling for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c35ba20-5279-40d4-af31-24dc60d0cce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.773496Z",
     "iopub.status.busy": "2025-10-27T07:52:24.773438Z",
     "iopub.status.idle": "2025-10-27T07:52:24.931977Z",
     "shell.execute_reply": "2025-10-27T07:52:24.931619Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m train_transform = transforms.Compose([\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     transforms.Resize((\u001b[43mimg_w\u001b[49m, img_h)),\n\u001b[32m      3\u001b[39m     transforms.ToTensor(),\n\u001b[32m      4\u001b[39m     transforms.Normalize([\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], [\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m      5\u001b[39m ])\n\u001b[32m      6\u001b[39m full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n\u001b[32m      7\u001b[39m test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'img_w' is not defined"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586a494-a1b1-47ec-af9d-d63b6eca8f14",
   "metadata": {},
   "source": [
    "## Task 2: Instantiate PyTorch model\n",
    "Check the availability of CUDA device and set the `device` parameter accordingly.\n",
    "\n",
    "Based on the `CNN_ViT_Hybrid` function, instantiate the PyTorch model and move the model to the available `device`\n",
    "\n",
    "In this cell, you will:\n",
    "1. instantiate the PyTorch CNN_ViT_Hybrid with the previously declared model parameters\n",
    "3. detect the device for model inference\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ab3c8b-0ea4-44b2-934b-abe8b9744422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.933333Z",
     "iopub.status.busy": "2025-10-27T07:52:24.933263Z",
     "iopub.status.idle": "2025-10-27T07:52:24.934734Z",
     "shell.execute_reply": "2025-10-27T07:52:24.934411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0014627-d5d6-4ced-add8-616ff4d6dc3b",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create model instance\n",
    "pytorch_model = CNN_ViT_Hybrid(num_classes=num_classes,\n",
    "                      heads=attn_heads,\n",
    "                      depth=depth,\n",
    "                      embed_dim=embed_dim).to(device)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ddbc20-497f-49fe-abf1-8eb559e759b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.935707Z",
     "iopub.status.busy": "2025-10-27T07:52:24.935644Z",
     "iopub.status.idle": "2025-10-27T07:52:24.937231Z",
     "shell.execute_reply": "2025-10-27T07:52:24.936845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the PyTorch model on cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the PyTorch model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98188d2b-b2eb-479b-8152-167b6c292016",
   "metadata": {},
   "source": [
    "### --- PyTorch pre-trained ViT model loading ---\n",
    "In this cell, you will load the PyTorch model state dict with **`strict=False`** for flexibility.\n",
    "\n",
    "Thus, you prepare the model for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244c5b03-6aeb-4e20-800e-224f41ee249c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.938216Z",
     "iopub.status.busy": "2025-10-27T07:52:24.938142Z",
     "iopub.status.idle": "2025-10-27T07:52:24.949818Z",
     "shell.execute_reply": "2025-10-27T07:52:24.949430Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      5\u001b[39m     map_location=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mpytorch_model\u001b[49m.load_state_dict(torch.load(pytorch_state_dict_path, map_location=map_location), strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded model state dict, now getting predictions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pytorch_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained CNN-ViT hybrid model weights \n",
    "if device==\"cpu\":\n",
    "    map_location=torch.device(\"cpu\")\n",
    "else:\n",
    "    map_location=torch.device(\"cuda\")\n",
    "\n",
    "pytorch_model.load_state_dict(torch.load(pytorch_state_dict_path, map_location=map_location), strict=False)\n",
    "print(\"Loaded model state dict, now getting predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fe0fb-58a0-4243-b8d1-1b2ac71b25d9",
   "metadata": {},
   "source": [
    "### PyTorch model inference metrics\n",
    "\n",
    "Now, you perform:\n",
    "1. inference on test_loader\n",
    "2. collecte prediction, labels, and probabilities (for class 1)\n",
    "3. Uses no_grad for efficiency and eval mode\n",
    "4. Use tqdm to show progress.\n",
    "5. Move the data to the training device (CPU/GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c80a6dd4-c8a5-4ebf-967e-aef956953870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:24.950844Z",
     "iopub.status.busy": "2025-10-27T07:52:24.950782Z",
     "iopub.status.idle": "2025-10-27T07:52:25.024282Z",
     "shell.execute_reply": "2025-10-27T07:52:25.023805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 μs, sys: 1e+03 ns, total: 5 μs\n",
      "Wall time: 4.77 μs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall_preds_pytorch = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mall_labels_pytorch = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mall_probs_pytorch = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mpytorch_model.eval()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mwith torch.no_grad():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#    for images, labels in test_loader:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        images = images.to(device)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        outputs = pytorch_model(images)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        preds = torch.argmax(outputs, dim=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        all_probs_pytorch.extend(probs.cpu())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        all_labels_pytorch.extend(labels.numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1452\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1416\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1414\u001b[39m st = clock2()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:5\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'pytorch_model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "pytorch_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = pytorch_model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436d2fd-d169-4b47-bbda-ffd58dd03ea5",
   "metadata": {},
   "source": [
    "## Keras model loading\n",
    "\n",
    "To load the Keras based CNN-ViT hybrid model, you will\n",
    "\n",
    "- define **custom Keras layers** with serialization for model saving/loading for:\n",
    "    - `position embedding`\n",
    "    - `transformer block`\n",
    "\n",
    "This step is essential for reconstructing the ViT architecture in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7383f40e-7bbc-43ba-b821-d5917feb9f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.025415Z",
     "iopub.status.busy": "2025-10-27T07:52:25.025343Z",
     "iopub.status.idle": "2025-10-27T07:52:25.028402Z",
     "shell.execute_reply": "2025-10-27T07:52:25.028099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Positional embedding that Keras can track\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class AddPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super(AddPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.pos = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, tokens):\n",
    "        return tokens + self.pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"embed_dim\":   self.embed_dim,\n",
    "        })\n",
    "        return {**config}\n",
    "\n",
    "# One Transformer encoder block\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.dropout   = dropout\n",
    "        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.norm1(x + self.mha(x, x))\n",
    "        return self.norm2(x + self.mlp(x))\n",
    "\n",
    "    # ---- NEW ----\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":  self.embed_dim,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"mlp_dim\":    self.mlp_dim,\n",
    "            \"dropout\":    self.dropout,\n",
    "        })\n",
    "        return {**config}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27632548-a47a-4d87-a552-eb5d4d68cb91",
   "metadata": {},
   "source": [
    "### --- Keras pre-trained ViT model loading ---\n",
    "\n",
    "Here, you will load the pre-trained Keras model using **`load_model`**, providing **custom objects** for deserialization of user-defined layers. This enables inference with the hybrid model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18e3ebf9-ca84-4dc4-bf0b-6a751ae809de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.029456Z",
     "iopub.status.busy": "2025-10-27T07:52:25.029394Z",
     "iopub.status.idle": "2025-10-27T07:52:25.486910Z",
     "shell.execute_reply": "2025-10-27T07:52:25.486449Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- load CNN-ViT hybrid model ------------------\n",
    "keras_model = load_model(keras_model_name,\n",
    "                         custom_objects={\n",
    "                         \"AddPositionEmbedding\": AddPositionEmbedding,\n",
    "                         \"TransformerBlock\":     TransformerBlock\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f66ac-9364-43fb-939d-4fe9bbccdaad",
   "metadata": {},
   "source": [
    "### Define dataloader\n",
    "\n",
    "In this cell, you create an ImageDataGenerator for rescaling and a generator for flowing images from directory, matching PyTorch setup for consistent evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1671c3-68d9-4782-b010-678dc3468332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.488266Z",
     "iopub.status.busy": "2025-10-27T07:52:25.488190Z",
     "iopub.status.idle": "2025-10-27T07:52:25.503315Z",
     "shell.execute_reply": "2025-10-27T07:52:25.502906Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m datagen = ImageDataGenerator(rescale=\u001b[32m1.\u001b[39m/\u001b[32m255\u001b[39m)\n\u001b[32m      2\u001b[39m prediction_generator = datagen.flow_from_directory(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mdataset_path\u001b[49m,\n\u001b[32m      4\u001b[39m     target_size=(img_w, img_h),\n\u001b[32m      5\u001b[39m     batch_size=batch_size,\n\u001b[32m      6\u001b[39m     class_mode=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81465044-63f3-4894-8a83-6ac01a9b9330",
   "metadata": {},
   "source": [
    "### Collecting metrics for Keras-based CNN-ViT hybrid model\n",
    "Now, run the inference of the Keras-based CNN-ViT hybrid model and collect the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd8ad5e-158b-4290-b5fa-c08742aa1f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.504424Z",
     "iopub.status.busy": "2025-10-27T07:52:25.504355Z",
     "iopub.status.idle": "2025-10-27T07:52:25.546883Z",
     "shell.execute_reply": "2025-10-27T07:52:25.546548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 1 μs, total: 4 μs\n",
      "Wall time: 5.25 μs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prediction_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mall_probs_keras = keras_model.predict(prediction_generator, verbose=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mall_preds_keras = np.argmax(all_probs_keras, axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mall_labels_keras = prediction_generator.classes\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1452\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Personal/coursera/capstone/venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1416\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1414\u001b[39m st = clock2()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'prediction_generator' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_probs_keras = keras_model.predict(prediction_generator, verbose=1)\n",
    "all_preds_keras = np.argmax(all_probs_keras, axis=1)\n",
    "all_labels_keras = prediction_generator.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198b994-b702-427c-b996-009042a048e9",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d07927-80a9-4e52-ace0-954e175259cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.548115Z",
     "iopub.status.busy": "2025-10-27T07:52:25.548050Z",
     "iopub.status.idle": "2025-10-27T07:52:25.654088Z",
     "shell.execute_reply": "2025-10-27T07:52:25.653604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.3 ms, sys: 12.9 ms, total: 84.1 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    y_prob = np.array(y_prob)\n",
    "    if len(y_prob.shape)<2:\n",
    "        roc_score = roc_auc_score(y_true, y_prob)\n",
    "    elif len(y_prob.shape)==2:\n",
    "        roc_score = roc_auc_score(y_true, y_prob[:,1])\n",
    "    else:\n",
    "        roc_score = np.nan\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_score,\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    \n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    if metrics[\"ROC-AUC\"] != np.nan:\n",
    "        print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b1a11-e385-455a-9812-e316ae454324",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c792fd-13c1-457c-8280-4c2e37316400",
   "metadata": {},
   "source": [
    "## Task 3: Print the evaluation metrics using `print_metrics` function for the **Keras** ViT model with name `Keras CNN-Vit Hybrid Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c4978ba-7eb8-4d79-ac70-3c8293f7b85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.655330Z",
     "iopub.status.busy": "2025-10-27T07:52:25.655108Z",
     "iopub.status.idle": "2025-10-27T07:52:25.656766Z",
     "shell.execute_reply": "2025-10-27T07:52:25.656432Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46e9cc-f3df-454e-89be-0a7a9c6d3dd9",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras CNN-Vit Hybrid Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f13bcf-5335-43f5-ae4b-0ba97cf00a8f",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313ce16-7e6e-44cd-bdfd-f2d475c9c4f5",
   "metadata": {},
   "source": [
    "## Task 4: Print the evaluation metrics using `print_metrics` function for the **PyTorch** ViT model with model name `PyTorch CNN-Vit Hybrid Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95e086ec-a627-42cd-b090-1636c92bacf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.657791Z",
     "iopub.status.busy": "2025-10-27T07:52:25.657719Z",
     "iopub.status.idle": "2025-10-27T07:52:25.659184Z",
     "shell.execute_reply": "2025-10-27T07:52:25.658804Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c34e4-d3fe-44d9-b167-b22ae031cc84",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = np.array(all_probs_pytorch),\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch CNN-Vit Hybrid Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460611f-c893-454c-9736-2ffe2f7f36ad",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c943a6f-8845-44f8-849e-9f283aa480f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.660187Z",
     "iopub.status.busy": "2025-10-27T07:52:25.660130Z",
     "iopub.status.idle": "2025-10-27T07:52:25.662227Z",
     "shell.execute_reply": "2025-10-27T07:52:25.661893Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.4f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24ad5d-fb9a-4856-87d6-75213bb87415",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4f3b2fa-484a-41c4-835b-c5efddc9772e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.663188Z",
     "iopub.status.busy": "2025-10-27T07:52:25.663133Z",
     "iopub.status.idle": "2025-10-27T07:52:25.676723Z",
     "shell.execute_reply": "2025-10-27T07:52:25.676411Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_roc(np.array(\u001b[43mall_labels_keras\u001b[49m), np.array(all_probs_keras[:, \u001b[32m1\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33mKeras Model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m plt.show()\n\u001b[32m      3\u001b[39m plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \u001b[33m\"\u001b[39m\u001b[33mPyTorch Model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_labels_keras' is not defined"
     ]
    }
   ],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras[:, 1]), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f9c29-fe34-4a8c-bc9f-2ec588f1d83c",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n",
    "Computed metrics for both models are used to generate a comparison table for key scores. This facilitates quick performance assessment between frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd8b1c4e-dbe5-431e-b0be-f39d2d6036c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.677789Z",
     "iopub.status.busy": "2025-10-27T07:52:25.677722Z",
     "iopub.status.idle": "2025-10-27T07:52:25.689910Z",
     "shell.execute_reply": "2025-10-27T07:52:25.689582Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# get the Keras model performance metrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics_keras = model_metrics(\u001b[43mall_labels_keras\u001b[49m, all_preds_keras, all_probs_keras, agri_class_labels)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# get the PyTorch model performance metrics\u001b[39;00m\n\u001b[32m      5\u001b[39m metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_labels_keras' is not defined"
     ]
    }
   ],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} | {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "print((\"\".join([\"-\" for _ in range(43)])))\n",
    "metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in metrics_list:\n",
    "    print(\"{:<18} | {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eedfe-457e-46fc-ae78-444e34213b7f",
   "metadata": {},
   "source": [
    "## Summary and discussion\n",
    "\n",
    "This notebook showcased a framework-agnostic workflow for importing, testing, and evaluating Vision Transformer models built in both Keras and PyTorch. By running the same input through each model, we examined the compatibility of results and gained practical experience handling architectural and data format variations.\n",
    "\n",
    "Key insights include the criticality of input format alignment, the subtle differences in model serialization/loading, and the framework-induced variations in prediction outputs. For a more robust evaluation, repeat this process with a labeled validation dataset, compute further metrics (precision, recall, F1-score), and systematically analyze speed and resource usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e463c-3c73-4ca7-a37d-4aa2af25d1d8",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f397f9-8861-4c85-8686-24c064a9cb2e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations! You have successfully completed this lab and now you have a good understanding about loading custom pre-trained models, for both Keras and PyTorch frameworks. Using these pre-trained models, you can now evaluate their performance and also infer unknown datasets.\n",
    "I hope you have learnt to apply the key concepts of Keras/Pytorch based classifiers, both traditional CNNs and more advanced and state-of-the-art, CNN-ViT hybrid models. Using this knowledge, now you should be able to tackle a variety of real world image classification problems. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d20ee-fe79-446f-93e9-6b7d8bec6406",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d6c63-9e28-4f73-adde-c23e5ccd8be7",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-28  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0ce86-a24f-40b1-ab17-f6bdf0e3db66",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:52:25.691265Z",
     "iopub.status.busy": "2025-10-27T07:52:25.691200Z",
     "iopub.status.idle": "2025-10-27T07:52:25.695814Z",
     "shell.execute_reply": "2025-10-27T07:52:25.695420Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (984090413.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 3 Lab 3: Land Classification - CNN-Transformer Integration EvaluationSolutions for all tasks (8 points total)Copy these code blocks into the corresponding cells in the notebook:lab-M4L1-Land-Classification-CNN-ViT-Integration-Evaluation-v1.ipynb\"\"\"import osimport torchimport torch.nn as nnimport numpy as npfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,                            f1_score, roc_auc_score, confusion_matrix,                            classification_report, ConfusionMatrixDisplay)# TensorFlow/Keras importsos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'import tensorflow as tffrom tensorflow.keras.models import load_modelfrom tensorflow.keras.preprocessing.image import ImageDataGenerator# =============================================================================# TASK 1: Define dataset directory, dataloader, and model hyperparameters# =============================================================================# TASK 1 ANSWER:dataset_path = os.path.join(\".\", \"images_dataSAT\")# Hyperparameters for dataloadersimg_w, img_h = 64, 64batch_size = 128num_classes = 2agri_class_labels = [\"non-agri\", \"agri\"]# Hyperparameters for PyTorch CNN-ViT Hybrid model (same as training)depth = 3attn_heads = 6embed_dim = 768print(\"TASK 1: Configuration Set\")print(\"=\"*70)print(f\"Dataset path: {dataset_path}\")print(f\"\\nDataloader hyperparameters:\")print(f\"  Image size: {img_w}x{img_h}\")print(f\"  Batch size: {batch_size}\")print(f\"  Number of classes: {num_classes}\")print(f\"  Class labels: {agri_class_labels}\")print(f\"\\nPyTorch model hyperparameters:\")print(f\"  Transformer depth: {depth}\")print(f\"  Attention heads: {attn_heads}\")print(f\"  Embedding dimension: {embed_dim}\")# =============================================================================# PYTORCH MODEL ARCHITECTURE (Same as training)# =============================================================================class ConvNet(nn.Module):    def __init__(self, num_classes: int):        super().__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 32, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(32),            nn.Conv2d(32, 64, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(64),            nn.Conv2d(64, 128, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(128),            nn.Conv2d(128, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),            nn.Conv2d(256, 512, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),            nn.Conv2d(512, 1024, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),        )    def forward_features(self, x: torch.Tensor) -> torch.Tensor:        return self.features(x)class PatchEmbed(nn.Module):    def __init__(self, input_channel=1024, embed_dim=768):        super().__init__()        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)    def forward(self, x):        x = self.proj(x).flatten(2).transpose(1, 2)        return xclass MHSA(nn.Module):    def __init__(self, dim, heads=8, dropout=0.):        super().__init__()        self.heads = heads        self.scale = (dim // heads) ** -0.5        self.qkv = nn.Linear(dim, dim * 3)        self.attn_drop = nn.Dropout(dropout)        self.proj = nn.Linear(dim, dim)        self.proj_drop = nn.Dropout(dropout)    def forward(self, x):        B, N, D = x.shape        q, k, v = self.qkv(x).chunk(3, dim=-1)        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale        attn = self.attn_drop(attn.softmax(dim=-1))        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)        return self.proj_drop(self.proj(x))class TransformerBlock(nn.Module):    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):        super().__init__()        self.norm1 = nn.LayerNorm(dim)        self.attn  = MHSA(dim, heads, dropout)        self.norm2 = nn.LayerNorm(dim)        self.mlp   = nn.Sequential(            nn.Linear(dim, int(dim * mlp_ratio)),            nn.GELU(), nn.Dropout(dropout),            nn.Linear(int(dim * mlp_ratio), dim),            nn.Dropout(dropout)        )    def forward(self, x):        x = x + self.attn(self.norm1(x))        x = x + self.mlp(self.norm2(x))        return xclass ViT(nn.Module):    def __init__(self, in_ch=1024, num_classes=2,                 embed_dim=768, depth=6, heads=8,                 mlp_ratio=4., dropout=0.1, max_tokens=50):        super().__init__()        self.patch = PatchEmbed(in_ch, embed_dim)        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))        self.blocks = nn.ModuleList([            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)            for _ in range(depth)        ])        self.norm = nn.LayerNorm(embed_dim)        self.head = nn.Linear(embed_dim, num_classes)    def forward(self, x):        x = self.patch(x)        B, L, _ = x.shape        cls = self.cls.expand(B, -1, -1)        x = torch.cat((cls, x), 1)        x = x + self.pos[:, :L + 1]        for blk in self.blocks:            x = blk(x)        return self.head(self.norm(x)[:, 0])class CNN_ViT_Hybrid(nn.Module):    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):        super().__init__()        self.cnn = ConvNet(num_classes)        self.vit = ViT(num_classes=num_classes,                      embed_dim=embed_dim,                      depth=depth,                      heads=heads)    def forward(self, x):        return self.vit(self.cnn.forward_features(x))# =============================================================================# TASK 2: Instantiate PyTorch model# =============================================================================# Check devicedevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"# TASK 2 ANSWER:pytorch_model = CNN_ViT_Hybrid(    num_classes=num_classes,    heads=attn_heads,    depth=depth,    embed_dim=embed_dim).to(device)print(f\"\\nTASK 2: PyTorch Model Instantiated\")print(f\"  Device: {device}\")print(f\"  Model: CNN_ViT_Hybrid\")print(f\"  Parameters: depth={depth}, heads={attn_heads}, embed_dim={embed_dim}\")# Load pre-trained weightspytorch_state_dict_path = \"pytorch_cnn_vit_ai_capstone_model_state_dict.pth\"if os.path.exists(pytorch_state_dict_path):    pytorch_model.load_state_dict(        torch.load(pytorch_state_dict_path, map_location=device),        strict=False    )    print(f\"  Loaded weights from: {pytorch_state_dict_path}\")else:    print(f\"  Warning: Pre-trained weights not found at {pytorch_state_dict_path}\")# =============================================================================# PYTORCH DATALOADER# =============================================================================test_transform = transforms.Compose([    transforms.Resize((img_w, img_h)),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])full_dataset = datasets.ImageFolder(dataset_path, transform=test_transform)test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)print(f\"\\nPyTorch DataLoader created:\")print(f\"  Total samples: {len(full_dataset)}\")print(f\"  Batches: {len(test_loader)}\")# =============================================================================# PYTORCH MODEL INFERENCE# =============================================================================print(\"\\nRunning PyTorch model inference...\")all_preds_pytorch = []all_labels_pytorch = []all_probs_pytorch = []pytorch_model.eval()with torch.no_grad():    for images, labels in tqdm(test_loader, desc=\"PyTorch Inference\"):        images = images.to(device)        outputs = pytorch_model(images)        preds = torch.argmax(outputs, dim=1)        probs = torch.softmax(outputs, dim=1)[:, 1]  # Prob for class 1        all_probs_pytorch.extend(probs.cpu())        all_preds_pytorch.extend(preds.cpu().numpy().flatten())        all_labels_pytorch.extend(labels.numpy())print(f\"PyTorch predictions collected: {len(all_preds_pytorch)}\")# =============================================================================# KERAS MODEL LOADING# =============================================================================# Custom Keras layers (required for loading)@tf.keras.utils.register_keras_serializable(package=\"Custom\")class AddPositionEmbedding(tf.keras.layers.Layer):    def __init__(self, num_patches, embed_dim, **kwargs):        super().__init__(**kwargs)        self.num_patches = num_patches        self.embed_dim   = embed_dim        self.pos = self.add_weight(            name=\"pos_embedding\",            shape=(1, num_patches, embed_dim),            initializer=\"random_normal\",            trainable=True        )    def call(self, tokens):        return tokens + self.pos    def get_config(self):        config = super().get_config()        config.update({            \"num_patches\": self.num_patches,            \"embed_dim\":   self.embed_dim,        })        return config@tf.keras.utils.register_keras_serializable(package=\"Custom\")class TransformerBlockKeras(tf.keras.layers.Layer):    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):        super().__init__(**kwargs)        self.embed_dim = embed_dim        self.num_heads = num_heads        self.mlp_dim   = mlp_dim        self.dropout   = dropout        self.mha  = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embed_dim)        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        self.mlp = tf.keras.Sequential([            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),            tf.keras.layers.Dropout(dropout),            tf.keras.layers.Dense(embed_dim),            tf.keras.layers.Dropout(dropout)        ])    def call(self, x):        x = self.norm1(x + self.mha(x, x))        return self.norm2(x + self.mlp(x))    def get_config(self):        config = super().get_config()        config.update({            \"embed_dim\":  self.embed_dim,            \"num_heads\":  self.num_heads,            \"mlp_dim\":    self.mlp_dim,            \"dropout\":    self.dropout,        })        return config# Load Keras modelkeras_model_path = \"keras_cnn_vit_ai_capstone.keras\"if os.path.exists(keras_model_path):    keras_model = load_model(        keras_model_path,        custom_objects={            \"AddPositionEmbedding\": AddPositionEmbedding,            \"TransformerBlock\": TransformerBlockKeras        }    )    print(f\"\\nKeras model loaded from: {keras_model_path}\")else:    print(f\"\\nWarning: Keras model not found at {keras_model_path}\")    keras_model = None# =============================================================================# KERAS DATALOADER# =============================================================================if keras_model is not None:    datagen = ImageDataGenerator(rescale=1./255)    prediction_generator = datagen.flow_from_directory(        dataset_path,        target_size=(img_w, img_h),        batch_size=batch_size,        class_mode=\"binary\",        shuffle=False    )    print(f\"\\nKeras DataGenerator created:\")    print(f\"  Total samples: {prediction_generator.samples}\")    # Run inference    print(\"\\nRunning Keras model inference...\")    all_probs_keras = keras_model.predict(prediction_generator, verbose=1)    all_preds_keras = np.argmax(all_probs_keras, axis=1)    all_labels_keras = prediction_generator.classes    print(f\"Keras predictions collected: {len(all_preds_keras)}\")# =============================================================================# EVALUATION METRICS FUNCTION# =============================================================================def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):    \"\"\"Print comprehensive evaluation metrics\"\"\"    # Compute metrics    accuracy = accuracy_score(y_true, y_pred)    precision = precision_score(y_true, y_pred, average='binary')    recall = recall_score(y_true, y_pred, average='binary')    f1 = f1_score(y_true, y_pred, average='binary')    # ROC-AUC    y_prob = np.array(y_prob)    if len(y_prob.shape) < 2:        roc_auc = roc_auc_score(y_true, y_prob)    elif len(y_prob.shape) == 2:        roc_auc = roc_auc_score(y_true, y_prob[:, 1])    else:        roc_auc = np.nan    # Print results    print(f\"\\n{'='*70}\")    print(f\"Evaluation metrics for the {model_name}\")    print(f\"{'='*70}\")    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")    print(f\"Precision: {precision:.4f}\")    print(f\"Recall:    {recall:.4f}\")    print(f\"F1-Score:  {f1:.4f}\")    if not np.isnan(roc_auc):        print(f\"ROC-AUC:   {roc_auc:.4f}\")    # Classification report    print(f\"\\nClassification Report:\")    print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))    # Confusion matrix    cm = confusion_matrix(y_true, y_pred)    print(f\"\\nConfusion Matrix:\")    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)    fig, ax = plt.subplots(figsize=(8, 6))    disp.plot(ax=ax, cmap='Blues', values_format='d')    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')    plt.tight_layout()    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_cm.png', dpi=300)    plt.show()# =============================================================================# TASK 3: Print evaluation metrics for Keras model# =============================================================================if keras_model is not None:    # TASK 3 ANSWER:    print_metrics(        y_true=all_labels_keras,        y_pred=all_preds_keras,        y_prob=all_probs_keras,        class_labels=agri_class_labels,        model_name=\"Keras CNN-ViT Hybrid Model\"    )else:    print(\"\\nTASK 3: Skipped (Keras model not available)\")# =============================================================================# TASK 4: Print evaluation metrics for PyTorch model# =============================================================================# TASK 4 ANSWER:print_metrics(    y_true=all_labels_pytorch,    y_pred=all_preds_pytorch,    y_prob=np.array(all_probs_pytorch),    class_labels=agri_class_labels,    model_name=\"PyTorch CNN-ViT Hybrid Model\")# =============================================================================# MODEL COMPARISON# =============================================================================if keras_model is not None:    print(\"\\n\" + \"=\"*70)    print(\"MODEL COMPARISON\")    print(\"=\"*70)    keras_acc = accuracy_score(all_labels_keras, all_preds_keras)    pytorch_acc = accuracy_score(all_labels_pytorch, all_preds_pytorch)    print(f\"\\nAccuracy Comparison:\")    print(f\"  Keras:   {keras_acc:.4f} ({keras_acc*100:.2f}%)\")    print(f\"  PyTorch: {pytorch_acc:.4f} ({pytorch_acc*100:.2f}%)\")    print(f\"  Difference: {abs(keras_acc - pytorch_acc):.4f}\")    # Visualization    fig, ax = plt.subplots(figsize=(10, 6))    models = ['Keras CNN-ViT', 'PyTorch CNN-ViT']    accuracies = [keras_acc, pytorch_acc]    colors = ['skyblue', 'coral']    bars = ax.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')    ax.set_ylabel('Accuracy')    ax.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')    ax.set_ylim([0.8, 1.0])    ax.grid(True, alpha=0.3, axis='y')    # Add value labels on bars    for bar in bars:        height = bar.get_height()        ax.text(bar.get_x() + bar.get_width()/2., height,               f'{height:.4f}',               ha='center', va='bottom', fontweight='bold')    plt.tight_layout()    plt.savefig('model_comparison.png', dpi=300)    plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 3 (Final Evaluation) - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Configured dataset path and hyperparameters\")print(f\"Task 2: Instantiated PyTorch model (depth={depth}, heads={attn_heads})\")print(f\"Task 3: Printed Keras CNN-ViT Hybrid Model metrics\")print(f\"Task 4: Printed PyTorch CNN-ViT Hybrid Model metrics\")print(f\"\\nPyTorch Final Accuracy: {pytorch_acc:.4f} ({pytorch_acc*100:.2f}%)\")if keras_model is not None:    print(f\"Keras Final Accuracy: {keras_acc:.4f} ({keras_acc*100:.2f}%)\")print(\"\\nCongratulations! All 9 labs completed successfully!\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                                                                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 3 Lab 3: Land Classification - CNN-Transformer Integration EvaluationSolutions for all tasks (8 points total)Copy these code blocks into the corresponding cells in the notebook:lab-M4L1-Land-Classification-CNN-ViT-Integration-Evaluation-v1.ipynb\"\"\"import osimport torchimport torch.nn as nnimport numpy as npfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,                            f1_score, roc_auc_score, confusion_matrix,                            classification_report, ConfusionMatrixDisplay)# TensorFlow/Keras importsos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'import tensorflow as tffrom tensorflow.keras.models import load_modelfrom tensorflow.keras.preprocessing.image import ImageDataGenerator# =============================================================================# TASK 1: Define dataset directory, dataloader, and model hyperparameters# =============================================================================# TASK 1 ANSWER:dataset_path = os.path.join(\".\", \"images_dataSAT\")# Hyperparameters for dataloadersimg_w, img_h = 64, 64batch_size = 128num_classes = 2agri_class_labels = [\"non-agri\", \"agri\"]# Hyperparameters for PyTorch CNN-ViT Hybrid model (same as training)depth = 3attn_heads = 6embed_dim = 768print(\"TASK 1: Configuration Set\")print(\"=\"*70)print(f\"Dataset path: {dataset_path}\")print(f\"\\nDataloader hyperparameters:\")print(f\"  Image size: {img_w}x{img_h}\")print(f\"  Batch size: {batch_size}\")print(f\"  Number of classes: {num_classes}\")print(f\"  Class labels: {agri_class_labels}\")print(f\"\\nPyTorch model hyperparameters:\")print(f\"  Transformer depth: {depth}\")print(f\"  Attention heads: {attn_heads}\")print(f\"  Embedding dimension: {embed_dim}\")# =============================================================================# PYTORCH MODEL ARCHITECTURE (Same as training)# =============================================================================class ConvNet(nn.Module):    def __init__(self, num_classes: int):        super().__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 32, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(32),            nn.Conv2d(32, 64, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(64),            nn.Conv2d(64, 128, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(128),            nn.Conv2d(128, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),            nn.Conv2d(256, 512, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),            nn.Conv2d(512, 1024, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),        )    def forward_features(self, x: torch.Tensor) -> torch.Tensor:        return self.features(x)class PatchEmbed(nn.Module):    def __init__(self, input_channel=1024, embed_dim=768):        super().__init__()        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)    def forward(self, x):        x = self.proj(x).flatten(2).transpose(1, 2)        return xclass MHSA(nn.Module):    def __init__(self, dim, heads=8, dropout=0.):        super().__init__()        self.heads = heads        self.scale = (dim // heads) ** -0.5        self.qkv = nn.Linear(dim, dim * 3)        self.attn_drop = nn.Dropout(dropout)        self.proj = nn.Linear(dim, dim)        self.proj_drop = nn.Dropout(dropout)    def forward(self, x):        B, N, D = x.shape        q, k, v = self.qkv(x).chunk(3, dim=-1)        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale        attn = self.attn_drop(attn.softmax(dim=-1))        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)        return self.proj_drop(self.proj(x))class TransformerBlock(nn.Module):    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):        super().__init__()        self.norm1 = nn.LayerNorm(dim)        self.attn  = MHSA(dim, heads, dropout)        self.norm2 = nn.LayerNorm(dim)        self.mlp   = nn.Sequential(            nn.Linear(dim, int(dim * mlp_ratio)),            nn.GELU(), nn.Dropout(dropout),            nn.Linear(int(dim * mlp_ratio), dim),            nn.Dropout(dropout)        )    def forward(self, x):        x = x + self.attn(self.norm1(x))        x = x + self.mlp(self.norm2(x))        return xclass ViT(nn.Module):    def __init__(self, in_ch=1024, num_classes=2,                 embed_dim=768, depth=6, heads=8,                 mlp_ratio=4., dropout=0.1, max_tokens=50):        super().__init__()        self.patch = PatchEmbed(in_ch, embed_dim)        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))        self.blocks = nn.ModuleList([            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)            for _ in range(depth)        ])        self.norm = nn.LayerNorm(embed_dim)        self.head = nn.Linear(embed_dim, num_classes)    def forward(self, x):        x = self.patch(x)        B, L, _ = x.shape        cls = self.cls.expand(B, -1, -1)        x = torch.cat((cls, x), 1)        x = x + self.pos[:, :L + 1]        for blk in self.blocks:            x = blk(x)        return self.head(self.norm(x)[:, 0])class CNN_ViT_Hybrid(nn.Module):    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):        super().__init__()        self.cnn = ConvNet(num_classes)        self.vit = ViT(num_classes=num_classes,                      embed_dim=embed_dim,                      depth=depth,                      heads=heads)    def forward(self, x):        return self.vit(self.cnn.forward_features(x))# =============================================================================# TASK 2: Instantiate PyTorch model# =============================================================================# Check devicedevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"# TASK 2 ANSWER:pytorch_model = CNN_ViT_Hybrid(    num_classes=num_classes,    heads=attn_heads,    depth=depth,    embed_dim=embed_dim).to(device)print(f\"\\nTASK 2: PyTorch Model Instantiated\")print(f\"  Device: {device}\")print(f\"  Model: CNN_ViT_Hybrid\")print(f\"  Parameters: depth={depth}, heads={attn_heads}, embed_dim={embed_dim}\")# Load pre-trained weightspytorch_state_dict_path = \"pytorch_cnn_vit_ai_capstone_model_state_dict.pth\"if os.path.exists(pytorch_state_dict_path):    pytorch_model.load_state_dict(        torch.load(pytorch_state_dict_path, map_location=device),        strict=False    )    print(f\"  Loaded weights from: {pytorch_state_dict_path}\")else:    print(f\"  Warning: Pre-trained weights not found at {pytorch_state_dict_path}\")# =============================================================================# PYTORCH DATALOADER# =============================================================================test_transform = transforms.Compose([    transforms.Resize((img_w, img_h)),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])full_dataset = datasets.ImageFolder(dataset_path, transform=test_transform)test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)print(f\"\\nPyTorch DataLoader created:\")print(f\"  Total samples: {len(full_dataset)}\")print(f\"  Batches: {len(test_loader)}\")# =============================================================================# PYTORCH MODEL INFERENCE# =============================================================================print(\"\\nRunning PyTorch model inference...\")all_preds_pytorch = []all_labels_pytorch = []all_probs_pytorch = []pytorch_model.eval()with torch.no_grad():    for images, labels in tqdm(test_loader, desc=\"PyTorch Inference\"):        images = images.to(device)        outputs = pytorch_model(images)        preds = torch.argmax(outputs, dim=1)        probs = torch.softmax(outputs, dim=1)[:, 1]  # Prob for class 1        all_probs_pytorch.extend(probs.cpu())        all_preds_pytorch.extend(preds.cpu().numpy().flatten())        all_labels_pytorch.extend(labels.numpy())print(f\"PyTorch predictions collected: {len(all_preds_pytorch)}\")# =============================================================================# KERAS MODEL LOADING# =============================================================================# Custom Keras layers (required for loading)@tf.keras.utils.register_keras_serializable(package=\"Custom\")class AddPositionEmbedding(tf.keras.layers.Layer):    def __init__(self, num_patches, embed_dim, **kwargs):        super().__init__(**kwargs)        self.num_patches = num_patches        self.embed_dim   = embed_dim        self.pos = self.add_weight(            name=\"pos_embedding\",            shape=(1, num_patches, embed_dim),            initializer=\"random_normal\",            trainable=True        )    def call(self, tokens):        return tokens + self.pos    def get_config(self):        config = super().get_config()        config.update({            \"num_patches\": self.num_patches,            \"embed_dim\":   self.embed_dim,        })        return config@tf.keras.utils.register_keras_serializable(package=\"Custom\")class TransformerBlockKeras(tf.keras.layers.Layer):    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):        super().__init__(**kwargs)        self.embed_dim = embed_dim        self.num_heads = num_heads        self.mlp_dim   = mlp_dim        self.dropout   = dropout        self.mha  = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embed_dim)        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        self.mlp = tf.keras.Sequential([            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),            tf.keras.layers.Dropout(dropout),            tf.keras.layers.Dense(embed_dim),            tf.keras.layers.Dropout(dropout)        ])    def call(self, x):        x = self.norm1(x + self.mha(x, x))        return self.norm2(x + self.mlp(x))    def get_config(self):        config = super().get_config()        config.update({            \"embed_dim\":  self.embed_dim,            \"num_heads\":  self.num_heads,            \"mlp_dim\":    self.mlp_dim,            \"dropout\":    self.dropout,        })        return config# Load Keras modelkeras_model_path = \"keras_cnn_vit_ai_capstone.keras\"if os.path.exists(keras_model_path):    keras_model = load_model(        keras_model_path,        custom_objects={            \"AddPositionEmbedding\": AddPositionEmbedding,            \"TransformerBlock\": TransformerBlockKeras        }    )    print(f\"\\nKeras model loaded from: {keras_model_path}\")else:    print(f\"\\nWarning: Keras model not found at {keras_model_path}\")    keras_model = None# =============================================================================# KERAS DATALOADER# =============================================================================if keras_model is not None:    datagen = ImageDataGenerator(rescale=1./255)    prediction_generator = datagen.flow_from_directory(        dataset_path,        target_size=(img_w, img_h),        batch_size=batch_size,        class_mode=\"binary\",        shuffle=False    )    print(f\"\\nKeras DataGenerator created:\")    print(f\"  Total samples: {prediction_generator.samples}\")    # Run inference    print(\"\\nRunning Keras model inference...\")    all_probs_keras = keras_model.predict(prediction_generator, verbose=1)    all_preds_keras = np.argmax(all_probs_keras, axis=1)    all_labels_keras = prediction_generator.classes    print(f\"Keras predictions collected: {len(all_preds_keras)}\")# =============================================================================# EVALUATION METRICS FUNCTION# =============================================================================def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):    \"\"\"Print comprehensive evaluation metrics\"\"\"    # Compute metrics    accuracy = accuracy_score(y_true, y_pred)    precision = precision_score(y_true, y_pred, average='binary')    recall = recall_score(y_true, y_pred, average='binary')    f1 = f1_score(y_true, y_pred, average='binary')    # ROC-AUC    y_prob = np.array(y_prob)    if len(y_prob.shape) < 2:        roc_auc = roc_auc_score(y_true, y_prob)    elif len(y_prob.shape) == 2:        roc_auc = roc_auc_score(y_true, y_prob[:, 1])    else:        roc_auc = np.nan    # Print results    print(f\"\\n{'='*70}\")    print(f\"Evaluation metrics for the {model_name}\")    print(f\"{'='*70}\")    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")    print(f\"Precision: {precision:.4f}\")    print(f\"Recall:    {recall:.4f}\")    print(f\"F1-Score:  {f1:.4f}\")    if not np.isnan(roc_auc):        print(f\"ROC-AUC:   {roc_auc:.4f}\")    # Classification report    print(f\"\\nClassification Report:\")    print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))    # Confusion matrix    cm = confusion_matrix(y_true, y_pred)    print(f\"\\nConfusion Matrix:\")    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)    fig, ax = plt.subplots(figsize=(8, 6))    disp.plot(ax=ax, cmap='Blues', values_format='d')    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')    plt.tight_layout()    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_cm.png', dpi=300)    plt.show()# =============================================================================# TASK 3: Print evaluation metrics for Keras model# =============================================================================if keras_model is not None:    # TASK 3 ANSWER:    print_metrics(        y_true=all_labels_keras,        y_pred=all_preds_keras,        y_prob=all_probs_keras,        class_labels=agri_class_labels,        model_name=\"Keras CNN-ViT Hybrid Model\"    )else:    print(\"\\nTASK 3: Skipped (Keras model not available)\")# =============================================================================# TASK 4: Print evaluation metrics for PyTorch model# =============================================================================# TASK 4 ANSWER:print_metrics(    y_true=all_labels_pytorch,    y_pred=all_preds_pytorch,    y_prob=np.array(all_probs_pytorch),    class_labels=agri_class_labels,    model_name=\"PyTorch CNN-ViT Hybrid Model\")# =============================================================================# MODEL COMPARISON# =============================================================================if keras_model is not None:    print(\"\\n\" + \"=\"*70)    print(\"MODEL COMPARISON\")    print(\"=\"*70)    keras_acc = accuracy_score(all_labels_keras, all_preds_keras)    pytorch_acc = accuracy_score(all_labels_pytorch, all_preds_pytorch)    print(f\"\\nAccuracy Comparison:\")    print(f\"  Keras:   {keras_acc:.4f} ({keras_acc*100:.2f}%)\")    print(f\"  PyTorch: {pytorch_acc:.4f} ({pytorch_acc*100:.2f}%)\")    print(f\"  Difference: {abs(keras_acc - pytorch_acc):.4f}\")    # Visualization    fig, ax = plt.subplots(figsize=(10, 6))    models = ['Keras CNN-ViT', 'PyTorch CNN-ViT']    accuracies = [keras_acc, pytorch_acc]    colors = ['skyblue', 'coral']    bars = ax.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')    ax.set_ylabel('Accuracy')    ax.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')    ax.set_ylim([0.8, 1.0])    ax.grid(True, alpha=0.3, axis='y')    # Add value labels on bars    for bar in bars:        height = bar.get_height()        ax.text(bar.get_x() + bar.get_width()/2., height,               f'{height:.4f}',               ha='center', va='bottom', fontweight='bold')    plt.tight_layout()    plt.savefig('model_comparison.png', dpi=300)    plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 3 (Final Evaluation) - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Configured dataset path and hyperparameters\")print(f\"Task 2: Instantiated PyTorch model (depth={depth}, heads={attn_heads})\")print(f\"Task 3: Printed Keras CNN-ViT Hybrid Model metrics\")print(f\"Task 4: Printed PyTorch CNN-ViT Hybrid Model metrics\")print(f\"\\nPyTorch Final Accuracy: {pytorch_acc:.4f} ({pytorch_acc*100:.2f}%)\")if keras_model is not None:    print(f\"Keras Final Accuracy: {keras_acc:.4f} ({keras_acc*100:.2f}%)\")print(\"\\nCongratulations! All 9 labs completed successfully!\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "0617c40a894fed25e60cecc592899ab7c410ed1cb7dbf852edb7281fb679f1af",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11ba3208f8d74d62abcf0c53090e3ee1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15e9cd7e30ee4e05be1400338f29cd76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1713e0261ceb49cf909449f94c91b57d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "299249090c284ae2ada8ffd59250fcdf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "416889374c394fd5badf5bbd021fa498": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_15e9cd7e30ee4e05be1400338f29cd76",
       "placeholder": "​",
       "style": "IPY_MODEL_a83c8d2783a64390be542d068dea77e6",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 11360.09it/s]"
      }
     },
     "4fb8d310a8f144ba932ba90a803c2407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6a1ae2eeb26a4c27a0e26a433d0c338a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "716d37d663954bbda6f582c08f5515eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_299249090c284ae2ada8ffd59250fcdf",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0f08215510d4623a17c969ec1c2c61b",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "7560d8a1be4f4abfae64b18115f52a76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d6aa2aa38fc435fbee6fcbab90c5cb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d8c8292238f4bd3ae1c094c6d2a65b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f0646ad851b47629112f7f18de07427",
       "placeholder": "​",
       "style": "IPY_MODEL_1713e0261ceb49cf909449f94c91b57d",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 13303151.79it/s]"
      }
     },
     "8283648ab0424bd0b131f44fc057e5de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93d8318277024fe79008ac9b6d7f5d30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f503377e9c904dd690e53737edb759f9",
        "IPY_MODEL_716d37d663954bbda6f582c08f5515eb",
        "IPY_MODEL_7d8c8292238f4bd3ae1c094c6d2a65b8"
       ],
       "layout": "IPY_MODEL_8283648ab0424bd0b131f44fc057e5de",
       "tabbable": null,
       "tooltip": null
      }
     },
     "977ad3c508f84ab8aaaeec6745580e0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9f0646ad851b47629112f7f18de07427": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a83c8d2783a64390be542d068dea77e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bc8d2678960643f8b928882ee37376ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca06f43504a84787b37e3c0eeb78df20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e01ea05873f84893be5a71933af6ddc7",
        "IPY_MODEL_ffe9a0c9174a4a48a44aac35fe1f2681",
        "IPY_MODEL_416889374c394fd5badf5bbd021fa498"
       ],
       "layout": "IPY_MODEL_11ba3208f8d74d62abcf0c53090e3ee1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d0f08215510d4623a17c969ec1c2c61b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e01ea05873f84893be5a71933af6ddc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7560d8a1be4f4abfae64b18115f52a76",
       "placeholder": "​",
       "style": "IPY_MODEL_4fb8d310a8f144ba932ba90a803c2407",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "f503377e9c904dd690e53737edb759f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7d6aa2aa38fc435fbee6fcbab90c5cb3",
       "placeholder": "​",
       "style": "IPY_MODEL_977ad3c508f84ab8aaaeec6745580e0f",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "ffe9a0c9174a4a48a44aac35fe1f2681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bc8d2678960643f8b928882ee37376ae",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6a1ae2eeb26a4c27a0e26a433d0c338a",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
