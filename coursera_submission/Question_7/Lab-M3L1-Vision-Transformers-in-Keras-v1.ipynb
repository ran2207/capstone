{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3057f4b9-bfce-4093-86a6-b8fb9b4702c5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e3d7-707a-4548-8f09-c7bd5bbfd619",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Vision Transformers Using Keras </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c1104-b4b0-427a-befb-1b205b14486c",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38e483-8620-422f-a38b-4b5fe1244513",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn to build a CNN-Vision Transformer (ViT) hybrid image classification model. You will start by loading an existing CNN that is good at recognizing small patterns in pictures. Then, you'll learn how to improve it with a transformer, which helps the model see and use wider and more complex relationships in an image. The notebook covers important topics like preparing your image data, making your model smarter with both local and global learning, and saving your best results automatically. By the end, you'll understand how CNN-ViT hybrid models work and how to train, evaluate, and visualize them for any image classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom-trained Keras CNN model to extract feature maps and feed them into a ViT architecture.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom-trained CNN model\n",
    "2. Extract feature maps from the CNN\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the combined model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08864f02",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. [ Custom positional embedding layer](#Custom-positional-embedding-layer)\n",
    "3. [Transformer block implementation](#Transformer-block-implementation)\n",
    "4. [Hybrid model builder function](#Hybrid-model-builder-function)\n",
    "5. [Model loading and setup](#Model-loading-and-setup)\n",
    "6. [Data generator configuration](#Data-generator-configuration)\n",
    "7. [Model checkpoint setup](#Model-checkpoint-setup)\n",
    "8. [Model training and compilation](#Model-training-and-compilation)\n",
    "9. [Model shape validation](#Model-shape-validation)\n",
    "10. [Training results visualization](#Training-results-visualization)\n",
    "\n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "Let's begin by downloading the dataset to evaluate the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from which the dataset would be downloaded\n",
    "2. The dataset downloading primary function, based on the `skillsnetwork` library\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f0820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:48.175159Z",
     "iopub.status.busy": "2025-10-27T07:50:48.174694Z",
     "iopub.status.idle": "2025-10-27T07:50:54.153761Z",
     "shell.execute_reply": "2025-10-27T07:50:54.153374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399bd837afcb4c91999aba94c85bd8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a5da95836f4fc0943ba3b49a7118c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7ca672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:54.154901Z",
     "iopub.status.busy": "2025-10-27T07:50:54.154812Z",
     "iopub.status.idle": "2025-10-27T07:50:55.801737Z",
     "shell.execute_reply": "2025-10-27T07:50:55.800990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 ms, sys: 9.1 ms, total: 13.2 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install Tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66b191c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:55.803967Z",
     "iopub.status.busy": "2025-10-27T07:50:55.803797Z",
     "iopub.status.idle": "2025-10-27T07:50:56.311826Z",
     "shell.execute_reply": "2025-10-27T07:50:56.311193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.19 (from versions: 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No matching distribution found for tensorflow==2.19\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 1.92 ms, sys: 3.56 ms, total: 5.48 ms\n",
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461a6d4-57ea-4431-bdc9-c30e436999f0",
   "metadata": {},
   "source": [
    "### Install SkLearn ML library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c5eb16-6bee-4ca9-bb01-271f0f708743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:56.313800Z",
     "iopub.status.busy": "2025-10-27T07:50:56.313650Z",
     "iopub.status.idle": "2025-10-27T07:50:56.825480Z",
     "shell.execute_reply": "2025-10-27T07:50:56.824835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in ./venv/lib/python3.13/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (2.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.16.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 1.88 ms, sys: 3.73 ms, total: 5.61 ms\n",
      "Wall time: 509 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation and visualization, and suppress warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0fcdf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:56.827119Z",
     "iopub.status.busy": "2025-10-27T07:50:56.826979Z",
     "iopub.status.idle": "2025-10-27T07:50:57.032766Z",
     "shell.execute_reply": "2025-10-27T07:50:57.032444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 141 ms, sys: 19.2 ms, total: 160 ms"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "Sets environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. Detects GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99804321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:57.033974Z",
     "iopub.status.busy": "2025-10-27T07:50:57.033871Z",
     "iopub.status.idle": "2025-10-27T07:50:59.682377Z",
     "shell.execute_reply": "2025-10-27T07:50:59.681896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 2.04 s, sys: 219 ms, total: 2.26 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e211b54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.683618Z",
     "iopub.status.busy": "2025-10-27T07:50:59.683471Z",
     "iopub.status.idle": "2025-10-27T07:50:59.685737Z",
     "shell.execute_reply": "2025-10-27T07:50:59.685431Z"
    }
   },
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570be57-93e3-49cd-9832-4979909cbeab",
   "metadata": {},
   "source": [
    "## Lab layout\n",
    "1. You will start by loading a pre-trained Convolutional Neural Network (CNN) to act as a robust feature extractor for your image dataset.\n",
    "2. After loading your CNN, you’ll select an intermediate feature map and reshape it into a sequence of tokens, getting your data ready for transformer-based learning.\n",
    "3. You’ll add custom positional embeddings to your tokens so that the model can retain the original spatial structure of your images, even after the features have been flattened.\n",
    "4. Next, you implement a Vision Transformer (ViT) encoder by stacking several transformer blocks, allowing your model to learn global relationships and context throughout the image.\n",
    "5. You’ll combine the CNN and ViT encoder into a single, hybrid model so that you can leverage both the local feature extraction power of CNNs and the global attention mechanism of transformers.\n",
    "6. When preparing your dataset, you’ll use Keras’s ImageDataGenerator to handle data augmentation and to properly encode your labels for multi-class image classification.\n",
    "7. You’ll set up a model checkpoint callback, letting your model automatically save its best weights whenever validation accuracy improves during training, so you always keep the most effective model.\n",
    "8. To ensure everything works smoothly, you’ll check the input and output shapes, which helps you catch architectural mistakes early.\n",
    "9. As your model trains, you’ll visualize both training and validation accuracy and loss, which will help you monitor performance and spot signs of overfitting or underfitting.\n",
    "10. Throughout the process, you can follow clear explanations in the notebook, making it easy for you to understand how each component—from CNN to transformer—is integrated to achieve stronger image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be04a5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.686852Z",
     "iopub.status.busy": "2025-10-27T07:50:59.686787Z",
     "iopub.status.idle": "2025-10-27T07:50:59.688702Z",
     "shell.execute_reply": "2025-10-27T07:50:59.688401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai-capstone-keras-best-model-model_downloaded.keras\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c801ed-dedc-4148-9c1f-6dd432d91093",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c483241b-9792-44e9-967f-2df9a6ff2b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.689729Z",
     "iopub.status.busy": "2025-10-27T07:50:59.689665Z",
     "iopub.status.idle": "2025-10-27T07:50:59.691388Z",
     "shell.execute_reply": "2025-10-27T07:50:59.690984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b3d89",
   "metadata": {},
   "source": [
    "## Model loading and setup\n",
    "\n",
    "Here, you will load a pre-trained CNN model and learn to work with saved Keras models and prepare them for use in the hybrid architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a12a8-aafb-48ec-bd8a-bb90073fd971",
   "metadata": {},
   "source": [
    "## Task 1: Load the pre-trained CNN model in `cnn_model` variable using `load_model()` function and print model summary using `summary()` method\n",
    "\n",
    "The `load_model()` function loads the complete Keras model, including architecture, weights, and compilation state. The loaded model serves as the CNN backbone for feature extraction in the hybrid architecture. The `cnn_model.summary()` line can be uncommented to inspect the model architecture and identify appropriate layers for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f9f5e1-b775-45e1-8bf7-ee0856b5a4c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.692333Z",
     "iopub.status.busy": "2025-10-27T07:50:59.692275Z",
     "iopub.status.idle": "2025-10-27T07:50:59.693738Z",
     "shell.execute_reply": "2025-10-27T07:50:59.693447Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccbcff-9fd5-4bb3-bc6a-cf00e3339009",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "cnn_model = load_model(keras_model_path) # Loading the CNN model\n",
    "\n",
    "cnn_model.summary() # Display model summary\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc551ed-bce9-4b1e-b615-ff5c608df8fa",
   "metadata": {},
   "source": [
    "## Task 2: Based on `model.summary()`, get the name of the layer from the CNN model for feature extraction in the variable `feature_layer_name`\n",
    "\n",
    "This is the last convolutional layer, usually before `GlobalAveragePooling2D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a6ab36-c62a-4f2e-906f-761282cd788d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.694702Z",
     "iopub.status.busy": "2025-10-27T07:50:59.694630Z",
     "iopub.status.idle": "2025-10-27T07:50:59.696196Z",
     "shell.execute_reply": "2025-10-27T07:50:59.695814Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb3c91-32d8-4f89-9be1-9a5b3997b3e4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "feature_layer_name = \"batch_normalization_5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbea4d3",
   "metadata": {},
   "source": [
    "## Custom positional embedding layer\n",
    "\n",
    "In this cell, you define a custom Keras layer called `AddPositionEmbedding` that implements positional embeddings for the Vision Transformer architecture. Positional embeddings are crucial in transformer models because they lack inherent spatial awareness, unlike convolutional layers that have built-in spatial inductive biases.\n",
    "\n",
    "- The class inherits from `layers.Layer`, making it a proper Keras custom layer\n",
    "- In the `__init__` method, it creates a trainable weight matrix using `self.add_weight()`\n",
    "- The positional embedding has shape `(1, num_patches, embed_dim)` where the first dimension allows broadcasting across batch sizes\n",
    "- The `initializer=\"random_normal\"` ensures the embeddings start with random values that will be learned during training\n",
    "- The `trainable=True` parameter makes these embeddings learnable parameters\n",
    "\n",
    "\n",
    "This layer is essential for the hybrid CNN-ViT architecture because when CNN feature maps are flattened into tokens, spatial relationships are lost. The positional embeddings restore spatial awareness by providing each token with information about its original spatial location in the feature map. This allows the transformer to understand which tokens are spatially adjacent or distant, enabling it to make spatially aware attention decisions.\n",
    "\n",
    "The `call` method adds the positional embeddings to the input tokens using element-wise addition. This is computationally efficient and follows the standard transformer approach, where positional information is added to preserve the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf233bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.697237Z",
     "iopub.status.busy": "2025-10-27T07:50:59.697177Z",
     "iopub.status.idle": "2025-10-27T07:50:59.699540Z",
     "shell.execute_reply": "2025-10-27T07:50:59.699188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Positional embedding that Keras can track\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class AddPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.pos = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, tokens):\n",
    "        return tokens + self.pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"embed_dim\":   self.embed_dim,\n",
    "        })\n",
    "        return {**config}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8388357",
   "metadata": {},
   "source": [
    "## Transformer block implementation\n",
    "\n",
    "In this code cell, you will implement a complete transformer encoder block, the fundamental building block of the Vision Transformer architecture. The `TransformerBlock` class encapsulates the standard transformer encoder architecture with multi-head self-attention and feed-forward neural network components.\n",
    "\n",
    "**Role in hybrid architecture:**\n",
    "In the CNN-ViT hybrid, these transformer blocks process the tokenized CNN feature maps, allowing the model to capture long-range spatial dependencies that CNNs might miss due to their limited receptive fields. The self-attention mechanism enables each spatial location to attend to all other locations, providing global context awareness.\n",
    "\n",
    "**Technical architecture:**\n",
    "- **Multi-head attention (MHA):** Uses `layers.MultiHeadAttention` with a configurable number of heads and key dimension equal to embed_dim\n",
    "- **Layer normalization:** Two `LayerNormalization` layers with epsilon=1e-6 for numerical stability\n",
    "- **MLP block:** A two-layer feed-forward network with GELU activation and dropout for regularization\n",
    "- **Residual connections:** Implements skip connections around both the attention and MLP blocks\n",
    "\n",
    "**Parameters:**\n",
    "- `embed_dim`: The dimensionality of token embeddings (typically matches CNN feature map channels)\n",
    "- `num_heads`: Number of attention heads (default 8, must divide embed_dim evenly)\n",
    "- `mlp_dim`: Hidden dimension of the MLP block (typically 4x embed_dim)\n",
    "- `dropout`: Dropout rate for regularization (default 0.1)\n",
    "\n",
    "**Forward pass logic:**\n",
    "Forward pass allows the model to capture both local and global dependencies in the feature representations while maintaining gradient flow through residual connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727fb05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.700518Z",
     "iopub.status.busy": "2025-10-27T07:50:59.700457Z",
     "iopub.status.idle": "2025-10-27T07:50:59.703037Z",
     "shell.execute_reply": "2025-10-27T07:50:59.702664Z"
    }
   },
   "outputs": [],
   "source": [
    "# One Transformer encoder block\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.dropout   = dropout\n",
    "        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.norm1(x + self.mha(x, x))\n",
    "        return self.norm2(x + self.mlp(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":  self.embed_dim,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"mlp_dim\":    self.mlp_dim,\n",
    "            \"dropout\":    self.dropout,\n",
    "        })\n",
    "        return {**config}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff6478",
   "metadata": {},
   "source": [
    "## Hybrid model builder function\n",
    "\n",
    "Now, you will define a function `build_cnn_vit_hybrid` that constructs the complete hybrid CNN-Vision Transformer model. This function represents the main architectural innovation of the notebook, combining the strengths of convolutional neural networks for local feature extraction with transformers for global context modeling.\n",
    "\n",
    "**Function architecture:**\n",
    "1. **CNN feature extraction:** Extracts intermediate feature maps from a pre-trained CNN at a specified layer\n",
    "2. **Tokenization:** Reshapes spatial feature maps into a sequence of tokens suitable for transformer processing\n",
    "3. **Positional encoding:** Adds learnable positional embeddings to maintain spatial relationships\n",
    "4. **Transformer stack:** Applies multiple transformer encoder blocks for global context modeling\n",
    "5. **Classification head:** Pools tokens and applies the final classification layer\n",
    "\n",
    "**Parameters:**\n",
    "- `cnn_model`: Pre-trained CNN model for feature extraction\n",
    "- `feature_layer_name`: Name of the CNN layer to extract features from (e.g., `batch_normalization_5` in the original model architecture)\n",
    "- `num_transformer_layers`: Number of transformer blocks to stack (default 4)\n",
    "- `num_heads`: Number of attention heads per transformer block (default 8)\n",
    "- `mlp_dim`: MLP hidden dimension in transformer blocks (default 2048)\n",
    "- `num_classes`: Number of output classes for classification\n",
    "\n",
    "The function first freezes the CNN backbone (`cnn_model.trainable = False`) to use it as a fixed feature extractor. It then extracts feature maps with shape (B, H, W, C) and reshapes them to (B, H*W, C), where each spatial location becomes a token. The `AddPositionEmbedding` layer adds spatial awareness, and multiple TransformerBlock layers process the tokens. Finally, `GlobalAveragePooling1D` aggregates all tokens, and a dense layer with softmax activation produces class predictions.\n",
    "\n",
    "This hybrid approach leverages CNN's local feature detection capabilities while adding the transformer's global attention mechanism. The result is a model that can capture both fine-grained local patterns and long-range spatial dependencies, potentially **outperforming pure CNN** or pure transformer approaches on vision tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2598f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.703968Z",
     "iopub.status.busy": "2025-10-27T07:50:59.703913Z",
     "iopub.status.idle": "2025-10-27T07:50:59.706116Z",
     "shell.execute_reply": "2025-10-27T07:50:59.705777Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=2):\n",
    "    # 1. Freeze or fine-tune the CNN as you prefer\n",
    "    cnn_model.trainable = False      # set True to fine-tune\n",
    "    \n",
    "    # 2. Feature extractor up to the chosen layer\n",
    "    features = cnn_model.get_layer(feature_layer_name).output\n",
    "    H, W, C = features.shape[1], features.shape[2], features.shape[3]\n",
    "    \n",
    "    # 3. Flatten spatial grid → tokens  &  add positional encoding\n",
    "    x = layers.Reshape((H * W, C))(features) \n",
    "    x = AddPositionEmbedding(H * W, C)(x)\n",
    "\n",
    "    # 4. Stack ViT encoder blocks\n",
    "    for _ in range(num_transformer_layers):\n",
    "        x = TransformerBlock(C, num_heads, mlp_dim)(x)\n",
    "\n",
    "    # 5. Token pooling & classification head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(cnn_model.layers[0].input, outputs, name=\"CNN_ViT_hybrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c3ff0",
   "metadata": {},
   "source": [
    "## Data generator configuration\n",
    "\n",
    "Now you will set up data preprocessing and augmentation pipeline using Keras' `ImageDataGenerator`.\n",
    "\n",
    "**Data configuration parameters:**\n",
    "- `img_w, img_h = 64, 64`: Input image dimensions (64x64 pixels)\n",
    "- `n_channels = 3`: RGB color channels\n",
    "- `batch_size = 128`: Number of samples per training batch\n",
    "- `num_classes = 2`: Binary classification setup\n",
    "\n",
    "**Generators:**\n",
    "Two separate generators are created:\n",
    "1. `train_gen`: Training data with augmentation and shuffling\n",
    "2. `val_gen`: Validation data with the same preprocessing but a different subset\n",
    "\n",
    "Both generators use `class_mode=\"categorical\"` for one-hot encoded labels, `target_size=(64,64)` for consistent input dimensions, and `shuffle=True` for randomized batch sampling.\n",
    "\n",
    "This augmentation strategy significantly increases the effective dataset size and helps prevent overfitting by exposing the model to varied versions of the same images. The validation split ensures proper model evaluation on unseen data, while the categorical class mode prepares labels for softmax classification in the hybrid model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc8c443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.707153Z",
     "iopub.status.busy": "2025-10-27T07:50:59.707075Z",
     "iopub.status.idle": "2025-10-27T07:50:59.801284Z",
     "shell.execute_reply": "2025-10-27T07:50:59.800835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n",
      "Found 4800 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )\n",
    "\n",
    "train_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                        target_size = (img_w, img_h),\n",
    "                                        batch_size= batch_size,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        subset=\"training\",\n",
    "                                        shuffle=True\n",
    "                                       )\n",
    "\n",
    "val_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                      target_size =(img_w, img_h),\n",
    "                                      batch_size = batch_size, \n",
    "                                      class_mode=\"categorical\",\n",
    "                                      subset=\"validation\",\n",
    "                                      shuffle=True\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20ce4",
   "metadata": {},
   "source": [
    "## Model checkpoint setup\n",
    "\n",
    "This code cell configures a `ModelCheckpoint` callback for saving the best model weights during training. Model checkpointing is a crucial practice in deep learning that prevents loss of training progress and enables recovery of the best-performing model state.\n",
    "\n",
    "**Checkpoint configuration:**\n",
    "- `filepath`: Specifies the file path and name for saving weights\n",
    "- `save_weights_only=True`: Saves only model weights, not the full model architecture (more efficient and avoids serialization issues)\n",
    "- `monitor='val_accuracy'`: Tracks validation accuracy as the metric for determining the \"best\" model\n",
    "- `mode='max'`: Indicates that higher validation accuracy values are better (use 'min' for loss metrics)\n",
    "- `save_best_only=True`: Only saves the model when validation accuracy improves, preventing storage of worse-performing checkpoints\n",
    "- `verbose=1`: Provides console output when a checkpoint is saved\n",
    "\n",
    "The checkpoint callback addresses several important training considerations:\n",
    "1. **Overfitting prevention:** Captures the model state at peak validation performance before overfitting occurs\n",
    "2. **Storage efficiency:** Saving weights only reduces file size compared to full model serialization\n",
    "3. **Automatic model saving:** Eliminates manual monitoring by automatically saving the best-performing epoch\n",
    "\n",
    "**Integration with training:**\n",
    "This callback will be passed to the `model.fit()` method, where it will monitor validation accuracy after each epoch. When validation accuracy improves, the callback saves the current model weights to the specified file. This ensures that even if training continues past the optimal point, the best-performing weights are preserved.\n",
    "\n",
    "**File naming convention:**\n",
    "The filename uses the `.model.keras` extension to indicate it contains the full model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30bfce4-40cf-4cbc-87ca-a3ffe0c176e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.802342Z",
     "iopub.status.busy": "2025-10-27T07:50:59.802265Z",
     "iopub.status.idle": "2025-10-27T07:50:59.804097Z",
     "shell.execute_reply": "2025-10-27T07:50:59.803737Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomPrintCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Print epoch number and starting time\n",
    "        print(f\"Epoch {(epoch + 1):02d} completed on {present_time()}\")\n",
    "time_print_callback = CustomPrintCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a6d3120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.804966Z",
     "iopub.status.busy": "2025-10-27T07:50:59.804904Z",
     "iopub.status.idle": "2025-10-27T07:50:59.806649Z",
     "shell.execute_reply": "2025-10-27T07:50:59.806350Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"keras_cnn_vit.model.keras\"\n",
    "# Save only weights to overcome the serialization issues with the hybrid model. The full model can be saved using the model architecture and weights.\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                save_weights_only=False,  # Set to True to save only weights\n",
    "                                monitor='val_loss',      # or 'val_accuracy', 'val_loss'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                                \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d310c72",
   "metadata": {},
   "source": [
    "## Model training and compilation\n",
    "\n",
    "Now, you will set up the core training pipeline, where the hybrid CNN-ViT model is built, compiled, and trained. This is the complete workflow from model instantiation to training execution with proper configuration for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596da50-d549-4f2a-a4b1-152291141de4",
   "metadata": {},
   "source": [
    "## Task 3: Define the model architecture in a variable named `hybrid_model` using the `build_cnn_vit_hybrid` function\n",
    "You may use the following parameters:\n",
    "\n",
    "- feature_layer_name: feature_layer_name\n",
    "- num_transformer_layers: 4\n",
    "- attention heads: 8\n",
    "- mlp dimension: 2048\n",
    "- num_classes: extract from training data generator (train_gen.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5abd941-a279-43bf-b4e7-0a89ffc922a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.807718Z",
     "iopub.status.busy": "2025-10-27T07:50:59.807645Z",
     "iopub.status.idle": "2025-10-27T07:50:59.809141Z",
     "shell.execute_reply": "2025-10-27T07:50:59.808835Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe987db-3731-4866-87d4-69c440feffca",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "num_classes = train_gen.num_classes\n",
    "hybrid_model = build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name=feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=train_gen.num_classes)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f907b-ec3b-49b8-ae94-c4944bad334d",
   "metadata": {},
   "source": [
    "## Task 4: Compile the model `hybrid_model` \n",
    "\n",
    "You may use the following parameters:\n",
    "- `optimizer=tf.keras.optimizers.Adam`\n",
    "- `learning rate: 0.0001`\n",
    "- `loss: categorical_crossentropy`\n",
    "- `metrics: accuracy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43445bc3-e606-4efd-9875-59d8578d69f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.810076Z",
     "iopub.status.busy": "2025-10-27T07:50:59.810015Z",
     "iopub.status.idle": "2025-10-27T07:50:59.811480Z",
     "shell.execute_reply": "2025-10-27T07:50:59.811183Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191748d2-897b-44c8-993e-e5f1697d98f3",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "hybrid_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                     loss=\"categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"],\n",
    "                    )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c5d3f",
   "metadata": {},
   "source": [
    "## Model shape validation\n",
    "\n",
    "This code cell performs the validation step to ensure the hybrid model produces outputs with the correct shape and dimensions. Shape validation is essential in deep learning to catch architectural errors early and verify that the model will work correctly with the expected input and output formats.\n",
    "\n",
    "**Validation process:**\n",
    "The cell creates a dummy input tensor using `tf.random.normal([1, img_w, img_h, n_channels])`, which generates random values with the same shape as actual input images:\n",
    "- Batch size: 1 (single sample for testing)\n",
    "- Width: `img_w` (64 pixels)\n",
    "- Height: `img_h` (64 pixels)\n",
    "- Channels: `n_channels` (3 for RGB)\n",
    "\n",
    "**Output verification:**\n",
    "The dummy input is passed through the hybrid model (`hybrid_model(dummy)`) to generate predictions. The expected output shape should be `(1, num_classes)` where:\n",
    "- First dimension (1): Batch size\n",
    "- Second dimension (`num_classes`): Number of classification classes\n",
    "\n",
    "**Technical benefits:**\n",
    "This validation step serves multiple purposes:\n",
    "1. **Architecture verification:** Confirms that all layers are properly connected and compatible\n",
    "2. **Dimension checking:** Ensures the model produces the expected output shape for classification\n",
    "3. **Early error detection:** Catches shape mismatches before actual training or inference\n",
    "4. **Model readiness:** Verifies the model is ready for production use\n",
    "\n",
    "**Importance:**\n",
    "If the output shape doesn't match expectations, it indicates potential issues in the hybrid architecture, such as incorrect reshaping operations, wrong number of classes configuration, or problems in the CNN-to-transformer transition. This simple test can save significant debugging time by catching architectural issues immediately after model construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22cbcd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.812519Z",
     "iopub.status.busy": "2025-10-27T07:50:59.812448Z",
     "iopub.status.idle": "2025-10-27T07:50:59.985269Z",
     "shell.execute_reply": "2025-10-27T07:50:59.984923Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hybrid_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensure end-to-end shapes line up\u001b[39;00m\n\u001b[32m      2\u001b[39m dummy = tf.random.normal([\u001b[32m1\u001b[39m, img_w, img_h, n_channels])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pred  = \u001b[43mhybrid_model\u001b[49m(dummy)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLogits shape:\u001b[39m\u001b[33m\"\u001b[39m, pred.shape)   \u001b[38;5;66;03m# should be (1, num_classes)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'hybrid_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure end-to-end shapes line up\n",
    "dummy = tf.random.normal([1, img_w, img_h, n_channels])\n",
    "pred  = hybrid_model(dummy)\n",
    "print(\"Logits shape:\", pred.shape)   # should be (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187a326-5472-4ade-b14d-3baf9992ae8a",
   "metadata": {},
   "source": [
    "## Task 5: Define the training configuration of the `hybrid_model`.\n",
    "In the interest of time, you can train for 3 epochs.\n",
    "Use the `checkpoint_cb` callback keyword for automatic saving of the best model state. \n",
    "\n",
    "To make sure that the computational resources are not overloaded, we will limit the number of batches used for training in each epoch. This can be done by **`steps_per_epoch`**. \n",
    "\n",
    "For this task use  **`steps_per_epoch = 128`**\n",
    "\n",
    "Feel free to play with these parameters if you are executing this on your local machine or any other platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccf54504-7295-4db0-bd24-654b05a931c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.986435Z",
     "iopub.status.busy": "2025-10-27T07:50:59.986370Z",
     "iopub.status.idle": "2025-10-27T07:50:59.987976Z",
     "shell.execute_reply": "2025-10-27T07:50:59.987608Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcb41b-5739-47e9-817d-b8ea7a32b7ff",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fit = hybrid_model.fit(train_gen,\n",
    "                       epochs=3,\n",
    "                       validation_data=val_gen,\n",
    "                       callbacks=[checkpoint_cb],\n",
    "                       steps_per_epoch = 128\n",
    "                        )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e06e06",
   "metadata": {},
   "source": [
    "## Training results visualization\n",
    "\n",
    "This code cell creates comprehensive visualizations of the training process by plotting both accuracy and loss curves. Visualization of training metrics is essential for understanding model performance, diagnosing training issues, and making informed decisions about model optimization.\n",
    "\n",
    "**Visualization setup:**\n",
    "The cell uses matplotlib to create two separate plots with consistent styling:\n",
    "- `fig_w, fig_h`: Sets figure dimensions for compact, readable plots\n",
    "- `plt.subplots(figsize=(fig_w, fig_h))`: Creates a subplot with specified dimensions\n",
    "\n",
    "**Accuracy plot analysis:**\n",
    "The first plot displays training and validation accuracy over epochs:\n",
    "- `fit.history['accuracy']`: Training accuracy progression\n",
    "- `fit.history['val_accuracy']`: Validation accuracy progression\n",
    "\n",
    "**Loss plot analysis:**\n",
    "The second plot shows training and validation loss curves:\n",
    "- `fit.history['loss']`: Training loss progression\n",
    "- `fit.history['val_loss']`: Validation loss progression\n",
    "\n",
    "***Importance:***\n",
    "These plots enable several important analyses:\n",
    "1. **Overfitting detection:** Diverging training and validation curves indicate overfitting\n",
    "2. **Convergence assessment:** Plateauing curves suggest training completion\n",
    "3. **Learning rate evaluation:** Oscillating curves may indicate learning rate issues\n",
    "4. **Model performance:** Final accuracy and loss values indicate overall model quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c26fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:59.988931Z",
     "iopub.status.busy": "2025-10-27T07:50:59.988860Z",
     "iopub.status.idle": "2025-10-27T07:51:00.041640Z",
     "shell.execute_reply": "2025-10-27T07:51:00.041218Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Plot Accuracy on the first subplot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m axs.plot(\u001b[43mfit\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTraining Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m axs.plot(fit.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mValidation Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m axs.set_title(\u001b[33m'\u001b[39m\u001b[33mModel Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig_w, fig_h = 4,3\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a580f2-7f09-4392-a807-b53419d4c8b1",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "You have successfully trained the ViT model for classification of agricultural land from satellite imagery using **Keras**\n",
    "In this lab, in the interest of time, you have trained the model for 3-5 epochs. However, usually you need to train the model for around 15-20 epochs, depending on the quality of training data and model metrics based on validation. \n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `checkpoint_cb` callback function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model weights: `keras_cnn_vit.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- In conjunction with the model architecture, these model weights can be used in other labs of this AI capstone course, instead of the weights provided at the above link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe9d4e-2621-49cd-9118-df88597f36ce",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a CNN-ViT hybrid image classification model.\n",
    "In this lab, you learnt how to combine a convolutional neural network (CNN) with a Vision Transformer (ViT) for advanced image classification tasks. Starting from a pre-trained CNN, you learnt how to extract intermediate features, reshape them as tokens, and provide them with positional embeddings. By stacking transformer encoder blocks on top, the model benefits from both local detail extraction and global context awareness. Throughout the lab, techniques for robust data preparation, efficient training with model checkpoints, and effective visualization of performance were covered. By completing the steps in this notebook, you now have hands-on experience implementing and evaluating a contemporary hybrid vision model using Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89c2b8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:00.043070Z",
     "iopub.status.busy": "2025-10-27T07:51:00.042985Z",
     "iopub.status.idle": "2025-10-27T07:51:00.047115Z",
     "shell.execute_reply": "2025-10-27T07:51:00.046742Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (526037183.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 3 Lab 1: Vision Transformers in KerasSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M3L1-Vision-Transformers-in-Keras-v1.ipynb\"\"\"import osimport tensorflow as tffrom tensorflow.keras import layersfrom tensorflow.keras.models import Model, load_modelfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpointfrom tensorflow.keras.optimizers import Adamimport matplotlib.pyplot as plt# =============================================================================# TASK 1: Load pre-trained CNN model and display summary# =============================================================================# Path to pre-trained Keras CNN modelkeras_model_path = 'ai-capstone-keras-best-model-model_downloaded.keras'# TASK 1 ANSWER:cnn_model = load_model(keras_model_path)  # Load the CNN model# Display model summary (uncomment to see architecture)cnn_model.summary()print(f\"Loaded CNN model from: {keras_model_path}\")print(f\"Model has {len(cnn_model.layers)} layers\")print(\"\\nNow examine the summary to identify the feature extraction layer...\")# =============================================================================# TASK 2: Identify feature layer name for feature extraction# =============================================================================# Based on the model summary from Task 1, identify the last convolutional# layer before GlobalAveragePooling2D# TASK 2 ANSWER:feature_layer_name = \"batch_normalization_5\"print(f\"\\nTASK 2: Feature extraction layer: {feature_layer_name}\")print(\"This is the last batch normalization layer before pooling.\")# =============================================================================# CUSTOM KERAS LAYERS FOR ViT# =============================================================================@tf.keras.utils.register_keras_serializable(package=\"Custom\")class AddPositionEmbedding(layers.Layer):    \"\"\"    Adds learnable positional embeddings to token sequences    \"\"\"    def __init__(self, num_patches, embed_dim, **kwargs):        super().__init__(**kwargs)        self.num_patches = num_patches        self.embed_dim   = embed_dim        self.pos = self.add_weight(            name=\"pos_embedding\",            shape=(1, num_patches, embed_dim),            initializer=\"random_normal\",            trainable=True        )    def call(self, tokens):        return tokens + self.pos    def get_config(self):        config = super().get_config()        config.update({            \"num_patches\": self.num_patches,            \"embed_dim\":   self.embed_dim,        })        return config@tf.keras.utils.register_keras_serializable(package=\"Custom\")class TransformerBlock(layers.Layer):    \"\"\"    Vision Transformer encoder block with multi-head attention and MLP    \"\"\"    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):        super().__init__(**kwargs)        self.embed_dim = embed_dim        self.num_heads = num_heads        self.mlp_dim   = mlp_dim        self.dropout   = dropout        # Multi-head attention        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)        # Layer normalization        self.norm1 = layers.LayerNormalization(epsilon=1e-6)        self.norm2 = layers.LayerNormalization(epsilon=1e-6)        # MLP block        self.mlp = tf.keras.Sequential([            layers.Dense(mlp_dim, activation=\"gelu\"),            layers.Dropout(dropout),            layers.Dense(embed_dim),            layers.Dropout(dropout)        ])    def call(self, x):        # Attention block with residual connection        x = self.norm1(x + self.mha(x, x))        # MLP block with residual connection        return self.norm2(x + self.mlp(x))    def get_config(self):        config = super().get_config()        config.update({            \"embed_dim\":  self.embed_dim,            \"num_heads\":  self.num_heads,            \"mlp_dim\":    self.mlp_dim,            \"dropout\":    self.dropout,        })        return config# =============================================================================# HYBRID MODEL BUILDER FUNCTION# =============================================================================def build_cnn_vit_hybrid(cnn_model,                        feature_layer_name,                        num_transformer_layers=4,                        num_heads=8,                        mlp_dim=2048,                        num_classes=2):    \"\"\"    Build CNN-ViT hybrid model    Args:        cnn_model: Pre-trained CNN model        feature_layer_name: Name of layer to extract features from        num_transformer_layers: Number of transformer blocks        num_heads: Number of attention heads        mlp_dim: MLP hidden dimension        num_classes: Number of output classes    Returns:        Hybrid CNN-ViT model    \"\"\"    # Freeze CNN backbone (optional: set to True to fine-tune)    cnn_model.trainable = False    # Extract feature maps from specified layer    features = cnn_model.get_layer(feature_layer_name).output    H, W, C = features.shape[1], features.shape[2], features.shape[3]    # Reshape spatial grid to token sequence    x = layers.Reshape((H * W, C))(features)    # Add positional embeddings    x = AddPositionEmbedding(H * W, C)(x)    # Stack transformer encoder blocks    for _ in range(num_transformer_layers):        x = TransformerBlock(C, num_heads, mlp_dim)(x)    # Global average pooling over tokens    x = layers.GlobalAveragePooling1D()(x)    # Classification head    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)    # Create model    model = Model(cnn_model.layers[0].input, outputs, name=\"CNN_ViT_Hybrid\")    return model# =============================================================================# TASK 3: Define hybrid_model using build_cnn_vit_hybrid function# =============================================================================# Parameters for the hybrid modelnum_classes = 2  # Binary classification (agri vs non-agri)# TASK 3 ANSWER:hybrid_model = build_cnn_vit_hybrid(    cnn_model,    feature_layer_name=feature_layer_name,    num_transformer_layers=4,    num_heads=8,    mlp_dim=2048,    num_classes=num_classes)print(\"\\nTASK 3: Hybrid CNN-ViT model created\")print(f\"  Transformer layers: 4\")print(f\"  Attention heads: 8\")print(f\"  MLP dimension: 2048\")print(f\"  Output classes: {num_classes}\")# Display model summaryprint(\"\\nHybrid Model Architecture:\")hybrid_model.summary()# =============================================================================# TASK 4: Compile the hybrid_model# =============================================================================# TASK 4 ANSWER:hybrid_model.compile(    optimizer=Adam(learning_rate=1e-4),  # Lower learning rate for fine-tuning    loss=\"categorical_crossentropy\",    metrics=[\"accuracy\"])print(\"\\nTASK 4: Model compiled successfully!\")print(\"  Optimizer: Adam (lr=0.0001)\")print(\"  Loss: categorical_crossentropy\")print(\"  Metrics: accuracy\")# =============================================================================# DATA GENERATORS# =============================================================================dataset_path = './images_dataSAT'img_w, img_h = 64, 64batch_size = 4  # Small batch size due to model complexitynum_classes = 2# ImageDataGenerator with augmentationdatagen = ImageDataGenerator(    rescale=1./255,    rotation_range=40,    width_shift_range=0.2,    height_shift_range=0.2,    shear_range=0.2,    zoom_range=0.2,    horizontal_flip=True,    fill_mode=\"nearest\",    validation_split=0.2)# Training generatortrain_gen = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode=\"categorical\",    subset=\"training\",    shuffle=True)# Validation generatorval_gen = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode=\"categorical\",    subset=\"validation\",    shuffle=True)print(f\"\\nData Generators Created:\")print(f\"  Training samples: {train_gen.samples}\")print(f\"  Validation samples: {val_gen.samples}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# MODEL CHECKPOINT# =============================================================================model_name = \"keras_cnn_vit.model.keras\"checkpoint_cb = ModelCheckpoint(    filepath=model_name,    save_weights_only=False,    monitor='val_loss',    mode='min',    save_best_only=True,    verbose=1)print(f\"\\nCheckpoint callback created:\")print(f\"  Save path: {model_name}\")print(f\"  Monitor: val_loss (min)\")# =============================================================================# TASK 5: Train the hybrid model# =============================================================================epochs = 3steps_per_epoch = 128  # Limit steps for computational efficiencyprint(f\"\\nTASK 5: Training hybrid model...\")print(f\"  Epochs: {epochs}\")print(f\"  Steps per epoch: {steps_per_epoch}\")print(\"\\nThis may take 30-60 minutes depending on hardware...\")# TASK 5 ANSWER:fit = hybrid_model.fit(    train_gen,    epochs=epochs,    validation_data=val_gen,    callbacks=[checkpoint_cb],    steps_per_epoch=steps_per_epoch)print(\"\\nTraining completed!\")# =============================================================================# VISUALIZE TRAINING HISTORY# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot Accuracyaxes[0].plot(fit.history['accuracy'], label='Training Accuracy', marker='o')axes[0].plot(fit.history['val_accuracy'], label='Validation Accuracy', marker='s')axes[0].set_title('Keras CNN-ViT Hybrid: Model Accuracy', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Accuracy')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot Lossaxes[1].plot(fit.history['loss'], label='Training Loss', marker='o', color='coral')axes[1].plot(fit.history['val_loss'], label='Validation Loss', marker='s', color='dodgerblue')axes[1].set_title('Keras CNN-ViT Hybrid: Model Loss', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Loss')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('keras_cnn_vit_training_history.png', dpi=300)plt.show()print(\"\\nTraining plots saved as 'keras_cnn_vit_training_history.png'\")# =============================================================================# BONUS: Evaluate model# =============================================================================print(\"\\nEvaluating model on validation set...\")val_loss, val_accuracy = hybrid_model.evaluate(val_gen, verbose=1)print(f\"\\nFinal Validation Results:\")print(f\"  Loss: {val_loss:.4f}\")print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")# =============================================================================# MODEL SAVING# =============================================================================print(\"\\nSaving final model...\")hybrid_model.save('keras_cnn_vit_final.keras')print(\"Model saved as 'keras_cnn_vit_final.keras'\")print(\"\\nTo download the model:\")print(\"  1. Right-click on the file in the file browser\")print(\"  2. Select 'Download'\")print(\"  3. Save to your local machine for submission\")# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 1 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Loaded CNN model from {keras_model_path}\")print(f\"Task 2: Identified feature layer: {feature_layer_name}\")print(f\"Task 3: Built hybrid CNN-ViT model with 4 transformer layers\")print(f\"Task 4: Compiled model with Adam(1e-4) and categorical_crossentropy\")print(f\"Task 5: Trained model for {epochs} epochs with {steps_per_epoch} steps/epoch\")print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 3 Lab 1: Vision Transformers in KerasSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M3L1-Vision-Transformers-in-Keras-v1.ipynb\"\"\"import osimport tensorflow as tffrom tensorflow.keras import layersfrom tensorflow.keras.models import Model, load_modelfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpointfrom tensorflow.keras.optimizers import Adamimport matplotlib.pyplot as plt# =============================================================================# TASK 1: Load pre-trained CNN model and display summary# =============================================================================# Path to pre-trained Keras CNN modelkeras_model_path = 'ai-capstone-keras-best-model-model_downloaded.keras'# TASK 1 ANSWER:cnn_model = load_model(keras_model_path)  # Load the CNN model# Display model summary (uncomment to see architecture)cnn_model.summary()print(f\"Loaded CNN model from: {keras_model_path}\")print(f\"Model has {len(cnn_model.layers)} layers\")print(\"\\nNow examine the summary to identify the feature extraction layer...\")# =============================================================================# TASK 2: Identify feature layer name for feature extraction# =============================================================================# Based on the model summary from Task 1, identify the last convolutional# layer before GlobalAveragePooling2D# TASK 2 ANSWER:feature_layer_name = \"batch_normalization_5\"print(f\"\\nTASK 2: Feature extraction layer: {feature_layer_name}\")print(\"This is the last batch normalization layer before pooling.\")# =============================================================================# CUSTOM KERAS LAYERS FOR ViT# =============================================================================@tf.keras.utils.register_keras_serializable(package=\"Custom\")class AddPositionEmbedding(layers.Layer):    \"\"\"    Adds learnable positional embeddings to token sequences    \"\"\"    def __init__(self, num_patches, embed_dim, **kwargs):        super().__init__(**kwargs)        self.num_patches = num_patches        self.embed_dim   = embed_dim        self.pos = self.add_weight(            name=\"pos_embedding\",            shape=(1, num_patches, embed_dim),            initializer=\"random_normal\",            trainable=True        )    def call(self, tokens):        return tokens + self.pos    def get_config(self):        config = super().get_config()        config.update({            \"num_patches\": self.num_patches,            \"embed_dim\":   self.embed_dim,        })        return config@tf.keras.utils.register_keras_serializable(package=\"Custom\")class TransformerBlock(layers.Layer):    \"\"\"    Vision Transformer encoder block with multi-head attention and MLP    \"\"\"    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):        super().__init__(**kwargs)        self.embed_dim = embed_dim        self.num_heads = num_heads        self.mlp_dim   = mlp_dim        self.dropout   = dropout        # Multi-head attention        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)        # Layer normalization        self.norm1 = layers.LayerNormalization(epsilon=1e-6)        self.norm2 = layers.LayerNormalization(epsilon=1e-6)        # MLP block        self.mlp = tf.keras.Sequential([            layers.Dense(mlp_dim, activation=\"gelu\"),            layers.Dropout(dropout),            layers.Dense(embed_dim),            layers.Dropout(dropout)        ])    def call(self, x):        # Attention block with residual connection        x = self.norm1(x + self.mha(x, x))        # MLP block with residual connection        return self.norm2(x + self.mlp(x))    def get_config(self):        config = super().get_config()        config.update({            \"embed_dim\":  self.embed_dim,            \"num_heads\":  self.num_heads,            \"mlp_dim\":    self.mlp_dim,            \"dropout\":    self.dropout,        })        return config# =============================================================================# HYBRID MODEL BUILDER FUNCTION# =============================================================================def build_cnn_vit_hybrid(cnn_model,                        feature_layer_name,                        num_transformer_layers=4,                        num_heads=8,                        mlp_dim=2048,                        num_classes=2):    \"\"\"    Build CNN-ViT hybrid model    Args:        cnn_model: Pre-trained CNN model        feature_layer_name: Name of layer to extract features from        num_transformer_layers: Number of transformer blocks        num_heads: Number of attention heads        mlp_dim: MLP hidden dimension        num_classes: Number of output classes    Returns:        Hybrid CNN-ViT model    \"\"\"    # Freeze CNN backbone (optional: set to True to fine-tune)    cnn_model.trainable = False    # Extract feature maps from specified layer    features = cnn_model.get_layer(feature_layer_name).output    H, W, C = features.shape[1], features.shape[2], features.shape[3]    # Reshape spatial grid to token sequence    x = layers.Reshape((H * W, C))(features)    # Add positional embeddings    x = AddPositionEmbedding(H * W, C)(x)    # Stack transformer encoder blocks    for _ in range(num_transformer_layers):        x = TransformerBlock(C, num_heads, mlp_dim)(x)    # Global average pooling over tokens    x = layers.GlobalAveragePooling1D()(x)    # Classification head    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)    # Create model    model = Model(cnn_model.layers[0].input, outputs, name=\"CNN_ViT_Hybrid\")    return model# =============================================================================# TASK 3: Define hybrid_model using build_cnn_vit_hybrid function# =============================================================================# Parameters for the hybrid modelnum_classes = 2  # Binary classification (agri vs non-agri)# TASK 3 ANSWER:hybrid_model = build_cnn_vit_hybrid(    cnn_model,    feature_layer_name=feature_layer_name,    num_transformer_layers=4,    num_heads=8,    mlp_dim=2048,    num_classes=num_classes)print(\"\\nTASK 3: Hybrid CNN-ViT model created\")print(f\"  Transformer layers: 4\")print(f\"  Attention heads: 8\")print(f\"  MLP dimension: 2048\")print(f\"  Output classes: {num_classes}\")# Display model summaryprint(\"\\nHybrid Model Architecture:\")hybrid_model.summary()# =============================================================================# TASK 4: Compile the hybrid_model# =============================================================================# TASK 4 ANSWER:hybrid_model.compile(    optimizer=Adam(learning_rate=1e-4),  # Lower learning rate for fine-tuning    loss=\"categorical_crossentropy\",    metrics=[\"accuracy\"])print(\"\\nTASK 4: Model compiled successfully!\")print(\"  Optimizer: Adam (lr=0.0001)\")print(\"  Loss: categorical_crossentropy\")print(\"  Metrics: accuracy\")# =============================================================================# DATA GENERATORS# =============================================================================dataset_path = './images_dataSAT'img_w, img_h = 64, 64batch_size = 4  # Small batch size due to model complexitynum_classes = 2# ImageDataGenerator with augmentationdatagen = ImageDataGenerator(    rescale=1./255,    rotation_range=40,    width_shift_range=0.2,    height_shift_range=0.2,    shear_range=0.2,    zoom_range=0.2,    horizontal_flip=True,    fill_mode=\"nearest\",    validation_split=0.2)# Training generatortrain_gen = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode=\"categorical\",    subset=\"training\",    shuffle=True)# Validation generatorval_gen = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode=\"categorical\",    subset=\"validation\",    shuffle=True)print(f\"\\nData Generators Created:\")print(f\"  Training samples: {train_gen.samples}\")print(f\"  Validation samples: {val_gen.samples}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# MODEL CHECKPOINT# =============================================================================model_name = \"keras_cnn_vit.model.keras\"checkpoint_cb = ModelCheckpoint(    filepath=model_name,    save_weights_only=False,    monitor='val_loss',    mode='min',    save_best_only=True,    verbose=1)print(f\"\\nCheckpoint callback created:\")print(f\"  Save path: {model_name}\")print(f\"  Monitor: val_loss (min)\")# =============================================================================# TASK 5: Train the hybrid model# =============================================================================epochs = 3steps_per_epoch = 128  # Limit steps for computational efficiencyprint(f\"\\nTASK 5: Training hybrid model...\")print(f\"  Epochs: {epochs}\")print(f\"  Steps per epoch: {steps_per_epoch}\")print(\"\\nThis may take 30-60 minutes depending on hardware...\")# TASK 5 ANSWER:fit = hybrid_model.fit(    train_gen,    epochs=epochs,    validation_data=val_gen,    callbacks=[checkpoint_cb],    steps_per_epoch=steps_per_epoch)print(\"\\nTraining completed!\")# =============================================================================# VISUALIZE TRAINING HISTORY# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot Accuracyaxes[0].plot(fit.history['accuracy'], label='Training Accuracy', marker='o')axes[0].plot(fit.history['val_accuracy'], label='Validation Accuracy', marker='s')axes[0].set_title('Keras CNN-ViT Hybrid: Model Accuracy', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Accuracy')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot Lossaxes[1].plot(fit.history['loss'], label='Training Loss', marker='o', color='coral')axes[1].plot(fit.history['val_loss'], label='Validation Loss', marker='s', color='dodgerblue')axes[1].set_title('Keras CNN-ViT Hybrid: Model Loss', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Loss')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('keras_cnn_vit_training_history.png', dpi=300)plt.show()print(\"\\nTraining plots saved as 'keras_cnn_vit_training_history.png'\")# =============================================================================# BONUS: Evaluate model# =============================================================================print(\"\\nEvaluating model on validation set...\")val_loss, val_accuracy = hybrid_model.evaluate(val_gen, verbose=1)print(f\"\\nFinal Validation Results:\")print(f\"  Loss: {val_loss:.4f}\")print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")# =============================================================================# MODEL SAVING# =============================================================================print(\"\\nSaving final model...\")hybrid_model.save('keras_cnn_vit_final.keras')print(\"Model saved as 'keras_cnn_vit_final.keras'\")print(\"\\nTo download the model:\")print(\"  1. Right-click on the file in the file browser\")print(\"  2. Select 'Download'\")print(\"  3. Save to your local machine for submission\")# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 1 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Loaded CNN model from {keras_model_path}\")print(f\"Task 2: Identified feature layer: {feature_layer_name}\")print(f\"Task 3: Built hybrid CNN-ViT model with 4 transformer layers\")print(f\"Task 4: Compiled model with Adam(1e-4) and categorical_crossentropy\")print(f\"Task 5: Trained model for {epochs} epochs with {steps_per_epoch} steps/epoch\")print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "61f2830ce2097e8820ce6407e6567640ed4feb54a55a17665fb7f3f613234ff1",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05fb258843d74fe3ad7a517710fdbab8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16d9a36726584a38922f3e2c28e34ae1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2023ad15e23d4994b1e3204dd01ba8df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24a5da95836f4fc0943ba3b49a7118c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d566746554824e1492b0fff44108e9f3",
        "IPY_MODEL_592632be68cf4f07bce876d96ab1924a",
        "IPY_MODEL_fc29e5c2e124466ba078c2cffe83496e"
       ],
       "layout": "IPY_MODEL_05fb258843d74fe3ad7a517710fdbab8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "30b6cbc612ca42289734ee9b274d4d08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "36cfded004104cf99d02efe6f7d6b64e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "399bd837afcb4c91999aba94c85bd8dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9a4f405583b74773a6e31bf2712e5828",
        "IPY_MODEL_f831f672cb5643b6ac8961b4da38eb60",
        "IPY_MODEL_ee7e9206cdf54bbfb8035d965d021fde"
       ],
       "layout": "IPY_MODEL_2023ad15e23d4994b1e3204dd01ba8df",
       "tabbable": null,
       "tooltip": null
      }
     },
     "41a3813ad39d4c51b86ff8a6c9166655": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5221c98d58b74a3d90f76b9994d68082": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "549158c117ec4853aa2331b358c854e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "592632be68cf4f07bce876d96ab1924a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_af0405c435904b78a8d8e6cdc13ab203",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_36cfded004104cf99d02efe6f7d6b64e",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     },
     "79ab22af6c294647bee938da8916ca7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "889bace83f1444c783dd369d86f53c4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "906ed231b3d64db984df1672c55eabf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a4f405583b74773a6e31bf2712e5828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5221c98d58b74a3d90f76b9994d68082",
       "placeholder": "​",
       "style": "IPY_MODEL_41a3813ad39d4c51b86ff8a6c9166655",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "a7e9571b9fac4687a4951cd037c1a7e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af0405c435904b78a8d8e6cdc13ab203": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d566746554824e1492b0fff44108e9f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_549158c117ec4853aa2331b358c854e3",
       "placeholder": "​",
       "style": "IPY_MODEL_e463d0557708424abee648ad836ad16d",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "e463d0557708424abee648ad836ad16d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ee7e9206cdf54bbfb8035d965d021fde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_16d9a36726584a38922f3e2c28e34ae1",
       "placeholder": "​",
       "style": "IPY_MODEL_a7e9571b9fac4687a4951cd037c1a7e4",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:02&lt;00:00, 12042173.96it/s]"
      }
     },
     "f831f672cb5643b6ac8961b4da38eb60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_79ab22af6c294647bee938da8916ca7f",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_30b6cbc612ca42289734ee9b274d4d08",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "fc29e5c2e124466ba078c2cffe83496e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_906ed231b3d64db984df1672c55eabf5",
       "placeholder": "​",
       "style": "IPY_MODEL_889bace83f1444c783dd369d86f53c4b",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 10673.01it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
