{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba100652-ab0d-44a0-a145-00b5a2b06ff4",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Vision Transformers Using PyTorch </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c1104-b4b0-427a-befb-1b205b14486c",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915562dc-e4a2-4da0-a4b5-3d4a605d2987",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn to build a PyTorch-based hybrid Convolutional Neural Network (CNN) and Vision Transformer (ViT) for image classification. \n",
    "You'll start by using CNN layers to extract detailed features, such as edges and textures, from images. Then, you'll see how those features are passed to a Vision Transformer, which looks at the global correlations in the entire image by looking at all locations at once. Then, you will train a hybrid CNN-ViT model, and by the end of this lab, you'll also know how to monitor its performance. This approach gives you practical experience with state-of-the-art techniques in computer vision!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom trained PyTorch CNN model to extract feature maps and use them with a Vision Transformer (ViT) architecture to create a CNN-ViT hybrid architecture.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom trained PyTorch CNN model\n",
    "2. Extract feature maps from the PyTorch model\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the hybrid model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Model paths and download](#Model-paths-and-download)\n",
    "- [Defining pre-trained CNN backbone](#Defining-pre-trained-CNN-backbone)\n",
    "- [Vision Transformers](#Vision-Transformers)\n",
    "    - [Patch embedding](#Patch-embedding)\n",
    "    - [Multi-head self-attention (MHSA) module](#Multi-head-self-attention-(MHSA)-module)\n",
    "    - [Transformer block](#Transformer-block)\n",
    "    - [Vision Transformer (ViT) model](#Vision-Transformer-(ViT)-model)\n",
    "- [CNN-ViT hybrid model](#CNN-ViT-hybrid-model)\n",
    "- [Model training](#Model-training)\n",
    "- [Model evaluation](#Model-evaluation)\n",
    "- [Data preparation and loading](#Data-preparation-and-loading)\n",
    "- [Model initialization and training loop](#Model-initialization-and-training-loop)\n",
    "- [Plotting training and validation accuracy and loss](#Plotting-training-and-validation-accuracy-and-loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "Let's begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded\n",
    "2. The dataset downloading primary function, based on the `skillsnetwork` library\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77975cb-407e-4066-b3c7-4cc5e59ceb77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:04.711944Z",
     "iopub.status.busy": "2025-10-27T07:51:04.711552Z",
     "iopub.status.idle": "2025-10-27T07:51:11.477716Z",
     "shell.execute_reply": "2025-10-27T07:51:11.477318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1557058353ee457f8ca66a5c1178455c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00743bcc5ea04e25a9a41dfe02d16752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install PyTorch and python packages libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79b410f-de2d-4603-b062-f770fc469d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:11.478980Z",
     "iopub.status.busy": "2025-10-27T07:51:11.478896Z",
     "iopub.status.idle": "2025-10-27T07:51:13.803868Z",
     "shell.execute_reply": "2025-10-27T07:51:13.803057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.93 ms, sys: 9.95 ms, total: 17.9 ms\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26 matplotlib==3.9.2 skillsnetwork\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0fcdf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:13.806064Z",
     "iopub.status.busy": "2025-10-27T07:51:13.805887Z",
     "iopub.status.idle": "2025-10-27T07:51:13.985206Z",
     "shell.execute_reply": "2025-10-27T07:51:13.984767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 20.4 ms, total: 171 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b7cb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:13.986485Z",
     "iopub.status.busy": "2025-10-27T07:51:13.986383Z",
     "iopub.status.idle": "2025-10-27T07:51:15.256173Z",
     "shell.execute_reply": "2025-10-27T07:51:15.255852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported PyTorch libraries\n",
      "CPU times: user 759 ms, sys: 159 ms, total: 918 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "print(\"Imported PyTorch libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e211b54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.257634Z",
     "iopub.status.busy": "2025-10-27T07:51:15.257505Z",
     "iopub.status.idle": "2025-10-27T07:51:15.259846Z",
     "shell.execute_reply": "2025-10-27T07:51:15.259455Z"
    }
   },
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644faa0-0939-4668-a380-4c1ecce6e0e8",
   "metadata": {},
   "source": [
    "## Lab layout\n",
    "- First, you download the pre-trained PyTorch based CNN model.\n",
    "- Then, you define the CNN backbone. This is same as defined in the training of the pre-trained model.\n",
    "- The CNN backbone reduces the input image’s spatial dimensions and expands its feature channels. You then convert this feature map into a sequence of tokens for the Vision Transformer (ViT).\n",
    "- These tokens are passed into a ViT module. The ViT is applied after the CNN so it can model global relationships and context between different regions in the original image, something CNNs alone cannot do as effectively.\n",
    "- You use a sequential hybrid architecture: the CNN performs local feature extraction, and the ViT, using those extracted features, performs global reasoning. This leverages the strengths of both models for improved accuracy and generalization.\n",
    "- Feature reshaping is used: the CNN feature map is flattened and fed into the transformer, and positional encoding might be added to preserve spatial information.\n",
    "- You train this hybrid model end-to-end, meaning both the CNN and ViT parameters are updated together to optimize classification performance.\n",
    "- Throughout the process, you are able to monitor both local (CNN) and global (ViT) attention across the image, resulting in a model that is more robust and effective than using either approach alone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d1c145-57c5-4031-ac2b-d207dd229ffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.260918Z",
     "iopub.status.busy": "2025-10-27T07:51:15.260842Z",
     "iopub.status.idle": "2025-10-27T07:51:15.262565Z",
     "shell.execute_reply": "2025-10-27T07:51:15.262172Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5edf8ee-dab8-4a70-afd1-0a6cb65e7992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.263422Z",
     "iopub.status.busy": "2025-10-27T07:51:15.263362Z",
     "iopub.status.idle": "2025-10-27T07:51:15.265186Z",
     "shell.execute_reply": "2025-10-27T07:51:15.264851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai_capstone_pytorch_best_model_state_dict_downloaded.pth\n"
     ]
    }
   ],
   "source": [
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484b21c-163f-444a-8acb-134a04347a32",
   "metadata": {},
   "source": [
    "## Ensuring repeatability in PyTorch\n",
    "\n",
    "To achieve reproducible results when you train a CNN in PyTorch, you must seed every random-number generator and configures cuDNN for deterministic kernels.\n",
    "* **Python & NumPy** – Many data-pipeline utilities (shuffling lists, image augmentations) rely on these random-number generators. Seeding them first removes one entire layer of randomness.\n",
    "* **PyTorch CPU / GPU** – `torch.manual_seed` covers every op executed on the CPU, while `torch.cuda.manual_seed_all` applies the same seed to each GPU stream so that multi-GPU jobs stay in sync.\n",
    "* **cuDNN flags** – By default cuDNN picks the fastest convolution algorithm, which can vary run-to-run. Setting `deterministic=True` forces repeatable kernels and turning `benchmark` *off* prevents the auto-tuner from replacing those kernels mid-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c7de30-36e4-4dae-bd80-be7442bb4c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.266124Z",
     "iopub.status.busy": "2025-10-27T07:51:15.266068Z",
     "iopub.status.idle": "2025-10-27T07:51:15.267824Z",
     "shell.execute_reply": "2025-10-27T07:51:15.267526Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9c5ae1-9248-431f-a606-5778f2f5ca01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.268679Z",
     "iopub.status.busy": "2025-10-27T07:51:15.268621Z",
     "iopub.status.idle": "2025-10-27T07:51:15.273188Z",
     "shell.execute_reply": "2025-10-27T07:51:15.272816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 7331 - main process is now deterministic.\n"
     ]
    }
   ],
   "source": [
    "SEED = 7331\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} - main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-doc",
   "metadata": {},
   "source": [
    "## Defining pre-trained CNN backbone\n",
    "\n",
    "In this cell, you will create and implement a **ConvNet** class. This class serves as the convolutional backbone for the hybrid CNN-ViT architecture. \n",
    "The design of this class will be **same as the training architecture**, with six progressive convolutional blocks.\n",
    "\n",
    "- **`forward_features()`**: Returns the raw convolutional feature map (B, 1024, H, W) for use by the **ViT component**\n",
    "\n",
    "- **Role in hybrid architecture**: In the hybrid model, this CNN serves as a **local feature extractor**, capturing low-level patterns, edges, and textures before passing the feature-rich representation to the Vision Transformer for global context modeling. The 1024-channel output provides a rich semantic representation that the ViT can process as a sequence of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a120e6b-5689-4be7-9709-d7ebfa3cd3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.274158Z",
     "iopub.status.busy": "2025-10-27T07:51:15.274090Z",
     "iopub.status.idle": "2025-10-27T07:51:15.276511Z",
     "shell.execute_reply": "2025-10-27T07:51:15.276118Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    ''' \n",
    "    Class to define the architecture same as the imported pre-trained CNN model for extracting the` feature map\n",
    "    '''\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.features(x)      # (B,1024,H,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e6958-07b3-4037-9f91-d7ac6e4d2f81",
   "metadata": {},
   "source": [
    "# Vision Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdcfe-8228-4e8a-851e-bbd79f5f18e5",
   "metadata": {},
   "source": [
    "### Patch embedding\n",
    " The **PatchEmbed** class implements the **interface** between the CNN feature extractor and the Vision Transformer, converting spatial feature maps into a sequence of tokens suitable for self-attention processing.\n",
    "\n",
    "Unlike traditional ViT implementations that divide raw images into fixed-size patches, this hybrid approach operates on CNN feature maps. The implementation uses a **1×1 convolution** as a learned linear projection to transform the 1024-dimensional CNN features into the transformer's embedding dimension (default 768).\n",
    "\n",
    "The transformation process involves three key steps:\n",
    "1. **Channel projection**: `nn.Conv2d(in_ch, embed_dim, kernel_size=1)` reduces or expands the channel dimension from 1024 to the preferred embedding size\n",
    "2. **Spatial flattening**: `.flatten(2)` collapses the height and width dimensions (H×W) into a single sequence dimension\n",
    "3. **Tensor reshaping**: `.transpose(1,2)` reorders dimensions from (B, D, L) to (B, L, D) where L=H×W represents the sequence length\n",
    "\n",
    "**Integration with hybrid architecture**\n",
    "\n",
    "In the context of the hybrid model, this patch embedding serves as the **bridge** between local CNN features and global transformer processing. Since the CNN has already extracted meaningful local patterns, the patch embedding focuses on format conversion rather than feature extraction. This design is more **efficient** than traditional ViT patch embedding since the CNN has already performed the heavy lifting of feature extraction from raw pixels.\n",
    "\n",
    "The **output tensor** (B, L, D) represents a batch of sequences where each sequence contains L tokens (corresponding to spatial locations in the feature map) with D-dimensional embeddings ready for transformer processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a517d5-b6fa-4fe5-a848-02cfc84106eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.277435Z",
     "iopub.status.busy": "2025-10-27T07:51:15.277377Z",
     "iopub.status.idle": "2025-10-27T07:51:15.279180Z",
     "shell.execute_reply": "2025-10-27T07:51:15.278842Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, input_channel=1024, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)  # 1×1 conv\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhsa-doc",
   "metadata": {},
   "source": [
    "## Multi-head self-attention (MHSA) module\n",
    "\n",
    "The **MHSA** class implements the self-attention mechanism that enables the Vision Transformer to model long-range dependencies and global context across all spatial locations in the feature map.\n",
    "\n",
    "The implementation follows the scaled dot-product attention formula: **Attention(Q,K,V) = softmax(QK^T/√d)V**, where Q, K, and V represent query, key, and value matrices, respectively. The scaling factor 1/sqrt(d) prevents the dot products from becoming too large, which would push the softmax function into regions with extremely small gradients.\n",
    "\n",
    "The multi-head mechanism splits the embedding dimension across multiple attention heads, allowing the model to attend to different types of relationships simultaneously. \n",
    "\n",
    "### Key implementation details:\n",
    "\n",
    "- **Unified QKV projection**: A single linear layer `nn.Linear(dim, dim*3)` generates Q, K, and V matrices efficiently, reducing memory overhead compared to separate projections\n",
    "- **Head reshaping**: The tensor is reshaped from (B, N, D) to (B, heads, N, d) where d = D/heads, enabling parallel processing across heads\n",
    "- **Attention computation**: Matrix multiplication `torch.matmul(q, k.transpose(-2, -1))` computes attention scores, followed by scaling and softmax normalization\n",
    "- **Dropout regularization**: Two dropout layers prevent overfitting - one on attention weights and one on the final output\n",
    "\n",
    "\n",
    "Unlike CNNs, which have limited receptive fields, self-attention allows every token to interact directly with every other token in a single operation. This enables the model to **capture long-range spatial dependencies** that might be missed by purely convolutional approaches. \n",
    "\n",
    "#### In the hybrid architecture, this global modeling complements the local feature extraction performed by the CNN backbone.\n",
    "\n",
    "The attention weights provide interpretability, showing which spatial locations the model focuses on when making predictions. This component is valuable for both performance and explainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7114dc-5a8b-4aea-8509-e2dd13bb8f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.280065Z",
     "iopub.status.busy": "2025-10-27T07:51:15.280005Z",
     "iopub.status.idle": "2025-10-27T07:51:15.282416Z",
     "shell.execute_reply": "2025-10-27T07:51:15.282047Z"
    }
   },
   "outputs": [],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)  # (B, heads, N, d)\n",
    "        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attn_drop(attn.softmax(dim=-1))\n",
    "        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)\n",
    "        return self.proj_drop(self.proj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd0ac5-89ec-4445-9d6d-e3cc86dfbae1",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "This code defines a building block used in modern deep learning models, especially in Vision Transformers. The class is called TransformerBlock, and it is responsible for helping the model “pay attention” to the most important parts of its input and improve the final predictions.\n",
    "\n",
    "Let’s break down what happens inside:\n",
    "\n",
    "**Layer normalization (nn.LayerNorm):** This helps stabilize and speed up training by normalizing each row of the input data, which is useful before applying attention or a neural network layer.\n",
    "\n",
    "**Attention layer (MHSA):** This is the “Multi-Head Self Attention” block. It enables the model to look at all positions in the sequence (or image patches) at once, figuring out which ones are most important for each output. It’s like giving the model the power to focus on the key parts of an image or sentence.\n",
    "\n",
    "**MLP (nn.Sequential):** This is a **Multi-Layer Perceptron** or a mini neural network, made up of linear (fully connected) layers, a special activation (GELU), and dropout for regularization. This MLP has:\n",
    "- One linear layer that expands the input dimension by mlp_ratio (for example, 4× wider).\n",
    "- A GELU activation function (a nonlinear operation, similar to ReLU).\n",
    "- Dropout for regularization (helps prevent overfitting).\n",
    "- Another linear layer that shrinks the data back to the original dimension.\n",
    "- Another dropout layer.\n",
    "Here, MLP is designed to help the model learn better representations by combining and transforming the information after the attention step.\n",
    "\n",
    "**Skip/Residual connections (x + ...):** These connections support effective and stable training for deep neural networks by keeping pathways open for both forward information flow and backward gradient flow, making deep architectures such as transformers possible and practical. In transformers, skip connections are placed around both the attention and feedforward (MLP) sub-layers in each block. This stabilizes training of these very deep, stackable models, improves convergence speed, and lets them scale to larger datasets and more complex tasks.\n",
    "\n",
    "In summary, this block helps models understand relationships in their input data, making them more powerful for tasks such as image and language understanding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f158e5-3c37-4682-8fe8-1db3d6813dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.283348Z",
     "iopub.status.busy": "2025-10-27T07:51:15.283290Z",
     "iopub.status.idle": "2025-10-27T07:51:15.285325Z",
     "shell.execute_reply": "2025-10-27T07:51:15.284990Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = MHSA(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "                                    nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "                                    nn.GELU(), nn.Dropout(dropout),\n",
    "                                    nn.Linear(int(dim * mlp_ratio), dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vit-doc",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) model\n",
    "\n",
    "The **ViT** class represents the complete Vision Transformer implementation, including patch embedding, positional encoding, transformer blocks, and a classification head for global context modeling.\n",
    "\n",
    "The implementation uses a learnable **Classification [CLS] token**:\n",
    "- **Initialization**: `nn.Parameter(torch.zeros(1, 1, embed_dim))` creates a learnable token initialized to zeros\n",
    "- **Expansion**: `self.cls.expand(B, -1, -1)` replicates the token across the batch dimension\n",
    "- **Prepending**: `torch.cat((cls, x), 1)` concatenates the CLS token to the beginning of the sequence\n",
    "- **Classification**: Only the CLS token representation is used for final classification\n",
    "\n",
    "This allows the CLS token to aggregate information from all spatial locations through self-attention, creating a global representation suitable for classification.\n",
    "\n",
    "`nn.Parameter(torch.randn(1, max_tokens, embed_dim))` creates a large positional embedding matrix\n",
    "and `self.pos[:, :L+1]` dynamically slices the positional embeddings to match the actual sequence length. Together, these create a **dynamic positional encoding** system for the hybrid architecture where the CNN feature map size can vary based on input image dimensions.\n",
    "\n",
    "\n",
    "**`depth`** defines the number of transformerBlocks to be used in the transformer encoder\n",
    "### Classification head design\n",
    "\n",
    "The final classification pipeline includes:\n",
    "1. **Final normalization**: `self.norm(x)` applies LayerNorm to the final transformer output\n",
    "2. **CLS token extraction**: `[:, 0]` selects only the CLS token representation\n",
    "3. **Linear classification**: `self.head` maps the CLS representation to class logits\n",
    "\n",
    "The model's ability to handle variable sequence lengths makes it robust to different input sizes and CNN architectures, providing flexibility in deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1fd0bb-d07d-4b32-b441-2757d8b931b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.286246Z",
     "iopub.status.busy": "2025-10-27T07:51:15.286185Z",
     "iopub.status.idle": "2025-10-27T07:51:15.288653Z",
     "shell.execute_reply": "2025-10-27T07:51:15.288342Z"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_ch=1024, num_classes=2,\n",
    "                 embed_dim=768, depth=6, heads=8,\n",
    "                 mlp_ratio=4., dropout=0.1, max_tokens=50):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(in_ch, embed_dim)           # 1×1 conv\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                          # x: (B,C,H,W)\n",
    "        x = self.patch(x)                          # (B,L,D)\n",
    "        B, L, _ = x.shape\n",
    "        cls = self.cls.expand(B, -1, -1)           # (B,1,D)\n",
    "        x = torch.cat((cls, x), 1)                 # (B,L+1,D)\n",
    "        x = x + self.pos[:, :L + 1]                # match seq-len\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(self.norm(x)[:, 0])       # CLS token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-doc",
   "metadata": {},
   "source": [
    "## CNN-ViT hybrid model\n",
    "\n",
    "The **CNN_ViT_Hybrid** class represents the hybrid architecture, integrating the CNN backbone with the Vision Transformer to create a unified model that leverages both local and global feature processing capabilities.\n",
    "In this hybrid model, the pre-trained CNN layers can be frozen while fine-tuning the Vi and allows for **efficient attention** to operate on semantically rich CNN features rather than raw pixels\n",
    "Thus, this hybrid approach combines the excellent capabilities of CNN to capture local patterns efficiently with ViTs' global context modeling, while keeping the level of complexity low.\n",
    "\n",
    "The forward pass implements a **two-stage forward pass pipeline**:\n",
    "1. **Local feature extraction**: `self.cnn.forward_features(x)` processes the input image through the CNN backbone, extracting hierarchical local features and reducing spatial resolution while increasing semantic depth.\n",
    "2. **Global context modeling**: `self.vit(features)` takes the CNN feature map and processes it through the Vision Transformer for global reasoning and classification.\n",
    "\n",
    "Despite the modular design, the entire hybrid model remains **fully differentiable**, enabling end-to-end training where gradients could flow from the classification loss back through both the ViT and CNN components. This allows the CNN to learn features that are optimally suited for the downstream transformer processing, creating a synergistic relationship between the two architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf7d20d-9a55-4398-90a9-81e860f62838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.289700Z",
     "iopub.status.busy": "2025-10-27T07:51:15.289636Z",
     "iopub.status.idle": "2025-10-27T07:51:15.291440Z",
     "shell.execute_reply": "2025-10-27T07:51:15.291148Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvNet(num_classes)            # load weights later\n",
    "        self.vit = ViT(num_classes=num_classes,\n",
    "                       embed_dim=embed_dim,\n",
    "                       depth=depth,\n",
    "                       heads=heads)\n",
    "    def forward(self, x):\n",
    "        return self.vit(self.cnn.forward_features(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-doc",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "The **train** function implements a comprehensive training loop for one epoch, handling forward propagation, loss computation, backpropagation, and metric tracking in a memory-efficient manner.\n",
    "\n",
    "**Training mode**\n",
    "The function begins with `model.train()`, which configures the model for training by enabling dropout layers and gradient computation\n",
    "\n",
    "**Batch processing pipeline**\n",
    "Each training iteration follows a standard deep learning pipeline:\n",
    "\n",
    "1. **Data transfer**: `x, y = x.to(device), y.to(device)` moves input data and labels to the appropriate device (CPU/GPU)\n",
    "2. **Gradient reset**: `optimizer.zero_grad()` clears gradients from the previous iteration to prevent accumulation\n",
    "3. **Forward pass**: `out = model(x)` computes predictions through the hybrid CNN-ViT architecture\n",
    "4. **Loss computation**: `loss = criterion(out, y)` calculates cross-entropy loss between predictions and ground truth\n",
    "5. **Backpropagation**: `loss.backward()` computes gradients via automatic differentiation\n",
    "6. **Parameter update**: `optimizer.step()` updates model parameters using the computed gradients\n",
    "\n",
    "**Metric accumulation**: The function tracks two key metrics:\n",
    "- **Weighted loss**: `loss.item() * x.size(0)` accumulates loss weighted by batch size for accurate averaging\n",
    "- **Correct predictions**: `(out.argmax(1) == y).sum().item()` counts correct predictions using argmax for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c5540f-b131-4cfd-9d18-93d51b4dca23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.292376Z",
     "iopub.status.busy": "2025-10-27T07:51:15.292317Z",
     "iopub.status.idle": "2025-10-27T07:51:15.294413Z",
     "shell.execute_reply": "2025-10-27T07:51:15.294004Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    loss_sum, correct = 0, 0\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Training  \")):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-doc",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Here, you will evaluate the model, implementing inference without gradient computation to optimize memory usage and computational speed.\n",
    "\n",
    "Two fundamental differences between using the model for evaluation and testing versus training are:\n",
    "- **`torch.no_grad()`**:  Disables gradient computation and accelerate inference\n",
    "- **`model.eval()`**: Switches the model to evaluation mode, ensures deterministic outputs\n",
    "\n",
    "The **evaluation loop** mirrors the training loop structure but omits gradient-related operations.\n",
    "\n",
    "This function **integrates with the training pipeline**, providing regular validation checks that help monitor model progress, detect overfitting, and make informed decisions about training continuation, learning rate adjustments, and model selection. The consistent interface with the training function enables easy integration into automated training workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa42f92-0538-4490-a62f-8d1d37b64fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.295379Z",
     "iopub.status.busy": "2025-10-27T07:51:15.295303Z",
     "iopub.status.idle": "2025-10-27T07:51:15.297263Z",
     "shell.execute_reply": "2025-10-27T07:51:15.296964Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_sum, correct = 0, 0\n",
    "        for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Validation\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fafbd-1abd-4df8-a3a6-4d05b05f7307",
   "metadata": {},
   "source": [
    "## Data preparation and loading\n",
    "\n",
    "Here, you implement data preparation for the entire data pipeline, from raw image loading to batched tensor delivery.\n",
    "\n",
    "You define the key training hyperparameters:\n",
    "- **Image size (64×64)**: Chosen for computational efficiency while maintaining sufficient resolution for feature extraction\n",
    "- **Batch size (128)**: Balances memory usage with gradient stability and training speed\n",
    "- **learning rate (0.001)**: A conservative number for learning\n",
    "- **number of classes**: Total number of classes to be classified by the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c925c5c6-80f1-42f2-9212-f4c02bc50d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.298169Z",
     "iopub.status.busy": "2025-10-27T07:51:15.298110Z",
     "iopub.status.idle": "2025-10-27T07:51:15.299786Z",
     "shell.execute_reply": "2025-10-27T07:51:15.299494Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_cls  = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411ea27-22c9-404a-973b-e0c362e190f9",
   "metadata": {},
   "source": [
    "### Training data transformations\n",
    "The **training transform** pipeline implements several **augmentation techniques** including Random Rotation, Random Horizontal Flip, Random Affine with Shear and normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c719f2-ff7e-4e00-ad27-256a6d2cdc36",
   "metadata": {},
   "source": [
    "## Task: Create `train_transform` transforms for the training dataset\n",
    "- Use the following parameters:\n",
    "    - Size: `img_size x img_size`\n",
    "    - `RandomRotation`: 40\n",
    "    - `RandomHorizontalFlip`\n",
    "    - `RandomAffine(0, shear=0.2)`\n",
    "    - `Normalization` values: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b7aec3-13f2-4b33-8093-2edb676a86b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.300822Z",
     "iopub.status.busy": "2025-10-27T07:51:15.300763Z",
     "iopub.status.idle": "2025-10-27T07:51:15.302272Z",
     "shell.execute_reply": "2025-10-27T07:51:15.301924Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3d4eb-f274-4fb7-be03-55235456b920",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c55c42-34cb-40ba-821a-f1d3f870ae0d",
   "metadata": {},
   "source": [
    "### Validation data transformations\n",
    "The **validation transform** is minimal for **deterministic preprocessing** to ensure reproducible validation results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e944b6e-9aa1-4413-ae6d-5943aa7abe2d",
   "metadata": {},
   "source": [
    "## Task: Create `val_transform` transforms for the validation dataset\n",
    "- Use the following parameters:\n",
    "    - Size: `img_size x img_size`\n",
    "    - `Normalization` values: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f8d96f8-c895-4fc7-9818-384393dc238c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.303213Z",
     "iopub.status.busy": "2025-10-27T07:51:15.303155Z",
     "iopub.status.idle": "2025-10-27T07:51:15.304669Z",
     "shell.execute_reply": "2025-10-27T07:51:15.304352Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5870c47-4c92-4b81-b46d-3ce72eb45952",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "val_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40beced-ab3f-4bda-876d-0a61b137382a",
   "metadata": {},
   "source": [
    "### The DataLoader\n",
    "\n",
    "The DataLoader setup is optimized for training:\n",
    "- **Shuffling**: Training data is shuffled to prevent batch-level patterns\n",
    "- **No Validation Shuffling**: Validation order is consistent for reproducible results\n",
    "- **`batch_size`**: Efficient tensor batching for GPU utilization\n",
    "\n",
    "You begin by splitting the dataset into `training` and `validation` data using `random_split` feature. Here, you define **80%** (0.8 fraction) of the total dataset for training and rest for validation.\n",
    "\n",
    "Next, you apply the `train_transform` to `train_dataset` and `val_transform` to `val_dataset` to make the dataset ready for DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68dee90-ccc1-4933-b55e-cfa1ec1b2fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.305724Z",
     "iopub.status.busy": "2025-10-27T07:51:15.305650Z",
     "iopub.status.idle": "2025-10-27T07:51:15.368037Z",
     "shell.execute_reply": "2025-10-27T07:51:15.367726Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m full_dataset = datasets.ImageFolder(dataset_path, transform=\u001b[43mtrain_transform\u001b[49m)\n\u001b[32m      3\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(full_dataset))\n\u001b[32m      4\u001b[39m val_size = \u001b[38;5;28mlen\u001b[39m(full_dataset) - train_size\n",
      "\u001b[31mNameError\u001b[39m: name 'train_transform' is not defined"
     ]
    }
   ],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96107c1-49a1-428b-b412-ff214cdfa2ac",
   "metadata": {},
   "source": [
    "## Task: Create the Dataloader `train_loader` and `val_loader` using `train_dataset` and `val_dataset`\n",
    "- Use the following parameters:\n",
    "    - `batch_size=batch_size`\n",
    "    - for `train_loader`: `shuffle=True`\n",
    "    - for `val_loader`: `shuffle=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d72de3d-4f8e-445b-991d-9474ce10ffa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.369187Z",
     "iopub.status.busy": "2025-10-27T07:51:15.369114Z",
     "iopub.status.idle": "2025-10-27T07:51:15.370720Z",
     "shell.execute_reply": "2025-10-27T07:51:15.370356Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea67f23-a7f6-4a72-b6a7-5f8ac702561c",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                         )\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                       )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loop-doc",
   "metadata": {},
   "source": [
    "## Model initialization and training loop\n",
    "\n",
    "This cell orchestrates the complete training pipeline, from model instantiation through iterative training and validation, implementing comprehensive monitoring and logging for effective model development.\n",
    "\n",
    "- **CUDA detection**: `torch.cuda.is_available()` checks for GPU availability\n",
    "\n",
    "The hybrid model is instantiated with carefully chosen hyperparameters:\n",
    "- **Number of classes**: `num_classes=2` configured for the specific dataset\n",
    "- **Default architecture**: Uses ViT configuration (768 embedding dim, 1  transformer layer, 1 heads)\n",
    "\n",
    "### Transfer learning integration\n",
    "\n",
    "The commented line demonstrates transfer learning capability:\n",
    "- **Pre-trained weights**: Option to load pre-trained CNN backbone weights\n",
    "- **Flexible loading**: `strict=False` allows partial weight loading\n",
    "\n",
    "**Optimizer**: The training uses the `adam` optimizer.\n",
    "\n",
    "The training loop collects the **training and validation metrics** to track training performance and monitor generalization of the model.\n",
    "\n",
    "### Training loop architecture\n",
    "\n",
    "Each epoch follows a structured pipeline:\n",
    "1. **Timing**: `time.time()` tracks epoch duration for performance monitoring\n",
    "2. **Training phase**: Calls the training function with appropriate parameters\n",
    "3. **Validation phase**: Evaluates model on validation set\n",
    "4. **Logging**: Comprehensive output showing all metrics and timing\n",
    "5. **Storage**: Appends metrics to tracking lists for later analysis\n",
    "\n",
    "In this training cell, you create a robust, monitored, and efficient training pipeline that provides comprehensive insights into model performance while maintaining computational efficiency and enabling easy debugging and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1605bf28-929b-4eda-8da7-61ef0f8b0f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.371603Z",
     "iopub.status.busy": "2025-10-27T07:51:15.371546Z",
     "iopub.status.idle": "2025-10-27T07:51:15.494198Z",
     "shell.execute_reply": "2025-10-27T07:51:15.493795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on cpu\n",
      "epochs:5 | batch:32 | attn_heads:6 | depth:3 | embed_dim:768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01/05 started at 20251027_115115 (UTC)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m start_time = time.time()\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m started at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpresent_time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (UTC)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m tr_loss,tr_acc = train(model, \u001b[43mtrain_loader\u001b[49m, optimizer, criterion, device)\n\u001b[32m     42\u001b[39m te_loss,te_acc = evaluate(model, val_loader, criterion, device)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m |\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 6\n",
    "depth      = 3\n",
    "embed_dim  = 768\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n",
    "\n",
    "model_dict_name = f\"ai_capstone_pytorch_vit_model_state_dict.pth\"\n",
    "\n",
    "model     = CNN_ViT_Hybrid(num_classes=num_cls,\n",
    "                            heads=attn_heads,\n",
    "                            depth=depth,\n",
    "                            embed_dim=embed_dim\n",
    "                           ).to(device)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# loading pre-trained CNN weights\n",
    "model.cnn.load_state_dict(torch.load(pytorch_state_dict_path), strict=False)\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "tr_loss_all = []\n",
    "te_loss_all = []\n",
    "tr_acc_all = []\n",
    "te_acc_all = []\n",
    "\n",
    "training_time = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss,tr_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    te_loss,te_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {te_loss:.4f} acc {te_acc:.4f} |\"\n",
    "          f\" in  {time.time()-start_time:.02f}s\"\n",
    "        )\n",
    "    tr_loss_all.append(tr_loss)\n",
    "    te_loss_all.append(te_loss)\n",
    "    tr_acc_all.append(tr_acc)\n",
    "    te_acc_all.append(te_acc)\n",
    "    training_time.append(time.time() - start_time)\n",
    "    \n",
    "    # Save the best model\n",
    "    avg_te_loss = te_loss\n",
    "    if avg_te_loss < best_loss:\n",
    "        print(f\"Current loss ({avg_te_loss:.04f}) lower than previous best loss ({ best_loss:.04f}), Saving current model state\")\n",
    "        best_loss = avg_te_loss\n",
    "        torch.save(model.state_dict(), model_dict_name)\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668583e1-9f69-4b2d-83dd-78d7ecbb5e27",
   "metadata": {},
   "source": [
    "### Hyperparameter cheatsheet (depth based)\n",
    "\n",
    "The Depth of the transformer blocks signify the number of transformer blocks stacked in the model. This is one of the most important features which differentiates a ViT from CNN.\n",
    "\n",
    "This table proides a basic overview on **how depth affects** other hyperparameters and model performance.\n",
    "\n",
    "|  **Depth** | **Attention heads** | **Dataset Size** |  **Performance** | **learning rate** | **Feature Complexity** | **Learning Focus** |\n",
    "|:---:|:---:|---|:---:|---|:---|:---:|\n",
    "| **3** | 6 | size < 1000 | Underfitting - too shallow |0.001 (Shallow: can handle higher learning rates) | Low-level features | Edges, textures, basic patterns |\n",
    "| **6** | 6 | size <1000 | Good for simple tasks |0.001 (Shallow: can handle higher learning rates) | Mid-level features | Shapes, object parts, spatial relationships |\n",
    "| **12** | 12 | 1000 < size < 10000 | Standard choice - good balance | 0.0005 (Medium: moderate learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **18** | 12 | 10000 < size < 100000 | High performance on complex tasks | 0.0003 (Deep: lower learning rate for stability) | High-level features | Objects, semantic concepts, global context |\n",
    "| **24** | 16 | 100000 < size | Diminishing returns, overfitting risk | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **36** | 16 | 100000 < size | Likely overkill for most tasks | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc7b27-134c-42f8-9034-decf80168e46",
   "metadata": {},
   "source": [
    "### Task: Design and train a CNN-ViT hybrid model `model_test` with the following hyperparameters:\n",
    "- `epochs=5`\n",
    "- `attn heads=12`\n",
    "- `transformer block depth = 12`\n",
    "- `embed_dim=768`\n",
    "\n",
    "Save the `accuracy` and `loss` metrics in\n",
    "- `tr_loss_all_test` for training loss\n",
    "- `te_loss_all_test` for validation/testing loss\n",
    "- `tr_acc_all_test` for training accuracy\n",
    "- `te_acc_all_test` for validation/testing accuracy\n",
    "\n",
    "Save the training times in `training_time_test`\n",
    "\n",
    "Save the best model as **`ai_capstone_pytorch_vit_model_test_state_dict.pth`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c542f0ad-6d11-4afc-b21d-ffa6a7bf8330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.495360Z",
     "iopub.status.busy": "2025-10-27T07:51:15.495280Z",
     "iopub.status.idle": "2025-10-27T07:51:15.496843Z",
     "shell.execute_reply": "2025-10-27T07:51:15.496541Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251201c9-1224-4063-be10-0e8828654dcf",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "## Please use the space below to write your answer\n",
    "\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 12\n",
    "depth      = 12\n",
    "embed_dim  = 768\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n",
    "\n",
    "model_dict_name = f\"ai_capstone_pytorch_vit_model_test_state_dict.pth\"\n",
    "\n",
    "model_test = CNN_ViT_Hybrid(num_classes=num_cls,\n",
    "                            heads=attn_heads,\n",
    "                            depth=depth,\n",
    "                            embed_dim=embed_dim\n",
    "                           ).to(device)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# loading pre-trained CNN weights\n",
    "model_test.cnn.load_state_dict(torch.load(pytorch_state_dict_path), strict=False)\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model_test.parameters(), lr=lr)\n",
    "\n",
    "best_loss = float('inf')\n",
    "tr_loss_all_test = []\n",
    "te_loss_all_test = []\n",
    "tr_acc_all_test = []\n",
    "te_acc_all_test = []\n",
    "training_time_test = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss,tr_acc = train(model_test, train_loader, optimizer, criterion, device)\n",
    "    te_loss,te_acc = evaluate(model_test, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {te_loss:.4f} acc {te_acc:.4f} |\"\n",
    "          f\" in  {time.time()-start_time:.02f}s\"\n",
    "        )\n",
    "    tr_loss_all_test.append(tr_loss)\n",
    "    te_loss_all_test.append(te_loss)\n",
    "    tr_acc_all_test.append(tr_acc)\n",
    "    te_acc_all_test.append(te_acc)\n",
    "    training_time_test.append(time.time() - start_time)\n",
    "\n",
    "    # Save the best model\n",
    "    avg_te_loss = te_loss\n",
    "    if avg_te_loss < best_loss:\n",
    "        print(f\"Current loss ({avg_te_loss:.04f}) lower than previous best loss ({ best_loss:.04f}), Saving current model state\")\n",
    "        best_loss = avg_te_loss\n",
    "        torch.save(model_test.state_dict(), model_dict_name)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-doc",
   "metadata": {},
   "source": [
    "## Plotting training and validation accuracy and loss\n",
    "\n",
    "This visualization cell creates comprehensive learning curves that provide crucial insights into model training dynamics, performance trends, and potential issues such as overfitting or underfitting.\n",
    "\n",
    "The implementation creates two separate plots for different aspects of training analysis:\n",
    "- **Accuracy plot**: Shows classification performance trends over epochs\n",
    "- **Loss plot**: Reveals optimization dynamics and convergence behavior\n",
    "\n",
    "\n",
    "These plots enable several important diagnostic assessments:\n",
    "- **Overfitting detection**: Widening gap between training and validation metrics\n",
    "- **Underfitting identification**: Both metrics plateau at suboptimal levels\n",
    "- **Training completion**: Convergence indicates when to stop training\n",
    "- **Hyperparameter evaluation**: Curves help assess learning rate, regularization effectiveness\n",
    "\n",
    "These learning curves serve as essential tools for understanding model behavior, diagnosing training issues, and making informed decisions about hyperparameter adjustments, training duration, and model architecture modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0050164-56da-44b4-99f5-1851f142aa10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.497842Z",
     "iopub.status.busy": "2025-10-27T07:51:15.497781Z",
     "iopub.status.idle": "2025-10-27T07:51:15.580540Z",
     "shell.execute_reply": "2025-10-27T07:51:15.580201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_w, fig_h = 6,4\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(tr_acc_all, label='Training Accuracy')\n",
    "axs.plot(te_acc_all, label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(tr_loss_all, label='Training Loss')\n",
    "axs.plot(te_loss_all, label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158bb89-889d-430b-90c2-d54c4e313171",
   "metadata": {},
   "source": [
    "## Task: Compare the performance of `model` with `model_test` by plotting the validation loss for `model` and `model_test` ViTs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3cf54d-9029-4094-81ad-e89e6b16ee62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.581771Z",
     "iopub.status.busy": "2025-10-27T07:51:15.581696Z",
     "iopub.status.idle": "2025-10-27T07:51:15.583361Z",
     "shell.execute_reply": "2025-10-27T07:51:15.583011Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c595d-12ed-4e30-8ec9-bb615ca3c591",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(te_loss_all, label='Validation Loss (model)')\n",
    "axs.plot(te_loss_all_test, label='Validation Loss (model_test)')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980db06-f25b-43f5-a20b-4f5e8363bf7c",
   "metadata": {},
   "source": [
    "## Task: Compare the training times of `model` with `model_test` by plotting the training time for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0ef17b1-d965-4706-93d3-0b824fce7313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.584365Z",
     "iopub.status.busy": "2025-10-27T07:51:15.584306Z",
     "iopub.status.idle": "2025-10-27T07:51:15.585849Z",
     "shell.execute_reply": "2025-10-27T07:51:15.585453Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3c82b-063f-4296-8686-10e33a6c633e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(training_time, label='Training time (model)')\n",
    "axs.plot(training_time_test, label='Training time (model_test)')\n",
    "axs.set_title('Training time')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Seconds')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9b69b-4e5e-45eb-9678-5fc2d7204189",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "You have successfully trained the ViT model for the classification of agricultural land from satellite imagery. \n",
    "In this lab, in the interest of time, you have trained the model for 3-5 epochs. However, usually you train the model for around 15-20 epochs, depending on the quality of training data and model metrics for the validation. \n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `torch.save` function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for inferring unclassified images. \n",
    "\n",
    "- You can download the trained model file: `ai_capstone_pytorch_vit_model_state_dict.pth` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-clicking\" on the file and then clicking \"Download\".\n",
    "- This model could be used in other labs of this AI capstone course, instead of the model provided at the above link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf51ac-815d-4762-9b53-350b7316fd00",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a PyTorch-based hybrid Convolutional Neural Network (CNN) and Vision Transformer (ViT) for image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-24  | 1.0  | Aman  |  Created the lab |\n",
    "| 2025-07-24  | 1.1  | Leah Hanson  | QA reviewed for IBM style guide adherence |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3493e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:51:15.587071Z",
     "iopub.status.busy": "2025-10-27T07:51:15.587006Z",
     "iopub.status.idle": "2025-10-27T07:51:15.591317Z",
     "shell.execute_reply": "2025-10-27T07:51:15.590988Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4018262313.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 3 Lab 2: Vision Transformers in PyTorchSolutions for all tasks (12 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M3L2-Vision-Transformers-in-PyTorch-v1.ipynb\"\"\"import osimport timeimport torchimport torch.nn as nnfrom torch.utils.data import DataLoader, random_splitfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# PYTORCH VIT ARCHITECTURE COMPONENTS# =============================================================================class ConvNet(nn.Module):    \"\"\"CNN backbone for feature extraction\"\"\"    def __init__(self, num_classes=2):        super().__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),            nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),            nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),            nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),            nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),            nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024)        )    def forward_features(self, x):        return self.features(x)class PatchEmbed(nn.Module):    \"\"\"Convert CNN features to token embeddings\"\"\"    def __init__(self, input_channel=1024, embed_dim=768):        super().__init__()        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)    def forward(self, x):        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)        return xclass MHSA(nn.Module):    \"\"\"Multi-Head Self Attention\"\"\"    def __init__(self, dim, heads=8, dropout=0.):        super().__init__()        self.heads = heads        self.scale = (dim // heads) ** -0.5        self.qkv = nn.Linear(dim, dim * 3)        self.attn_drop = nn.Dropout(dropout)        self.proj = nn.Linear(dim, dim)        self.proj_drop = nn.Dropout(dropout)    def forward(self, x):        B, N, D = x.shape        q, k, v = self.qkv(x).chunk(3, dim=-1)        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale        attn = self.attn_drop(attn.softmax(dim=-1))        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)        return self.proj_drop(self.proj(x))class TransformerBlock(nn.Module):    \"\"\"Transformer encoder block\"\"\"    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):        super().__init__()        self.norm1 = nn.LayerNorm(dim)        self.attn  = MHSA(dim, heads, dropout)        self.norm2 = nn.LayerNorm(dim)        self.mlp   = nn.Sequential(            nn.Linear(dim, int(dim * mlp_ratio)),            nn.GELU(),            nn.Dropout(dropout),            nn.Linear(int(dim * mlp_ratio), dim),            nn.Dropout(dropout)        )    def forward(self, x):        x = x + self.attn(self.norm1(x))        x = x + self.mlp(self.norm2(x))        return xclass ViT(nn.Module):    \"\"\"Vision Transformer\"\"\"    def __init__(self, in_ch=1024, num_classes=2,                 embed_dim=768, depth=6, heads=8,                 mlp_ratio=4., dropout=0.1, max_tokens=50):        super().__init__()        self.patch = PatchEmbed(in_ch, embed_dim)        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))        self.blocks = nn.ModuleList([            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)            for _ in range(depth)        ])        self.norm = nn.LayerNorm(embed_dim)        self.head = nn.Linear(embed_dim, num_classes)    def forward(self, x):        x = self.patch(x)        B, L, _ = x.shape        cls = self.cls.expand(B, -1, -1)        x = torch.cat((cls, x), 1)        x = x + self.pos[:, :L + 1]        for blk in self.blocks:            x = blk(x)        return self.head(self.norm(x)[:, 0])class CNN_ViT_Hybrid(nn.Module):    \"\"\"CNN-ViT Hybrid model\"\"\"    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):        super().__init__()        self.cnn = ConvNet(num_classes)        self.vit = ViT(num_classes=num_classes,                      embed_dim=embed_dim,                      depth=depth,                      heads=heads)    def forward(self, x):        return self.vit(self.cnn.forward_features(x))# =============================================================================# TASK 1: Define train_transform# =============================================================================img_size = 64# TASK 1 ANSWER:train_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.RandomRotation(40),    transforms.RandomHorizontalFlip(),    transforms.RandomAffine(0, shear=0.2),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])print(\"TASK 1: Training transform created\")print(\"  - Resize(64, 64)\")print(\"  - RandomRotation(40)\")print(\"  - RandomHorizontalFlip()\")print(\"  - RandomAffine(shear=0.2)\")print(\"  - Normalize\")# =============================================================================# TASK 2: Define val_transform# =============================================================================# TASK 2 ANSWER:val_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])print(\"\\nTASK 2: Validation transform created\")print(\"  - Resize(64, 64)\")print(\"  - Normalize (no augmentation)\")# =============================================================================# DATASET LOADING# =============================================================================dataset_path = './images_dataSAT'# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)# Split 80/20train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42))# Apply validation transformval_dataset.dataset.transform = val_transformprint(f\"\\nDataset split:\")print(f\"  Training: {len(train_dataset)}\")print(f\"  Validation: {len(val_dataset)}\")# =============================================================================# TASK 3: Create train_loader and val_loader# =============================================================================batch_size = 32# TASK 3 ANSWER:train_loader = DataLoader(    train_dataset,    batch_size=batch_size,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=batch_size,    shuffle=False,    num_workers=0)print(f\"\\nTASK 3: DataLoaders created\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# TRAINING FUNCTIONS# =============================================================================def train(model, loader, optimizer, criterion, device):    model.train()    loss_sum, correct = 0, 0    for x, y in tqdm(loader, desc=\"Training  \"):        x, y = x.to(device), y.to(device)        optimizer.zero_grad()        out = model(x)        loss = criterion(out, y)        loss.backward()        optimizer.step()        loss_sum += loss.item() * x.size(0)        correct  += (out.argmax(1) == y).sum().item()    return loss_sum / len(loader.dataset), correct / len(loader.dataset)def evaluate(model, loader, criterion, device):    with torch.no_grad():        model.eval()        loss_sum, correct = 0, 0        for x, y in tqdm(loader, desc=\"Validation\"):            x, y = x.to(device), y.to(device)            out = model(x)            loss = criterion(out, y)            loss_sum += loss.item() * x.size(0)            correct  += (out.argmax(1) == y).sum().item()    return loss_sum / len(loader.dataset), correct / len(loader.dataset)# =============================================================================# TASK 4: Train CNN-ViT model with specified hyperparameters# =============================================================================device = \"cuda\" if torch.cuda.is_available() else \"cpu\"print(f\"\\nTraining on device: {device}\")# Hyperparameters (TASK 4 parameters)epochs = 5attn_heads = 12depth = 12embed_dim = 768lr = 0.001print(f\"\\nTASK 4: Training CNN-ViT Hybrid Model\")print(f\"  Epochs: {epochs}\")print(f\"  Attention heads: {attn_heads}\")print(f\"  Transformer depth: {depth}\")print(f\"  Embedding dimension: {embed_dim}\")print(f\"  Learning rate: {lr}\")# Create modelmodel = CNN_ViT_Hybrid(    num_classes=2,    heads=attn_heads,    depth=depth,    embed_dim=embed_dim).to(device)# Load pre-trained CNN weightspytorch_state_dict_path = 'ai-capstone-pytorch-best-model_downloaded.pth'if os.path.exists(pytorch_state_dict_path):    model.cnn.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)    print(f\"  Loaded pre-trained CNN from: {pytorch_state_dict_path}\")criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=lr)# Training loopbest_loss = float('inf')tr_loss_all = []te_loss_all = []tr_acc_all = []te_acc_all = []training_time = []print(\"\\nStarting training...\")for epoch in range(1, epochs + 1):    start_time = time.time()    print(f\"\\nEpoch {epoch:02d}/{epochs:02d}\")    tr_loss, tr_acc = train(model, train_loader, optimizer, criterion, device)    te_loss, te_acc = evaluate(model, val_loader, criterion, device)    epoch_time = time.time() - start_time    print(f\"  Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f}\")    print(f\"  Val Loss:   {te_loss:.4f} | Val Acc:   {te_acc:.4f}\")    print(f\"  Time: {epoch_time:.2f}s\")    tr_loss_all.append(tr_loss)    te_loss_all.append(te_loss)    tr_acc_all.append(tr_acc)    te_acc_all.append(te_acc)    training_time.append(epoch_time)    # Save best model    if te_loss < best_loss:        best_loss = te_loss        torch.save(model.state_dict(), 'pytorch_cnn_vit_model.pth')        print(f\"  *** Saved best model! ***\")print(\"\\nTraining completed!\")# =============================================================================# TASK 5: Plot validation loss comparison (model vs model_test)# =============================================================================# For demonstration, let's create a simpler model (model_test) with different hyperparametersprint(\"\\nCreating model_test with depth=3, heads=6 for comparison...\")model_test = CNN_ViT_Hybrid(    num_classes=2,    heads=6,    depth=3,    embed_dim=768).to(device)if os.path.exists(pytorch_state_dict_path):    model_test.cnn.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)criterion_test = nn.CrossEntropyLoss()optimizer_test = torch.optim.Adam(model_test.parameters(), lr=lr)te_loss_all_test = []training_time_test = []print(\"Training model_test...\")for epoch in range(1, epochs + 1):    start_time = time.time()    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} (model_test)\")    tr_loss, tr_acc = train(model_test, train_loader, optimizer_test, criterion_test, device)    te_loss, te_acc = evaluate(model_test, val_loader, criterion_test, device)    te_loss_all_test.append(te_loss)    training_time_test.append(time.time() - start_time)    print(f\"  Val Loss: {te_loss:.4f} | Val Acc: {te_acc:.4f}\")# TASK 5 ANSWER: Plot validation loss comparisonfig, ax = plt.subplots(figsize=(10, 6))ax.plot(range(1, epochs+1), te_loss_all, label='Model (depth=12, heads=12)', marker='o')ax.plot(range(1, epochs+1), te_loss_all_test, label='Model_test (depth=3, heads=6)', marker='s')ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')ax.set_xlabel('Epoch')ax.set_ylabel('Validation Loss')ax.legend()ax.grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_vit_val_loss_comparison.png', dpi=300)plt.show()print(\"\\nTASK 5: Validation loss comparison plot saved\")# =============================================================================# TASK 6: Plot training time comparison# =============================================================================# TASK 6 ANSWER:fig, ax = plt.subplots(figsize=(10, 6))ax.plot(range(1, epochs+1), training_time, label='Model (depth=12, heads=12)', marker='o')ax.plot(range(1, epochs+1), training_time_test, label='Model_test (depth=3, heads=6)', marker='s')ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')ax.set_xlabel('Epoch')ax.set_ylabel('Time (seconds)')ax.legend()ax.grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_vit_training_time_comparison.png', dpi=300)plt.show()print(\"\\nTASK 6: Training time comparison plot saved\")# =============================================================================# ADDITIONAL PLOTS# =============================================================================# Plot all metricsfig, axes = plt.subplots(2, 2, figsize=(14, 10))# Training Lossaxes[0, 0].plot(range(1, epochs+1), tr_loss_all, marker='o', color='coral')axes[0, 0].set_title('Training Loss')axes[0, 0].set_xlabel('Epoch')axes[0, 0].set_ylabel('Loss')axes[0, 0].grid(True, alpha=0.3)# Validation Lossaxes[0, 1].plot(range(1, epochs+1), te_loss_all, marker='s', color='dodgerblue')axes[0, 1].set_title('Validation Loss')axes[0, 1].set_xlabel('Epoch')axes[0, 1].set_ylabel('Loss')axes[0, 1].grid(True, alpha=0.3)# Training Accuracyaxes[1, 0].plot(range(1, epochs+1), tr_acc_all, marker='o', color='green')axes[1, 0].set_title('Training Accuracy')axes[1, 0].set_xlabel('Epoch')axes[1, 0].set_ylabel('Accuracy')axes[1, 0].grid(True, alpha=0.3)# Validation Accuracyaxes[1, 1].plot(range(1, epochs+1), te_acc_all, marker='s', color='purple')axes[1, 1].set_title('Validation Accuracy')axes[1, 1].set_xlabel('Epoch')axes[1, 1].set_ylabel('Accuracy')axes[1, 1].grid(True, alpha=0.3)plt.suptitle('PyTorch CNN-ViT Training Metrics', fontsize=16, fontweight='bold')plt.tight_layout()plt.savefig('pytorch_vit_all_metrics.png', dpi=300)plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 2 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Created train_transform with augmentation\")print(f\"Task 2: Created val_transform without augmentation\")print(f\"Task 3: Created train_loader and val_loader\")print(f\"Task 4: Trained CNN-ViT (epochs={epochs}, heads={attn_heads}, depth={depth})\")print(f\"Task 5: Plotted validation loss comparison\")print(f\"Task 6: Plotted training time comparison\")print(f\"\\nFinal Model Validation Accuracy: {te_acc_all[-1]:.4f} ({te_acc_all[-1]*100:.2f}%)\")print(f\"Best Validation Loss: {best_loss:.4f}\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 3 Lab 2: Vision Transformers in PyTorchSolutions for all tasks (12 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M3L2-Vision-Transformers-in-PyTorch-v1.ipynb\"\"\"import osimport timeimport torchimport torch.nn as nnfrom torch.utils.data import DataLoader, random_splitfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltimport numpy as np# =============================================================================# PYTORCH VIT ARCHITECTURE COMPONENTS# =============================================================================class ConvNet(nn.Module):    \"\"\"CNN backbone for feature extraction\"\"\"    def __init__(self, num_classes=2):        super().__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),            nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),            nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),            nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),            nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),            nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024)        )    def forward_features(self, x):        return self.features(x)class PatchEmbed(nn.Module):    \"\"\"Convert CNN features to token embeddings\"\"\"    def __init__(self, input_channel=1024, embed_dim=768):        super().__init__()        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)    def forward(self, x):        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)        return xclass MHSA(nn.Module):    \"\"\"Multi-Head Self Attention\"\"\"    def __init__(self, dim, heads=8, dropout=0.):        super().__init__()        self.heads = heads        self.scale = (dim // heads) ** -0.5        self.qkv = nn.Linear(dim, dim * 3)        self.attn_drop = nn.Dropout(dropout)        self.proj = nn.Linear(dim, dim)        self.proj_drop = nn.Dropout(dropout)    def forward(self, x):        B, N, D = x.shape        q, k, v = self.qkv(x).chunk(3, dim=-1)        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale        attn = self.attn_drop(attn.softmax(dim=-1))        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)        return self.proj_drop(self.proj(x))class TransformerBlock(nn.Module):    \"\"\"Transformer encoder block\"\"\"    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):        super().__init__()        self.norm1 = nn.LayerNorm(dim)        self.attn  = MHSA(dim, heads, dropout)        self.norm2 = nn.LayerNorm(dim)        self.mlp   = nn.Sequential(            nn.Linear(dim, int(dim * mlp_ratio)),            nn.GELU(),            nn.Dropout(dropout),            nn.Linear(int(dim * mlp_ratio), dim),            nn.Dropout(dropout)        )    def forward(self, x):        x = x + self.attn(self.norm1(x))        x = x + self.mlp(self.norm2(x))        return xclass ViT(nn.Module):    \"\"\"Vision Transformer\"\"\"    def __init__(self, in_ch=1024, num_classes=2,                 embed_dim=768, depth=6, heads=8,                 mlp_ratio=4., dropout=0.1, max_tokens=50):        super().__init__()        self.patch = PatchEmbed(in_ch, embed_dim)        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))        self.blocks = nn.ModuleList([            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)            for _ in range(depth)        ])        self.norm = nn.LayerNorm(embed_dim)        self.head = nn.Linear(embed_dim, num_classes)    def forward(self, x):        x = self.patch(x)        B, L, _ = x.shape        cls = self.cls.expand(B, -1, -1)        x = torch.cat((cls, x), 1)        x = x + self.pos[:, :L + 1]        for blk in self.blocks:            x = blk(x)        return self.head(self.norm(x)[:, 0])class CNN_ViT_Hybrid(nn.Module):    \"\"\"CNN-ViT Hybrid model\"\"\"    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):        super().__init__()        self.cnn = ConvNet(num_classes)        self.vit = ViT(num_classes=num_classes,                      embed_dim=embed_dim,                      depth=depth,                      heads=heads)    def forward(self, x):        return self.vit(self.cnn.forward_features(x))# =============================================================================# TASK 1: Define train_transform# =============================================================================img_size = 64# TASK 1 ANSWER:train_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.RandomRotation(40),    transforms.RandomHorizontalFlip(),    transforms.RandomAffine(0, shear=0.2),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])print(\"TASK 1: Training transform created\")print(\"  - Resize(64, 64)\")print(\"  - RandomRotation(40)\")print(\"  - RandomHorizontalFlip()\")print(\"  - RandomAffine(shear=0.2)\")print(\"  - Normalize\")# =============================================================================# TASK 2: Define val_transform# =============================================================================# TASK 2 ANSWER:val_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])print(\"\\nTASK 2: Validation transform created\")print(\"  - Resize(64, 64)\")print(\"  - Normalize (no augmentation)\")# =============================================================================# DATASET LOADING# =============================================================================dataset_path = './images_dataSAT'# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)# Split 80/20train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42))# Apply validation transformval_dataset.dataset.transform = val_transformprint(f\"\\nDataset split:\")print(f\"  Training: {len(train_dataset)}\")print(f\"  Validation: {len(val_dataset)}\")# =============================================================================# TASK 3: Create train_loader and val_loader# =============================================================================batch_size = 32# TASK 3 ANSWER:train_loader = DataLoader(    train_dataset,    batch_size=batch_size,    shuffle=True,    num_workers=0)val_loader = DataLoader(    val_dataset,    batch_size=batch_size,    shuffle=False,    num_workers=0)print(f\"\\nTASK 3: DataLoaders created\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Val batches: {len(val_loader)}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# TRAINING FUNCTIONS# =============================================================================def train(model, loader, optimizer, criterion, device):    model.train()    loss_sum, correct = 0, 0    for x, y in tqdm(loader, desc=\"Training  \"):        x, y = x.to(device), y.to(device)        optimizer.zero_grad()        out = model(x)        loss = criterion(out, y)        loss.backward()        optimizer.step()        loss_sum += loss.item() * x.size(0)        correct  += (out.argmax(1) == y).sum().item()    return loss_sum / len(loader.dataset), correct / len(loader.dataset)def evaluate(model, loader, criterion, device):    with torch.no_grad():        model.eval()        loss_sum, correct = 0, 0        for x, y in tqdm(loader, desc=\"Validation\"):            x, y = x.to(device), y.to(device)            out = model(x)            loss = criterion(out, y)            loss_sum += loss.item() * x.size(0)            correct  += (out.argmax(1) == y).sum().item()    return loss_sum / len(loader.dataset), correct / len(loader.dataset)# =============================================================================# TASK 4: Train CNN-ViT model with specified hyperparameters# =============================================================================device = \"cuda\" if torch.cuda.is_available() else \"cpu\"print(f\"\\nTraining on device: {device}\")# Hyperparameters (TASK 4 parameters)epochs = 5attn_heads = 12depth = 12embed_dim = 768lr = 0.001print(f\"\\nTASK 4: Training CNN-ViT Hybrid Model\")print(f\"  Epochs: {epochs}\")print(f\"  Attention heads: {attn_heads}\")print(f\"  Transformer depth: {depth}\")print(f\"  Embedding dimension: {embed_dim}\")print(f\"  Learning rate: {lr}\")# Create modelmodel = CNN_ViT_Hybrid(    num_classes=2,    heads=attn_heads,    depth=depth,    embed_dim=embed_dim).to(device)# Load pre-trained CNN weightspytorch_state_dict_path = 'ai-capstone-pytorch-best-model_downloaded.pth'if os.path.exists(pytorch_state_dict_path):    model.cnn.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)    print(f\"  Loaded pre-trained CNN from: {pytorch_state_dict_path}\")criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=lr)# Training loopbest_loss = float('inf')tr_loss_all = []te_loss_all = []tr_acc_all = []te_acc_all = []training_time = []print(\"\\nStarting training...\")for epoch in range(1, epochs + 1):    start_time = time.time()    print(f\"\\nEpoch {epoch:02d}/{epochs:02d}\")    tr_loss, tr_acc = train(model, train_loader, optimizer, criterion, device)    te_loss, te_acc = evaluate(model, val_loader, criterion, device)    epoch_time = time.time() - start_time    print(f\"  Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f}\")    print(f\"  Val Loss:   {te_loss:.4f} | Val Acc:   {te_acc:.4f}\")    print(f\"  Time: {epoch_time:.2f}s\")    tr_loss_all.append(tr_loss)    te_loss_all.append(te_loss)    tr_acc_all.append(tr_acc)    te_acc_all.append(te_acc)    training_time.append(epoch_time)    # Save best model    if te_loss < best_loss:        best_loss = te_loss        torch.save(model.state_dict(), 'pytorch_cnn_vit_model.pth')        print(f\"  *** Saved best model! ***\")print(\"\\nTraining completed!\")# =============================================================================# TASK 5: Plot validation loss comparison (model vs model_test)# =============================================================================# For demonstration, let's create a simpler model (model_test) with different hyperparametersprint(\"\\nCreating model_test with depth=3, heads=6 for comparison...\")model_test = CNN_ViT_Hybrid(    num_classes=2,    heads=6,    depth=3,    embed_dim=768).to(device)if os.path.exists(pytorch_state_dict_path):    model_test.cnn.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)criterion_test = nn.CrossEntropyLoss()optimizer_test = torch.optim.Adam(model_test.parameters(), lr=lr)te_loss_all_test = []training_time_test = []print(\"Training model_test...\")for epoch in range(1, epochs + 1):    start_time = time.time()    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} (model_test)\")    tr_loss, tr_acc = train(model_test, train_loader, optimizer_test, criterion_test, device)    te_loss, te_acc = evaluate(model_test, val_loader, criterion_test, device)    te_loss_all_test.append(te_loss)    training_time_test.append(time.time() - start_time)    print(f\"  Val Loss: {te_loss:.4f} | Val Acc: {te_acc:.4f}\")# TASK 5 ANSWER: Plot validation loss comparisonfig, ax = plt.subplots(figsize=(10, 6))ax.plot(range(1, epochs+1), te_loss_all, label='Model (depth=12, heads=12)', marker='o')ax.plot(range(1, epochs+1), te_loss_all_test, label='Model_test (depth=3, heads=6)', marker='s')ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')ax.set_xlabel('Epoch')ax.set_ylabel('Validation Loss')ax.legend()ax.grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_vit_val_loss_comparison.png', dpi=300)plt.show()print(\"\\nTASK 5: Validation loss comparison plot saved\")# =============================================================================# TASK 6: Plot training time comparison# =============================================================================# TASK 6 ANSWER:fig, ax = plt.subplots(figsize=(10, 6))ax.plot(range(1, epochs+1), training_time, label='Model (depth=12, heads=12)', marker='o')ax.plot(range(1, epochs+1), training_time_test, label='Model_test (depth=3, heads=6)', marker='s')ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')ax.set_xlabel('Epoch')ax.set_ylabel('Time (seconds)')ax.legend()ax.grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_vit_training_time_comparison.png', dpi=300)plt.show()print(\"\\nTASK 6: Training time comparison plot saved\")# =============================================================================# ADDITIONAL PLOTS# =============================================================================# Plot all metricsfig, axes = plt.subplots(2, 2, figsize=(14, 10))# Training Lossaxes[0, 0].plot(range(1, epochs+1), tr_loss_all, marker='o', color='coral')axes[0, 0].set_title('Training Loss')axes[0, 0].set_xlabel('Epoch')axes[0, 0].set_ylabel('Loss')axes[0, 0].grid(True, alpha=0.3)# Validation Lossaxes[0, 1].plot(range(1, epochs+1), te_loss_all, marker='s', color='dodgerblue')axes[0, 1].set_title('Validation Loss')axes[0, 1].set_xlabel('Epoch')axes[0, 1].set_ylabel('Loss')axes[0, 1].grid(True, alpha=0.3)# Training Accuracyaxes[1, 0].plot(range(1, epochs+1), tr_acc_all, marker='o', color='green')axes[1, 0].set_title('Training Accuracy')axes[1, 0].set_xlabel('Epoch')axes[1, 0].set_ylabel('Accuracy')axes[1, 0].grid(True, alpha=0.3)# Validation Accuracyaxes[1, 1].plot(range(1, epochs+1), te_acc_all, marker='s', color='purple')axes[1, 1].set_title('Validation Accuracy')axes[1, 1].set_xlabel('Epoch')axes[1, 1].set_ylabel('Accuracy')axes[1, 1].grid(True, alpha=0.3)plt.suptitle('PyTorch CNN-ViT Training Metrics', fontsize=16, fontweight='bold')plt.tight_layout()plt.savefig('pytorch_vit_all_metrics.png', dpi=300)plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 3 Lab 2 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Created train_transform with augmentation\")print(f\"Task 2: Created val_transform without augmentation\")print(f\"Task 3: Created train_loader and val_loader\")print(f\"Task 4: Trained CNN-ViT (epochs={epochs}, heads={attn_heads}, depth={depth})\")print(f\"Task 5: Plotted validation loss comparison\")print(f\"Task 6: Plotted training time comparison\")print(f\"\\nFinal Model Validation Accuracy: {te_acc_all[-1]:.4f} ({te_acc_all[-1]*100:.2f}%)\")print(f\"Best Validation Loss: {best_loss:.4f}\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "3ad35712b296d635757464e3c9bafd86292bd10e56c19d75c4d9a65d356701cc",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00743bcc5ea04e25a9a41dfe02d16752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_123adba92ac146b991503bbe1cdb8205",
        "IPY_MODEL_356aee4b3a8b44a69227ccf4ef5627a5",
        "IPY_MODEL_02f001450d8b4fb89d927bc6847741ee"
       ],
       "layout": "IPY_MODEL_e94d0c68b9b24e0bb367e94c4f1332b9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "02f001450d8b4fb89d927bc6847741ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b66e33850d3455e9e093cdaeec0ab64",
       "placeholder": "​",
       "style": "IPY_MODEL_894f0b3648034ac69edec4c1d7f8ddd2",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 10608.75it/s]"
      }
     },
     "0b66e33850d3455e9e093cdaeec0ab64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "123adba92ac146b991503bbe1cdb8205": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7db1e0fde6d74680b8265776671e57a3",
       "placeholder": "​",
       "style": "IPY_MODEL_7d76abafd6854e86aacc23be619b43bc",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "14374833751649b18d602926ed8503fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_97952315c9b24a909fc0ff1e378990c4",
       "placeholder": "​",
       "style": "IPY_MODEL_aa65c09c49f8440eabc15ebbc1d60011",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 12518575.69it/s]"
      }
     },
     "1557058353ee457f8ca66a5c1178455c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7806acdf032f4dee9eacdb3718f82ea5",
        "IPY_MODEL_e11503d4775e4efdb70137c11eb32af7",
        "IPY_MODEL_14374833751649b18d602926ed8503fd"
       ],
       "layout": "IPY_MODEL_f99d54109f594dddba0b642c17b8dc29",
       "tabbable": null,
       "tooltip": null
      }
     },
     "166d59512cf9471ea39821917d4710d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "356aee4b3a8b44a69227ccf4ef5627a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f55d82c5740047a4b1175c40dd3a4d56",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e8e30b8490db4c108e38aaf84275cf5b",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     },
     "5151f2ce04f547fcb53b32f63b55d413": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7806acdf032f4dee9eacdb3718f82ea5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_166d59512cf9471ea39821917d4710d8",
       "placeholder": "​",
       "style": "IPY_MODEL_d6e8611fd9254736adbf713de5f7a443",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "7d76abafd6854e86aacc23be619b43bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7db1e0fde6d74680b8265776671e57a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "894f0b3648034ac69edec4c1d7f8ddd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "97952315c9b24a909fc0ff1e378990c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa65c09c49f8440eabc15ebbc1d60011": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b042e8b289d24b9d9d0bcfa251bae73e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6e8611fd9254736adbf713de5f7a443": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e11503d4775e4efdb70137c11eb32af7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b042e8b289d24b9d9d0bcfa251bae73e",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5151f2ce04f547fcb53b32f63b55d413",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "e8e30b8490db4c108e38aaf84275cf5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e94d0c68b9b24e0bb367e94c4f1332b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f55d82c5740047a4b1175c40dd3a4d56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f99d54109f594dddba0b642c17b8dc29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
