{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf6b95-f975-42df-b63b-f59078b5cd51",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Comparative Analysis of Keras and PyTorch Models </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f6f80-948d-4eb5-bd73-e328fc644dda",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b5aad",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "\n",
    "1. Prepare data, load and evaluate Keras model.\n",
    "2. Prepare data, load and evaluate PyTorch model.\n",
    "3. Compute multiple performance metrics including accuracy, precision, recall, and f1-score.\n",
    "4. Visualize receiver operating characteristic (ROC) curves.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639ce32-38b5-410f-9c8f-7186738b8a38",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will compare the performance of the Keras-based and the PyTorch based convolutional neural network (CNN) models using various evaluation metrics.  Common metrics include:\n",
    "\n",
    "- **Accuracy**: Measures how often the model is correct overall. A higher value means more total predictions are correct.\n",
    "\n",
    "- **Precision**: Measures how many predicted positives are actually correct. A higher value means fewer false positives (incorrectly predicted positives).\n",
    "\n",
    "- **Recall**: Measures how many real positives the model finds. A higher value means fewer false negatives (missed positive cases).\n",
    "\n",
    "- **F1 Score**: Tells us about the balance between precision and recall. A higher value means a better trade-off between precision and recall.\n",
    "\n",
    "- **ROC-AUC**: Measures the model’s ability to distinguish classes. A higher value reflects a model that can better distinguish between classes at all probability thresholds.\n",
    "\n",
    "\n",
    "For all these metrics, the model should aim for values as close to 1.0 (or 100%) as possible. Lower values indicate poorer model performance. There are exceptions for some metrics in other settings (like various loss functions, where lower is better), but for these standard classification metrics, higher is always better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6219bc4-772b-4a74-88b1-4efe292b53b7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "    \n",
    "1. [Data download and extraction](#Data-download-and-extraction)\n",
    "2. [Package installation](#Package-installation)\n",
    "3. [Library imports and setup](#Library-imports-and-setup)\n",
    "4. [Evaluation metrics](#Evaluation-metrics)\n",
    "    1. [Accuracy](#1.-Accuracy)\n",
    "    2. [Precision](#2.-Precision)\n",
    "    3. [Recall](#3.-Recall-(sensitivity-or-true-positive-rate))\n",
    "    4. [F1 score](#4.-F1-score)\n",
    "    5. [Confusion matrix](#5.-Confusion-matrix)\n",
    "    6. [ROC-AUC](#6.-ROC-AUC-(Receiver-operating-characteristic---Area-under-curve))\n",
    "6. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "7. [Model paths and download](#Model-paths-and-download)\n",
    "8. [Dataset path and parameters](#Dataset-path-and-parameters)\n",
    "9. [PyTorch model evaluation and prediction](#PyTorch-model-evaluation-and-prediction)\n",
    "10. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "11. [Keras model evaluation and prediction](#Keras-model-evaluation-and-prediction)\n",
    "12. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "13. [ROC curve plotting](#ROC-curve-plotting)\n",
    "14. [Comparing model performance](#Comparing-model-performance)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f0820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:52.277129Z",
     "iopub.status.busy": "2025-10-27T07:48:52.276693Z",
     "iopub.status.idle": "2025-10-27T07:48:58.963044Z",
     "shell.execute_reply": "2025-10-27T07:48:58.962708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80e3669ad9c4deaa3923af6835bb553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f851b21a15174e2ab92b8bdc98529966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7ca672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:58.964261Z",
     "iopub.status.busy": "2025-10-27T07:48:58.964174Z",
     "iopub.status.idle": "2025-10-27T07:49:00.497473Z",
     "shell.execute_reply": "2025-10-27T07:49:00.496679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 ms, sys: 9.09 ms, total: 12.9 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28739c-8b88-4fc1-901b-7a3c24f8dd4f",
   "metadata": {},
   "source": [
    "### Install PyTorch library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a97ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:00.499289Z",
     "iopub.status.busy": "2025-10-27T07:49:00.499132Z",
     "iopub.status.idle": "2025-10-27T07:49:31.055261Z",
     "shell.execute_reply": "2025-10-27T07:49:31.054619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.7.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch==2.7.0) (2025.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch==2.7.0) (3.0.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\r\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/68.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/68.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/68.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/68.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/68.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:33\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/68.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:33\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/68.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:33\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/68.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:38\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/68.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:39\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/68.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:32\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/68.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:27\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:25\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:23\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:23\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:21\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/68.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:25\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/68.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:24\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/68.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:23\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/68.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:22\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/68.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:21\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.8/68.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:21\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:19\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/68.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:20\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/68.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:19\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/68.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:19\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:17\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:17\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:17\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:16\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/68.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:16\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:16\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:15\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:15\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:15\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:15\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:14\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.5/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:14\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/68.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.8/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.8/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m48.2/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m49.3/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m49.3/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m49.3/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[32m49.3/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━\u001b[0m \u001b[32m50.3/68.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━\u001b[0m \u001b[32m51.4/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m52.4/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m53.5/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m53.5/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m54.5/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━\u001b[0m \u001b[32m55.6/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━\u001b[0m \u001b[32m56.6/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━\u001b[0m \u001b[32m57.7/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m58.7/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m59.8/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m59.8/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m60.8/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m61.9/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m62.9/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m62.9/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m64.0/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━\u001b[0m \u001b[32m65.0/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[32m65.3/68.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[32m66.1/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[32m67.1/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[32m67.1/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m68.2/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m68.2/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m68.2/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m68.2/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m68.2/68.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:23\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.9.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling torch-2.9.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled torch-2.9.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchvision 0.24.0 requires torch==2.9.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-2.7.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 485 ms, sys: 492 ms, total: 977 ms\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348399a-ee14-4050-a752-96f367f21b12",
   "metadata": {},
   "source": [
    "### Install PyTorch helper libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a20f84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:31.056678Z",
     "iopub.status.busy": "2025-10-27T07:49:31.056558Z",
     "iopub.status.idle": "2025-10-27T07:49:32.788424Z",
     "shell.execute_reply": "2025-10-27T07:49:32.787824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.22\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision==0.22) (2.3.4)\r\n",
      "Requirement already satisfied: torch==2.7.0 in ./venv/lib/python3.13/site-packages (from torchvision==0.22) (2.7.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.13/site-packages (from torchvision==0.22) (12.0.0)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch==2.7.0->torchvision==0.22) (2025.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: torchvision\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.24.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling torchvision-0.24.0:\r\n",
      "      Successfully uninstalled torchvision-0.24.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed torchvision-0.22.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 7.17 ms, sys: 5.9 ms, total: 13.1 ms\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66b191c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:32.789691Z",
     "iopub.status.busy": "2025-10-27T07:49:32.789578Z",
     "iopub.status.idle": "2025-10-27T07:49:33.278715Z",
     "shell.execute_reply": "2025-10-27T07:49:33.277954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.19 (from versions: 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.19\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 1.77 ms, sys: 2.99 ms, total: 4.76 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c8f13-46b3-4e8e-a988-3af92750dfee",
   "metadata": {},
   "source": [
    "### Install SkLearn library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9829348b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:33.280845Z",
     "iopub.status.busy": "2025-10-27T07:49:33.280685Z",
     "iopub.status.idle": "2025-10-27T07:49:33.787716Z",
     "shell.execute_reply": "2025-10-27T07:49:33.787163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in ./venv/lib/python3.13/site-packages (1.7.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (2.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.16.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 2.13 ms, sys: 4.02 ms, total: 6.15 ms\n",
      "Wall time: 504 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0fcdf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:33.789085Z",
     "iopub.status.busy": "2025-10-27T07:49:33.788986Z",
     "iopub.status.idle": "2025-10-27T07:49:33.951549Z",
     "shell.execute_reply": "2025-10-27T07:49:33.951171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 18.6 ms, total: 156 ms\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b7cb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:33.952782Z",
     "iopub.status.busy": "2025-10-27T07:49:33.952686Z",
     "iopub.status.idle": "2025-10-27T07:49:43.538680Z",
     "shell.execute_reply": "2025-10-27T07:49:43.538225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 835 ms, sys: 271 ms, total: 1.11 s\n",
      "Wall time: 9.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99804321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:43.539962Z",
     "iopub.status.busy": "2025-10-27T07:49:43.539831Z",
     "iopub.status.idle": "2025-10-27T07:49:46.238364Z",
     "shell.execute_reply": "2025-10-27T07:49:46.237889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 2.24 s, sys: 230 ms, total: 2.47 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae515f",
   "metadata": {},
   "source": [
    "## Evaluation metrics \n",
    "\n",
    "The following metrics are used for evaluation of various AI/ML models:\n",
    "    \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic - Area Under Curve (ROC-AUC)\n",
    "\n",
    "You can read about their calculation methods and their significance for model performance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e4b91-1b94-4563-a099-6a811a328f70",
   "metadata": {},
   "source": [
    "### 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "Accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In other words, it measures how often the classifier is correct overall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\\]\n",
    "\n",
    "- TP: True positives (correctly predicted positive cases)\n",
    "- TN: True negatives (correctly predicted negative cases)\n",
    "- FP: False positives (incorrectly predicted positive cases)\n",
    "- FN: False negatives (incorrectly predicted negative cases)\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "Accuracy is intuitive and easy to interpret, making it a common first metric for model evaluation. However, it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other). This is because a model can achieve high accuracy by simply predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879be2c6-dd4a-4efb-be7a-32a21ec3ba9d",
   "metadata": {},
   "source": [
    "### 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of positive predictions that are actually correct. It answers the question: \"Of all the samples that the model predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Precision is crucial when the cost of a false positive is high. For example, in medical diagnosis, predicting a disease when it's not present (false positive) can lead to unnecessary treatments. In land classification, high precision means that when the model predicts a tile as agricultural, it is likely correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be151b6-8f48-4ec2-9cc9-0e7cf24feac5",
   "metadata": {},
   "source": [
    "### 3. Recall (sensitivity or true positive rate)\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of actual positive cases that were correctly identified by the model. It answers: \"Of all the true positive samples, how many did the model identify?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Recall is important when the cost of missing a positive case (false negative) is high. In land classification, high recall means the model is good at finding all the agricultural land, even if it sometimes mislabels non-agricultural land as agricultural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eba49-ce14-4cef-9fb0-45c8a2a7931c",
   "metadata": {},
   "source": [
    "### 4. F1 score\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is especially useful when you need to find an equilibrium between precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "F1 = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "The F1 score is especially valuable when the class distribution is uneven or when both false positives and false negatives are important. It penalizes extreme values, so a model with high precision but low recall (or vice versa) will have a lower F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48236-1003-4888-b84c-bcd2b5b385e0",
   "metadata": {},
   "source": [
    "### 5. Confusion matrix\n",
    "\n",
    "**Definition:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification algorithm. It displays the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "|               | Predicted positive | Predicted negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Actual positive | True positive (TP) | False negative (FN) |\n",
    "| Actual negative | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**Significance:**\n",
    "The confusion matrix provides a detailed breakdown of model errors and successes, helping you understand not just how often the model is right, but *how* it is wrong. This is crucial for diagnosing issues like class imbalance or systematic misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b5a8b-6836-4fb7-86db-36c6b8efb675",
   "metadata": {},
   "source": [
    "### 6. ROC-AUC (Receiver operating characteristic - Area under curve)\n",
    "\n",
    "**Definition:**\n",
    "ROC-AUC measures the model's ability to distinguish between classes across all possible classification thresholds. The ROC curve plots the true positive rate (recall) against the false positive rate at various thresholds. The AUC (area under the curve) summarizes this performance in a single value between 0 and 1.\n",
    "\n",
    "**Significance:**\n",
    "A model with an ROC-AUC of 1.0 perfectly distinguishes between classes, while a value of 0.5 suggests random guessing. ROC-AUC is especially useful for imbalanced datasets and when you care about the ranking of predictions rather than their absolute values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51947c1-a446-45f4-83d7-b9d8472847e4",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ab94b1-7072-4d1f-8838-cf14c99a17a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:46.239930Z",
     "iopub.status.busy": "2025-10-27T07:49:46.239740Z",
     "iopub.status.idle": "2025-10-27T07:49:46.326051Z",
     "shell.execute_reply": "2025-10-27T07:49:46.325691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.7 ms, sys: 10.5 ms, total: 83.2 ms\n",
      "Wall time: 83.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e211b54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:46.327369Z",
     "iopub.status.busy": "2025-10-27T07:49:46.327204Z",
     "iopub.status.idle": "2025-10-27T07:49:46.329540Z",
     "shell.execute_reply": "2025-10-27T07:49:46.329195Z"
    }
   },
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be04a5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:49:46.330619Z",
     "iopub.status.busy": "2025-10-27T07:49:46.330545Z",
     "iopub.status.idle": "2025-10-27T07:50:27.782800Z",
     "shell.execute_reply": "2025-10-27T07:50:27.782021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded './ai-capstone-keras-best-model-model_downloaded.keras'.\n",
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded './ai_capstone_pytorch_best_model_state_dict_downloaded.pth'.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6a821",
   "metadata": {},
   "source": [
    "## Dataset path and parameters\n",
    "\n",
    "Here, for downstream processing, you define \n",
    "1. the dataset directory path\n",
    "2. define image dimensions\n",
    "3. number of channels\n",
    "4. batch size\n",
    "5. number of classes\n",
    "6. class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87cfea7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:27.784726Z",
     "iopub.status.busy": "2025-10-27T07:50:27.784577Z",
     "iopub.status.idle": "2025-10-27T07:50:27.787353Z",
     "shell.execute_reply": "2025-10-27T07:50:27.786778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2bc67",
   "metadata": {},
   "source": [
    "## Keras model evaluation and prediction\n",
    "\n",
    "In this cell, you will:\n",
    "- Use `ImageDataGenerator` to rescale images.\n",
    "- Load test images from the dataset directory.\n",
    "- Load the saved Keras model using `tf.keras.models.load_model`.\n",
    "- Run predictions on the test set, collect predicted probabilities, predicted classes, and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd7f5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:27.788752Z",
     "iopub.status.busy": "2025-10-27T07:50:27.788648Z",
     "iopub.status.idle": "2025-10-27T07:50:35.154965Z",
     "shell.execute_reply": "2025-10-27T07:50:35.154469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Steps: 47 with batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:   2%|▏         | 1/47 [00:00<00:12,  3.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:   4%|▍         | 2/47 [00:00<00:08,  5.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:   6%|▋         | 3/47 [00:00<00:07,  5.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:   9%|▊         | 4/47 [00:00<00:07,  6.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  11%|█         | 5/47 [00:00<00:06,  6.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  13%|█▎        | 6/47 [00:00<00:06,  6.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  15%|█▍        | 7/47 [00:01<00:05,  6.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  17%|█▋        | 8/47 [00:01<00:05,  6.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  19%|█▉        | 9/47 [00:01<00:05,  6.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  21%|██▏       | 10/47 [00:01<00:05,  6.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  23%|██▎       | 11/47 [00:01<00:05,  6.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  26%|██▌       | 12/47 [00:01<00:04,  7.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  28%|██▊       | 13/47 [00:01<00:04,  7.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  30%|██▉       | 14/47 [00:02<00:04,  7.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  32%|███▏      | 15/47 [00:02<00:04,  7.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  34%|███▍      | 16/47 [00:02<00:04,  7.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  36%|███▌      | 17/47 [00:02<00:04,  7.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  38%|███▊      | 18/47 [00:02<00:04,  7.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  40%|████      | 19/47 [00:02<00:03,  7.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  43%|████▎     | 20/47 [00:02<00:03,  7.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  45%|████▍     | 21/47 [00:03<00:03,  7.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  47%|████▋     | 22/47 [00:03<00:03,  7.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  49%|████▉     | 23/47 [00:03<00:03,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  51%|█████     | 24/47 [00:03<00:03,  7.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  53%|█████▎    | 25/47 [00:03<00:03,  7.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  55%|█████▌    | 26/47 [00:03<00:02,  7.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  57%|█████▋    | 27/47 [00:03<00:02,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  60%|█████▉    | 28/47 [00:04<00:02,  7.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  62%|██████▏   | 29/47 [00:04<00:02,  7.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  64%|██████▍   | 30/47 [00:04<00:02,  7.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  66%|██████▌   | 31/47 [00:04<00:02,  7.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  68%|██████▊   | 32/47 [00:04<00:02,  7.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  70%|███████   | 33/47 [00:04<00:01,  7.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  72%|███████▏  | 34/47 [00:04<00:01,  7.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  74%|███████▍  | 35/47 [00:05<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  77%|███████▋  | 36/47 [00:05<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  79%|███████▊  | 37/47 [00:05<00:01,  7.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  81%|████████  | 38/47 [00:05<00:01,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  83%|████████▎ | 39/47 [00:05<00:01,  7.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  85%|████████▌ | 40/47 [00:05<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  87%|████████▋ | 41/47 [00:05<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  89%|████████▉ | 42/47 [00:06<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  91%|█████████▏| 43/47 [00:06<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  94%|█████████▎| 44/47 [00:06<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  96%|█████████▌| 45/47 [00:06<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps:  98%|█████████▊| 46/47 [00:06<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps: 100%|██████████| 47/47 [00:06<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Steps: 100%|██████████| 47/47 [00:06<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.9 s, sys: 3.49 s, total: 57.4 s\n",
      "Wall time: 7.36 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "steps = int(np.ceil(prediction_generator.samples / prediction_generator.batch_size))\n",
    "batch_size = int(prediction_generator.batch_size)\n",
    "print(f\"Number of Steps: {steps} with batch size: {batch_size}\")\n",
    "\n",
    "all_preds_keras = []\n",
    "all_probs_keras = []\n",
    "all_labels_keras = []\n",
    "\n",
    "for step_idx, step in enumerate(tqdm(range(steps), desc=\"Steps\")):\n",
    "    images, labels = next(prediction_generator)\n",
    "    preds = keras_model.predict(images, verbose='0')\n",
    "    all_probs_keras.extend(preds)\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    all_preds_keras.extend(preds)\n",
    "    all_labels_keras.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d28b7-43ac-49c0-ac58-4b13fcf76fff",
   "metadata": {},
   "source": [
    "#### Question: What does the code **`preds > 0.5`** in line `preds = (preds > 0.5).astype(int).flatten()` do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05026e2c-8187-4651-9dab-cbfe19b644ce",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5ad27-832d-4f14-87d4-9ef5b856af62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6008b4c1-2d7d-4a93-9714-a687b3e29a5e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It converts all predictions greater than 0.5 to True or assign to class 1. Rest of the predictions are False, assigned to class 0\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971389d2-f67f-4c21-a79a-2db089082422",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec013b-f37f-4231-a33f-952650170d71",
   "metadata": {},
   "source": [
    "### Task 1: Print the performance metrics for the Keras model using `print_metrics` function\n",
    "\n",
    "Print various performance metrics for the **Keras** model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ab980a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:35.156475Z",
     "iopub.status.busy": "2025-10-27T07:50:35.156376Z",
     "iopub.status.idle": "2025-10-27T07:50:35.158011Z",
     "shell.execute_reply": "2025-10-27T07:50:35.157688Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2e854-e1c3-4d08-a0a3-eaa09340a389",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b613b4-911e-485c-9de7-f98b3c02e068",
   "metadata": {},
   "source": [
    "#### Question: What is the significance of `f1 score`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bfe96-2d87-46f1-9374-3d2755aae0c4",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b7c86-c3a3-40ec-bc53-de6df17dea29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e571bbc-2708-4db9-b9ca-6573c0bd095d",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It is useful when both false positives and false negatives are important\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c22e",
   "metadata": {},
   "source": [
    "## PyTorch model evaluation and prediction\n",
    "\n",
    "In this cell, you:\n",
    "- Set device for inference (GPU if available).\n",
    "- Define data transformations including resizing, normalization.\n",
    "- Load the dataset using `ImageFolder` and prepares a DataLoader.\n",
    "- Define the CNN architecture matching the saved state dict.\n",
    "- Load model weights.\n",
    "- Run inference on the test set, collecting predicted classes, probabilities, and true labels for metric calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d240b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:35.159254Z",
     "iopub.status.busy": "2025-10-27T07:50:35.159181Z",
     "iopub.status.idle": "2025-10-27T07:50:43.293559Z",
     "shell.execute_reply": "2025-10-27T07:50:43.293179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inference on cpu\n",
      "Created model, now loading the weights from saved model state dict\n",
      "Loaded model state dict, now getting predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:   2%|▏         | 1/47 [00:00<00:10,  4.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:   4%|▍         | 2/47 [00:00<00:08,  5.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:   6%|▋         | 3/47 [00:00<00:08,  5.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:   9%|▊         | 4/47 [00:00<00:07,  5.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  11%|█         | 5/47 [00:00<00:07,  5.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  13%|█▎        | 6/47 [00:01<00:07,  5.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  15%|█▍        | 7/47 [00:01<00:07,  5.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  17%|█▋        | 8/47 [00:01<00:06,  5.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  19%|█▉        | 9/47 [00:01<00:06,  5.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  21%|██▏       | 10/47 [00:01<00:06,  5.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  23%|██▎       | 11/47 [00:01<00:06,  5.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  26%|██▌       | 12/47 [00:02<00:06,  5.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  28%|██▊       | 13/47 [00:02<00:05,  5.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  30%|██▉       | 14/47 [00:02<00:05,  5.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  32%|███▏      | 15/47 [00:02<00:05,  5.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  34%|███▍      | 16/47 [00:02<00:05,  5.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  36%|███▌      | 17/47 [00:03<00:05,  5.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  38%|███▊      | 18/47 [00:03<00:05,  5.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  40%|████      | 19/47 [00:03<00:04,  5.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  43%|████▎     | 20/47 [00:03<00:04,  5.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  45%|████▍     | 21/47 [00:03<00:04,  5.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  47%|████▋     | 22/47 [00:03<00:04,  5.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  49%|████▉     | 23/47 [00:04<00:04,  5.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  51%|█████     | 24/47 [00:04<00:03,  5.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  53%|█████▎    | 25/47 [00:04<00:03,  5.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  55%|█████▌    | 26/47 [00:04<00:03,  5.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  57%|█████▋    | 27/47 [00:04<00:03,  5.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  60%|█████▉    | 28/47 [00:04<00:03,  5.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  62%|██████▏   | 29/47 [00:05<00:03,  5.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  64%|██████▍   | 30/47 [00:05<00:02,  5.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  66%|██████▌   | 31/47 [00:05<00:02,  5.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  68%|██████▊   | 32/47 [00:05<00:02,  5.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  70%|███████   | 33/47 [00:05<00:02,  5.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  72%|███████▏  | 34/47 [00:05<00:02,  5.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  74%|███████▍  | 35/47 [00:06<00:02,  5.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  77%|███████▋  | 36/47 [00:06<00:01,  6.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  79%|███████▊  | 37/47 [00:06<00:01,  6.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  81%|████████  | 38/47 [00:06<00:01,  6.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  83%|████████▎ | 39/47 [00:06<00:01,  6.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  85%|████████▌ | 40/47 [00:06<00:01,  6.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  87%|████████▋ | 41/47 [00:07<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  89%|████████▉ | 42/47 [00:07<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  91%|█████████▏| 43/47 [00:07<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  94%|█████████▎| 44/47 [00:07<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  96%|█████████▌| 45/47 [00:07<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step:  98%|█████████▊| 46/47 [00:07<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 100%|██████████| 47/47 [00:08<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 100%|██████████| 47/47 [00:08<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 s, sys: 2.46 s, total: 48.9 s\n",
      "Wall time: 8.13 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Processing inference on {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "    nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "    nn.Linear(2048, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(\"Created model, now loading the weights from saved model state dict\")\n",
    "model.load_state_dict(torch.load(pytorch_state_dict_path))\n",
    "print(\"Loaded model state dict, now getting predictions\")\n",
    "\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc86e4a-c572-439b-b23a-5e272299caf9",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119d485-8fbd-4c4e-9405-505369288e48",
   "metadata": {},
   "source": [
    "### Task 2: Print the performance metrics for the PyTorch model using `print_metrics`\n",
    "\n",
    "Print various performance metrics for the PyTorch model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47c2d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:43.294764Z",
     "iopub.status.busy": "2025-10-27T07:50:43.294680Z",
     "iopub.status.idle": "2025-10-27T07:50:43.296299Z",
     "shell.execute_reply": "2025-10-27T07:50:43.295971Z"
    }
   },
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c22ac-e1fa-489e-b886-8de9a7ff3848",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0a22b-3cf1-4543-b012-77c192042eff",
   "metadata": {},
   "source": [
    "#### Question: What are the total number of false negatives in the `confusion matrix` in the PyTorch model evaluated above? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf7d8f-a2c8-4e4f-9744-ccaf46354f70",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b7d4a-463d-4b46-96d7-1e333f542933",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14f876e-6165-4553-90f3-fcdd9bd1d056",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"Total Flase negatives are 5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebfcf7",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdd1da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:43.297507Z",
     "iopub.status.busy": "2025-10-27T07:50:43.297444Z",
     "iopub.status.idle": "2025-10-27T07:50:43.299800Z",
     "shell.execute_reply": "2025-10-27T07:50:43.299489Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792448",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "097b5831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:43.300825Z",
     "iopub.status.busy": "2025-10-27T07:50:43.300755Z",
     "iopub.status.idle": "2025-10-27T07:50:43.384940Z",
     "shell.execute_reply": "2025-10-27T07:50:43.384486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e6e57-9764-44a3-8566-cef177c1e684",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f871167c-5e80-4f64-bc89-106ffc63935b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:43.386216Z",
     "iopub.status.busy": "2025-10-27T07:50:43.386118Z",
     "iopub.status.idle": "2025-10-27T07:50:43.443119Z",
     "shell.execute_reply": "2025-10-27T07:50:43.442741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMetric\u001b[0m     Keras Model     PyTorch Model  \n",
      "\u001b[1mAccuracy\u001b[0m   0.9925          0.9988         \n",
      "\u001b[1mPrecision\u001b[0m  1.0000          0.9983         \n",
      "\u001b[1mRecall\u001b[0m     0.9850          0.9993         \n",
      "\u001b[1mF1 Score\u001b[0m   0.9924          0.9988         \n",
      "\u001b[1mROC-AUC\u001b[0m    1.0000          1.0000         \n"
     ]
    }
   ],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "\n",
    "mertics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in mertics_list:\n",
    "    print(\"{:<18} {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5367255-39c2-473d-95bb-393e8baf6622",
   "metadata": {},
   "source": [
    "### Metric analysis\n",
    "\n",
    "The metrics for the pre-trained Keras and PyTorch models for evaluating the provided dataset are:\n",
    "\n",
    "- **Accuracy**\n",
    "    1. Keras: 0.9925\n",
    "    2. PyTorch: 0.9988\n",
    "    \n",
    "    ===> Both models achieve exceptional accuracy, but the **PyTorch model makes fewer mistakes**.\n",
    "\n",
    "- **Precision**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 0.9983\n",
    "\n",
    "    ===> The **Keras** model perfectly **avoids false positives**, whereas the PyTorch model is slightly less perfect but still excellent.\n",
    "\n",
    "- **Recall**\n",
    "    1. Keras: 0.9850\n",
    "    2. PyTorch: 0.9993\n",
    "    \n",
    "    ===> The **PyTorch** model is marginally better at **identifying all true positives**, capturing nearly all actual positive cases, while the Keras model misses a few.\n",
    "\n",
    "- **F1 Score**\n",
    "    1. PyTorch: 0.9988\n",
    "    2. Keras: 0.9924\n",
    "    \n",
    "    ===> The F1 score, which balances precision and recall, favors the **PyTorch** model thanks to its **stronger recall**.\n",
    "\n",
    "- **ROC-AUC**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 1.0000\n",
    "    \n",
    "    ===> Both models reach maximum possible **discrimination between classes**, indicating outstanding capability for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30912d-7ebe-4a8f-b5f1-d14fb1a5a12f",
   "metadata": {},
   "source": [
    "### **Model comparison: Key insights**\n",
    "\n",
    "\n",
    "**PyTorch model strengths**\n",
    "\n",
    " - Achieves the highest scores in accuracy, recall, and F1, indicating extremely robust overall performance and near-perfect classification of positive cases\n",
    "- ROC-AUC of 1.0 shows perfect class separability\n",
    "\n",
    "\n",
    "**Keras model strengths**\n",
    "\n",
    "- Displays almost perfect precision every positive prediction made is correct\n",
    "- Also achieves perfect ROC-AUC, indicating outstanding discrimination ability\n",
    "\n",
    "\n",
    "**Common strength**\n",
    "\n",
    "- Both models deliver flawless ROC-AUC, suggesting both are highly effective for this classification task\n",
    "\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "Based on the scores from the uploaded pre-trained models:\n",
    "\n",
    "- The PyTorch model is preferable for applications where missing any positive instances is costly (higher recall)\n",
    "- The Keras model is optimal for scenarios where making any false positive error is unacceptable (higher precision).\n",
    "\n",
    "\n",
    "**Next**\n",
    "\n",
    "- Analyze the confusion matrices to investigate the errors.\n",
    "- Monitor real-world performance, as even marginal differences can become important in high-impact applications. \n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Both models excel in all evaluated metrics and would be highly reliable in production. The PyTorch model demonstrates a modest edge in recall and F1 score, while the Keras model maximizes precision. The choice between models should ultimately reflect the specific requirements and risk tolerance of your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb79672-3cdf-47b4-ae97-43d53965d480",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully evaluated and compared two deep learning models, one using Keras and the other using the PyTorch framework.\n",
    "\n",
    "You learnt about a comprehensive workflow for comparing Keras and PyTorch models on the same dataset and got hands-on experience on:\n",
    "- data preparation\n",
    "- model loading\n",
    "- predicting dataset\n",
    "- metric computation\n",
    "- ROC visualization\n",
    "- Model performance comparison\n",
    "\n",
    "Using these framework independent metrics, you now know how to evaluate different models for their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf898801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:50:43.444769Z",
     "iopub.status.busy": "2025-10-27T07:50:43.444679Z",
     "iopub.status.idle": "2025-10-27T07:50:43.449707Z",
     "shell.execute_reply": "2025-10-27T07:50:43.449234Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (168268951.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 2 Lab 3: Comparative Analysis of Keras and PyTorch ModelsSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L3-Comparative-Analysis-of-Keras-and-PyTorch-Models-v1.ipynb\"\"\"import numpy as npimport matplotlib.pyplot as pltfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,                            roc_auc_score, confusion_matrix, classification_report,                            ConfusionMatrixDisplay)# =============================================================================# SETUP: Load both Keras and PyTorch models (from previous labs)# =============================================================================print(\"=\"*70)print(\"Module 2 Lab 3: Comparative Analysis\")print(\"=\"*70)# This assumes you have already:# 1. Trained Keras model in M2L1 and saved predictions# 2. Trained PyTorch model in M2L2 and saved predictions# Example: Loading saved predictions (adjust based on how you saved them)# keras_preds = np.load('keras_predictions.npy')# keras_labels = np.load('keras_labels.npy')# pytorch_preds = np.load('pytorch_predictions.npy')# pytorch_labels = np.load('pytorch_labels.npy')# =============================================================================# QUESTION 1: What does preds > 0.5 do?# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 1: What does 'preds > 0.5' do?\")print(\"=\"*70)print(\"\"\"The expression 'preds > 0.5' performs element-wise threshold comparison onprediction probabilities to convert them to binary class labels.**Detailed Explanation:**1. **Input Format**:   - preds: array of probabilities, e.g., [0.23, 0.78, 0.91, 0.12]   - Range: [0, 1] (output from sigmoid activation)2. **Threshold Operation**:   - preds > 0.5 creates boolean array: [False, True, True, False]3. **Conversion to Binary**:   - .astype(int): Converts to integers [0, 1, 1, 0]4. **Why 0.5?**:   - Default decision boundary for binary classification   - p > 0.5 means model is more confident in class 1   - p <= 0.5 means model is more confident in class 0**Complete Expression:**   preds = (preds > 0.5).astype(int).flatten()   - preds > 0.5: Boolean comparison   - .astype(int): Convert True/False to 1/0   - .flatten(): Convert 2D array to 1D**Example:**   Input:  [[0.23], [0.78], [0.91], [0.12]]   > 0.5:  [[False], [True], [True], [False]]   .astype: [[0], [1], [1], [0]]   .flatten: [0, 1, 1, 0]**Customizable Threshold:**   - Use preds > 0.3 for higher recall (catch more positives)   - Use preds > 0.7 for higher precision (fewer false positives)\"\"\")# =============================================================================# HELPER FUNCTION: Print metrics# =============================================================================def print_metrics(y_true, y_pred, y_prob, model_name, class_labels=['Non-Agri', 'Agri']):    \"\"\"    Compute and print comprehensive metrics for model evaluation    \"\"\"    # Compute metrics    accuracy = accuracy_score(y_true, y_pred)    precision = precision_score(y_true, y_pred, average='binary')    recall = recall_score(y_true, y_pred, average='binary')    f1 = f1_score(y_true, y_pred, average='binary')    # ROC-AUC (need probabilities)    if y_prob is not None:        if len(y_prob.shape) > 1 and y_prob.shape[1] > 1:            # Multi-class probabilities, use class 1            roc_auc = roc_auc_score(y_true, y_prob[:, 1])        else:            # Binary probabilities            roc_auc = roc_auc_score(y_true, y_prob)    else:        roc_auc = None    # Print results    print(f\"\\n{'='*70}\")    print(f\"Evaluation Metrics for {model_name}\")    print(f\"{'='*70}\")    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")    print(f\"  Precision: {precision:.4f}\")    print(f\"  Recall:    {recall:.4f}\")    print(f\"  F1-Score:  {f1:.4f}\")    if roc_auc is not None:        print(f\"  ROC-AUC:   {roc_auc:.4f}\")    # Classification report    print(f\"\\n  Classification Report:\")    print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))    # Confusion matrix    cm = confusion_matrix(y_true, y_pred)    print(f\"  Confusion Matrix:\")    print(f\"                Predicted\")    print(f\"              0        1\")    print(f\"    Actual 0  {cm[0,0]:<8} {cm[0,1]:<8}\")    print(f\"           1  {cm[1,0]:<8} {cm[1,1]:<8}\")    # Confusion matrix visualization    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)    fig, ax = plt.subplots(figsize=(8, 6))    disp.plot(ax=ax, cmap='Blues', values_format='d')    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')    plt.tight_layout()    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png', dpi=300)    plt.show()    return {        'accuracy': accuracy,        'precision': precision,        'recall': recall,        'f1_score': f1,        'roc_auc': roc_auc,        'confusion_matrix': cm    }# =============================================================================# TASK 1: Print Keras model metrics using print_metrics# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 1: Keras Model Evaluation\")print(\"=\"*70)# Example: If you have Keras predictions loaded# Assuming from previous lab:# - keras_preds: Binary predictions (0 or 1)# - keras_labels: True labels (0 or 1)# - keras_probs: Probabilities for class 1# TASK 1 ANSWER (uncomment and use your actual data):# keras_metrics = print_metrics(#     y_true=keras_labels,#     y_pred=keras_preds,#     y_prob=keras_probs,#     model_name=\"Keras CNN Model\",#     class_labels=['Non-Agricultural', 'Agricultural']# )print(\"\"\"To complete Task 1, use:keras_metrics = print_metrics(    y_true=all_labels_keras,    y_pred=all_preds_keras,    y_prob=all_probs_keras,    model_name=\"Keras CNN Model\",    class_labels=['Non-Agricultural', 'Agricultural'])\"\"\")# =============================================================================# QUESTION 3: Explain the significance of the F1-score# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 3: Significance of F1-Score\")print(\"=\"*70)print(\"\"\"The F1-Score is the harmonic mean of Precision and Recall, providing a singlemetric that balances both measures of classification performance.**Formula:**   F1 = 2 * (Precision * Recall) / (Precision + Recall)**Why F1-Score is Important:**1. **Balances Precision and Recall**:   - Precision alone doesn't tell about false negatives   - Recall alone doesn't tell about false positives   - F1-Score considers both2. **Handles Class Imbalance**:   - Better than accuracy for imbalanced datasets   - Example: 95% non-agri, 5% agri     * Model predicting all non-agri: 95% accuracy, 0% F1 for agri class     * F1-Score reveals the poor performance3. **Single Performance Metric**:   - Easy to compare models   - Useful for model selection and hyperparameter tuning   - Better represents overall performance than accuracy alone4. **Domain-Specific Interpretation**:   - Medical diagnosis: High recall (catch all diseases)   - Spam detection: High precision (avoid false positives)   - F1-Score helps balance based on use case**Example Scenarios:**Scenario A (High Precision, Low Recall):   - Precision: 0.95 (few false positives)   - Recall: 0.60 (many false negatives)   - F1-Score: 0.73Scenario B (Balanced):   - Precision: 0.85   - Recall: 0.85   - F1-Score: 0.85Scenario C (Low Precision, High Recall):   - Precision: 0.60 (many false positives)   - Recall: 0.95 (few false negatives)   - F1-Score: 0.73**For Land Classification:**   - High F1-Score ensures we correctly identify agricultural land   - Minimizes both missing agricultural areas (false negatives)   - And incorrectly marking non-agricultural as agricultural (false positives)   - Critical for business decisions (fertilizer company expansion)\"\"\")# =============================================================================# TASK 2: Print PyTorch model metrics using print_metrics# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 2: PyTorch Model Evaluation\")print(\"=\"*70)# Example: If you have PyTorch predictions loaded# pytorch_metrics = print_metrics(#     y_true=pytorch_labels,#     y_pred=pytorch_preds,#     y_prob=pytorch_probs,#     model_name=\"PyTorch CNN Model\",#     class_labels=['Non-Agricultural', 'Agricultural']# )print(\"\"\"To complete Task 2, use:pytorch_metrics = print_metrics(    y_true=all_labels_pytorch,    y_pred=all_preds_pytorch,    y_prob=all_probs_pytorch,    model_name=\"PyTorch CNN Model\",    class_labels=['Non-Agricultural', 'Agricultural'])\"\"\")# =============================================================================# QUESTION 5: Count false negatives in PyTorch confusion matrix# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 5: Count False Negatives in Confusion Matrix\")print(\"=\"*70)print(\"\"\"False Negatives (FN) are located in the confusion matrix at position [1, 0]:   Confusion Matrix Structure:                   Predicted                   0      1         Actual 0  TN     FP                1  FN     TP   Where:   - TN (True Negative): [0, 0] - Correctly predicted as class 0   - FP (False Positive): [0, 1] - Incorrectly predicted as class 1   - FN (False Negative): [1, 0] - Incorrectly predicted as class 0   - TP (True Positive): [1, 1] - Correctly predicted as class 1**False Negatives Explained:**   - Actual class: 1 (Agricultural)   - Predicted class: 0 (Non-Agricultural)   - Model failed to detect agricultural land**How to Extract:**   cm = confusion_matrix(y_true, y_pred)   false_negatives = cm[1, 0]**Business Impact for Land Classification:**   - False negatives = missed agricultural land   - Fertilizer company misses expansion opportunities   - Potential revenue loss   - Should minimize FN to capture all potential markets**Example:**   cm = [[850, 50],     # Row 0: Non-Agricultural         [30, 920]]      # Row 1: Agricultural   - False Negatives (FN) = cm[1, 0] = 30   - Meaning: 30 agricultural lands were incorrectly classified as non-agricultural\"\"\")# Example code to extract false negatives# cm_pytorch = pytorch_metrics['confusion_matrix']# false_negatives_pytorch = cm_pytorch[1, 0]# print(f\"\\nFalse Negatives in PyTorch Model: {false_negatives_pytorch}\")# =============================================================================# COMPARATIVE ANALYSIS# =============================================================================def compare_models(keras_metrics, pytorch_metrics):    \"\"\"    Compare Keras and PyTorch models side-by-side    \"\"\"    print(\"\\n\" + \"=\"*70)    print(\"MODEL COMPARISON: Keras vs PyTorch\")    print(\"=\"*70)    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']    keras_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']    pytorch_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']    print(f\"\\n{'Metric':<15} | {'Keras':<12} | {'PyTorch':<12} | {'Difference':<12}\")    print(\"-\" * 70)    for name, k_key, p_key in zip(metrics_names, keras_keys, pytorch_keys):        k_val = keras_metrics[k_key]        p_val = pytorch_metrics[p_key]        if k_val is not None and p_val is not None:            diff = p_val - k_val            diff_str = f\"{diff:+.4f}\"            print(f\"{name:<15} | {k_val:.4f}      | {p_val:.4f}      | {diff_str}\")    # Confusion matrix comparison    print(\"\\n\" + \"=\"*70)    print(\"CONFUSION MATRIX COMPARISON\")    print(\"=\"*70)    cm_keras = keras_metrics['confusion_matrix']    cm_pytorch = pytorch_metrics['confusion_matrix']    print(\"\\nKeras CNN:\")    print(f\"  TN: {cm_keras[0,0]:<6} FP: {cm_keras[0,1]:<6}\")    print(f\"  FN: {cm_keras[1,0]:<6} TP: {cm_keras[1,1]:<6}\")    print(\"\\nPyTorch CNN:\")    print(f\"  TN: {cm_pytorch[0,0]:<6} FP: {cm_pytorch[0,1]:<6}\")    print(f\"  FN: {cm_pytorch[1,0]:<6} TP: {cm_pytorch[1,1]:<6}\")    # Visualization    fig, axes = plt.subplots(1, 2, figsize=(14, 5))    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']    keras_vals = [keras_metrics[m] for m in metrics_to_plot]    pytorch_vals = [pytorch_metrics[m] for m in metrics_to_plot]    x = np.arange(len(metrics_to_plot))    width = 0.35    axes[0].bar(x - width/2, keras_vals, width, label='Keras', color='skyblue')    axes[0].bar(x + width/2, pytorch_vals, width, label='PyTorch', color='coral')    axes[0].set_xlabel('Metric')    axes[0].set_ylabel('Score')    axes[0].set_title('Model Comparison: Metrics', fontweight='bold')    axes[0].set_xticks(x)    axes[0].set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'], rotation=45)    axes[0].legend()    axes[0].grid(True, alpha=0.3, axis='y')    # Training time comparison (if available)    # axes[1].bar(['Keras', 'PyTorch'], [keras_time, pytorch_time], color=['skyblue', 'coral'])    # axes[1].set_ylabel('Training Time (seconds)')    # axes[1].set_title('Training Time Comparison', fontweight='bold')    plt.tight_layout()    plt.savefig('keras_vs_pytorch_comparison.png', dpi=300)    plt.show()# Example usage (uncomment when you have both sets of metrics):# compare_models(keras_metrics, pytorch_metrics)# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 3 - All Tasks Completed\")print(\"=\"*70)print(\"Question 1: Explained 'preds > 0.5' threshold operation\")print(\"Task 1:     Printed Keras model metrics using print_metrics()\")print(\"Question 3: Explained significance of F1-Score\")print(\"Task 2:     Printed PyTorch model metrics using print_metrics()\")print(\"Question 5: Explained how to count False Negatives (cm[1,0])\")print(\"\\nComparative analysis complete - ready for Module 3!\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 2 Lab 3: Comparative Analysis of Keras and PyTorch ModelsSolutions for all tasks (10 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L3-Comparative-Analysis-of-Keras-and-PyTorch-Models-v1.ipynb\"\"\"import numpy as npimport matplotlib.pyplot as pltfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,                            roc_auc_score, confusion_matrix, classification_report,                            ConfusionMatrixDisplay)# =============================================================================# SETUP: Load both Keras and PyTorch models (from previous labs)# =============================================================================print(\"=\"*70)print(\"Module 2 Lab 3: Comparative Analysis\")print(\"=\"*70)# This assumes you have already:# 1. Trained Keras model in M2L1 and saved predictions# 2. Trained PyTorch model in M2L2 and saved predictions# Example: Loading saved predictions (adjust based on how you saved them)# keras_preds = np.load('keras_predictions.npy')# keras_labels = np.load('keras_labels.npy')# pytorch_preds = np.load('pytorch_predictions.npy')# pytorch_labels = np.load('pytorch_labels.npy')# =============================================================================# QUESTION 1: What does preds > 0.5 do?# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 1: What does 'preds > 0.5' do?\")print(\"=\"*70)print(\"\"\"The expression 'preds > 0.5' performs element-wise threshold comparison onprediction probabilities to convert them to binary class labels.**Detailed Explanation:**1. **Input Format**:   - preds: array of probabilities, e.g., [0.23, 0.78, 0.91, 0.12]   - Range: [0, 1] (output from sigmoid activation)2. **Threshold Operation**:   - preds > 0.5 creates boolean array: [False, True, True, False]3. **Conversion to Binary**:   - .astype(int): Converts to integers [0, 1, 1, 0]4. **Why 0.5?**:   - Default decision boundary for binary classification   - p > 0.5 means model is more confident in class 1   - p <= 0.5 means model is more confident in class 0**Complete Expression:**   preds = (preds > 0.5).astype(int).flatten()   - preds > 0.5: Boolean comparison   - .astype(int): Convert True/False to 1/0   - .flatten(): Convert 2D array to 1D**Example:**   Input:  [[0.23], [0.78], [0.91], [0.12]]   > 0.5:  [[False], [True], [True], [False]]   .astype: [[0], [1], [1], [0]]   .flatten: [0, 1, 1, 0]**Customizable Threshold:**   - Use preds > 0.3 for higher recall (catch more positives)   - Use preds > 0.7 for higher precision (fewer false positives)\"\"\")# =============================================================================# HELPER FUNCTION: Print metrics# =============================================================================def print_metrics(y_true, y_pred, y_prob, model_name, class_labels=['Non-Agri', 'Agri']):    \"\"\"    Compute and print comprehensive metrics for model evaluation    \"\"\"    # Compute metrics    accuracy = accuracy_score(y_true, y_pred)    precision = precision_score(y_true, y_pred, average='binary')    recall = recall_score(y_true, y_pred, average='binary')    f1 = f1_score(y_true, y_pred, average='binary')    # ROC-AUC (need probabilities)    if y_prob is not None:        if len(y_prob.shape) > 1 and y_prob.shape[1] > 1:            # Multi-class probabilities, use class 1            roc_auc = roc_auc_score(y_true, y_prob[:, 1])        else:            # Binary probabilities            roc_auc = roc_auc_score(y_true, y_prob)    else:        roc_auc = None    # Print results    print(f\"\\n{'='*70}\")    print(f\"Evaluation Metrics for {model_name}\")    print(f\"{'='*70}\")    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")    print(f\"  Precision: {precision:.4f}\")    print(f\"  Recall:    {recall:.4f}\")    print(f\"  F1-Score:  {f1:.4f}\")    if roc_auc is not None:        print(f\"  ROC-AUC:   {roc_auc:.4f}\")    # Classification report    print(f\"\\n  Classification Report:\")    print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))    # Confusion matrix    cm = confusion_matrix(y_true, y_pred)    print(f\"  Confusion Matrix:\")    print(f\"                Predicted\")    print(f\"              0        1\")    print(f\"    Actual 0  {cm[0,0]:<8} {cm[0,1]:<8}\")    print(f\"           1  {cm[1,0]:<8} {cm[1,1]:<8}\")    # Confusion matrix visualization    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)    fig, ax = plt.subplots(figsize=(8, 6))    disp.plot(ax=ax, cmap='Blues', values_format='d')    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')    plt.tight_layout()    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png', dpi=300)    plt.show()    return {        'accuracy': accuracy,        'precision': precision,        'recall': recall,        'f1_score': f1,        'roc_auc': roc_auc,        'confusion_matrix': cm    }# =============================================================================# TASK 1: Print Keras model metrics using print_metrics# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 1: Keras Model Evaluation\")print(\"=\"*70)# Example: If you have Keras predictions loaded# Assuming from previous lab:# - keras_preds: Binary predictions (0 or 1)# - keras_labels: True labels (0 or 1)# - keras_probs: Probabilities for class 1# TASK 1 ANSWER (uncomment and use your actual data):# keras_metrics = print_metrics(#     y_true=keras_labels,#     y_pred=keras_preds,#     y_prob=keras_probs,#     model_name=\"Keras CNN Model\",#     class_labels=['Non-Agricultural', 'Agricultural']# )print(\"\"\"To complete Task 1, use:keras_metrics = print_metrics(    y_true=all_labels_keras,    y_pred=all_preds_keras,    y_prob=all_probs_keras,    model_name=\"Keras CNN Model\",    class_labels=['Non-Agricultural', 'Agricultural'])\"\"\")# =============================================================================# QUESTION 3: Explain the significance of the F1-score# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 3: Significance of F1-Score\")print(\"=\"*70)print(\"\"\"The F1-Score is the harmonic mean of Precision and Recall, providing a singlemetric that balances both measures of classification performance.**Formula:**   F1 = 2 * (Precision * Recall) / (Precision + Recall)**Why F1-Score is Important:**1. **Balances Precision and Recall**:   - Precision alone doesn't tell about false negatives   - Recall alone doesn't tell about false positives   - F1-Score considers both2. **Handles Class Imbalance**:   - Better than accuracy for imbalanced datasets   - Example: 95% non-agri, 5% agri     * Model predicting all non-agri: 95% accuracy, 0% F1 for agri class     * F1-Score reveals the poor performance3. **Single Performance Metric**:   - Easy to compare models   - Useful for model selection and hyperparameter tuning   - Better represents overall performance than accuracy alone4. **Domain-Specific Interpretation**:   - Medical diagnosis: High recall (catch all diseases)   - Spam detection: High precision (avoid false positives)   - F1-Score helps balance based on use case**Example Scenarios:**Scenario A (High Precision, Low Recall):   - Precision: 0.95 (few false positives)   - Recall: 0.60 (many false negatives)   - F1-Score: 0.73Scenario B (Balanced):   - Precision: 0.85   - Recall: 0.85   - F1-Score: 0.85Scenario C (Low Precision, High Recall):   - Precision: 0.60 (many false positives)   - Recall: 0.95 (few false negatives)   - F1-Score: 0.73**For Land Classification:**   - High F1-Score ensures we correctly identify agricultural land   - Minimizes both missing agricultural areas (false negatives)   - And incorrectly marking non-agricultural as agricultural (false positives)   - Critical for business decisions (fertilizer company expansion)\"\"\")# =============================================================================# TASK 2: Print PyTorch model metrics using print_metrics# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 2: PyTorch Model Evaluation\")print(\"=\"*70)# Example: If you have PyTorch predictions loaded# pytorch_metrics = print_metrics(#     y_true=pytorch_labels,#     y_pred=pytorch_preds,#     y_prob=pytorch_probs,#     model_name=\"PyTorch CNN Model\",#     class_labels=['Non-Agricultural', 'Agricultural']# )print(\"\"\"To complete Task 2, use:pytorch_metrics = print_metrics(    y_true=all_labels_pytorch,    y_pred=all_preds_pytorch,    y_prob=all_probs_pytorch,    model_name=\"PyTorch CNN Model\",    class_labels=['Non-Agricultural', 'Agricultural'])\"\"\")# =============================================================================# QUESTION 5: Count false negatives in PyTorch confusion matrix# =============================================================================print(\"\\n\" + \"=\"*70)print(\"QUESTION 5: Count False Negatives in Confusion Matrix\")print(\"=\"*70)print(\"\"\"False Negatives (FN) are located in the confusion matrix at position [1, 0]:   Confusion Matrix Structure:                   Predicted                   0      1         Actual 0  TN     FP                1  FN     TP   Where:   - TN (True Negative): [0, 0] - Correctly predicted as class 0   - FP (False Positive): [0, 1] - Incorrectly predicted as class 1   - FN (False Negative): [1, 0] - Incorrectly predicted as class 0   - TP (True Positive): [1, 1] - Correctly predicted as class 1**False Negatives Explained:**   - Actual class: 1 (Agricultural)   - Predicted class: 0 (Non-Agricultural)   - Model failed to detect agricultural land**How to Extract:**   cm = confusion_matrix(y_true, y_pred)   false_negatives = cm[1, 0]**Business Impact for Land Classification:**   - False negatives = missed agricultural land   - Fertilizer company misses expansion opportunities   - Potential revenue loss   - Should minimize FN to capture all potential markets**Example:**   cm = [[850, 50],     # Row 0: Non-Agricultural         [30, 920]]      # Row 1: Agricultural   - False Negatives (FN) = cm[1, 0] = 30   - Meaning: 30 agricultural lands were incorrectly classified as non-agricultural\"\"\")# Example code to extract false negatives# cm_pytorch = pytorch_metrics['confusion_matrix']# false_negatives_pytorch = cm_pytorch[1, 0]# print(f\"\\nFalse Negatives in PyTorch Model: {false_negatives_pytorch}\")# =============================================================================# COMPARATIVE ANALYSIS# =============================================================================def compare_models(keras_metrics, pytorch_metrics):    \"\"\"    Compare Keras and PyTorch models side-by-side    \"\"\"    print(\"\\n\" + \"=\"*70)    print(\"MODEL COMPARISON: Keras vs PyTorch\")    print(\"=\"*70)    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']    keras_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']    pytorch_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']    print(f\"\\n{'Metric':<15} | {'Keras':<12} | {'PyTorch':<12} | {'Difference':<12}\")    print(\"-\" * 70)    for name, k_key, p_key in zip(metrics_names, keras_keys, pytorch_keys):        k_val = keras_metrics[k_key]        p_val = pytorch_metrics[p_key]        if k_val is not None and p_val is not None:            diff = p_val - k_val            diff_str = f\"{diff:+.4f}\"            print(f\"{name:<15} | {k_val:.4f}      | {p_val:.4f}      | {diff_str}\")    # Confusion matrix comparison    print(\"\\n\" + \"=\"*70)    print(\"CONFUSION MATRIX COMPARISON\")    print(\"=\"*70)    cm_keras = keras_metrics['confusion_matrix']    cm_pytorch = pytorch_metrics['confusion_matrix']    print(\"\\nKeras CNN:\")    print(f\"  TN: {cm_keras[0,0]:<6} FP: {cm_keras[0,1]:<6}\")    print(f\"  FN: {cm_keras[1,0]:<6} TP: {cm_keras[1,1]:<6}\")    print(\"\\nPyTorch CNN:\")    print(f\"  TN: {cm_pytorch[0,0]:<6} FP: {cm_pytorch[0,1]:<6}\")    print(f\"  FN: {cm_pytorch[1,0]:<6} TP: {cm_pytorch[1,1]:<6}\")    # Visualization    fig, axes = plt.subplots(1, 2, figsize=(14, 5))    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']    keras_vals = [keras_metrics[m] for m in metrics_to_plot]    pytorch_vals = [pytorch_metrics[m] for m in metrics_to_plot]    x = np.arange(len(metrics_to_plot))    width = 0.35    axes[0].bar(x - width/2, keras_vals, width, label='Keras', color='skyblue')    axes[0].bar(x + width/2, pytorch_vals, width, label='PyTorch', color='coral')    axes[0].set_xlabel('Metric')    axes[0].set_ylabel('Score')    axes[0].set_title('Model Comparison: Metrics', fontweight='bold')    axes[0].set_xticks(x)    axes[0].set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'], rotation=45)    axes[0].legend()    axes[0].grid(True, alpha=0.3, axis='y')    # Training time comparison (if available)    # axes[1].bar(['Keras', 'PyTorch'], [keras_time, pytorch_time], color=['skyblue', 'coral'])    # axes[1].set_ylabel('Training Time (seconds)')    # axes[1].set_title('Training Time Comparison', fontweight='bold')    plt.tight_layout()    plt.savefig('keras_vs_pytorch_comparison.png', dpi=300)    plt.show()# Example usage (uncomment when you have both sets of metrics):# compare_models(keras_metrics, pytorch_metrics)# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 3 - All Tasks Completed\")print(\"=\"*70)print(\"Question 1: Explained 'preds > 0.5' threshold operation\")print(\"Task 1:     Printed Keras model metrics using print_metrics()\")print(\"Question 3: Explained significance of F1-Score\")print(\"Task 2:     Printed PyTorch model metrics using print_metrics()\")print(\"Question 5: Explained how to count False Negatives (cm[1,0])\")print(\"\\nComparative analysis complete - ready for Module 3!\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "a4d52cb879d68ff11658550d0ab0119df62da062a1361bfc915addcf5e1b237d",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a6f892ca6c46d4a5d4d33411822a81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2503ffe900194a87805a77ba15814ddb",
       "placeholder": "​",
       "style": "IPY_MODEL_423aafc84745419bb0ce7a5e9b2ee8f1",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 11486778.04it/s]"
      }
     },
     "0207dcec65164f729fb09467a0eb3478": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b2249f5363f403b8dd637cba610c7d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4891d5a5e0e242f7a815341042c21ba8",
       "placeholder": "​",
       "style": "IPY_MODEL_ba22dac609094dd29bfff8426b954d7a",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "2503ffe900194a87805a77ba15814ddb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d4dccbb0ead4c67aa6c1e9ba682f49f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "423aafc84745419bb0ce7a5e9b2ee8f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "47b4ebc388c04470bcdb649c22e3bcf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d4dccbb0ead4c67aa6c1e9ba682f49f",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b22c061c4ffc444790c50741c2735e4d",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     },
     "4891d5a5e0e242f7a815341042c21ba8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "689c791c79b24af38116ae15ed6c60fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76356c789c844e04a96b7e6014420a31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7f7277d762054ba3a9d2ea2185a0ee8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e5acc5419564d94aad158c5dd005365": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d68c776035d44a6c8a567350176a52bd",
       "placeholder": "​",
       "style": "IPY_MODEL_c661f809940b4c0ebcf9e595770ee3f0",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 11672.33it/s]"
      }
     },
     "a9f849b6f39343338b3e9092fa768623": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acec27087aaf4e149e57d1b7c20bbfee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7f7277d762054ba3a9d2ea2185a0ee8b",
       "placeholder": "​",
       "style": "IPY_MODEL_0207dcec65164f729fb09467a0eb3478",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "b22c061c4ffc444790c50741c2735e4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ba22dac609094dd29bfff8426b954d7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c1abf115188645b4a091450199211982": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_689c791c79b24af38116ae15ed6c60fb",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_76356c789c844e04a96b7e6014420a31",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "c661f809940b4c0ebcf9e595770ee3f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d68c776035d44a6c8a567350176a52bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de88332d64f44652a711ad2ba3a55690": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f80e3669ad9c4deaa3923af6835bb553": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0b2249f5363f403b8dd637cba610c7d0",
        "IPY_MODEL_c1abf115188645b4a091450199211982",
        "IPY_MODEL_01a6f892ca6c46d4a5d4d33411822a81"
       ],
       "layout": "IPY_MODEL_a9f849b6f39343338b3e9092fa768623",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f851b21a15174e2ab92b8bdc98529966": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_acec27087aaf4e149e57d1b7c20bbfee",
        "IPY_MODEL_47b4ebc388c04470bcdb649c22e3bcf7",
        "IPY_MODEL_9e5acc5419564d94aad158c5dd005365"
       ],
       "layout": "IPY_MODEL_de88332d64f44652a711ad2ba3a55690",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
