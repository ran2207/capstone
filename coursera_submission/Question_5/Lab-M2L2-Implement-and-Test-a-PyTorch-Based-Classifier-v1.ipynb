{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780b5ca2-b391-4f76-90dc-6b726241b3a5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7a68d-8e99-400b-bc1f-552da8a9b787",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Implement and Test a PyTorch-Based Classifier</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e509bb-6c72-4682-b16f-72dac36a7eba",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b73141-9c85-40bd-968d-a13a02a9e172",
   "metadata": {},
   "source": [
    "<h2>Objective</h2><ul>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "1. Create a PyTorch-based CNN model for classification.\n",
    "2. Train this model for the classification of agricultural and non-agricultural land.\n",
    "3. Evaluate the performance of this CNN model.\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5f76c-cce1-445b-a829-8ea855947f21",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **PyTorch-based Convolutional Neural Network (CNN)** for image classification, for agricultural images in your case. You will cover the following:\n",
    "1. *Data preparation*\n",
    "2. *Model architecture* definition\n",
    "3. *Training*, and\n",
    "4.  Model *performance analysis*\n",
    "\n",
    "The goal is to classify satellite images into two categories: 'agricultural' and 'non-agricultural'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e940b-e14b-4d9f-870f-2ab6dbb50a00",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Ensuring repeatability in PyTorch](#Ensuring-repeatability-in-PyTorch)\n",
    "4. [Defining hyperparameters and device](#Defining-hyperparameters-and-device)\n",
    "5. [The data pipeline](#The-data-pipeline)\n",
    "6. [Defining the model](#Defining-the-model)\n",
    "7. [Training and validation](#Training-and-validation)\n",
    "8. [Save and download the trained model weights](#Save-and-download-the-trained-model-weights)\n",
    "9. [Visualizing training history](#Visualizing-training-history)\n",
    "10. [Final model evaluation](#Final-model-evaluation)\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330778f-8962-4f35-b524-260b072c76b2",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938b599-110d-4109-8543-49d0902f1f1e",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them, it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d59c3d4-6d48-4005-b0c5-1f58c2dfd4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:37.681652Z",
     "iopub.status.busy": "2025-10-27T07:48:37.681164Z",
     "iopub.status.idle": "2025-10-27T07:48:37.692199Z",
     "shell.execute_reply": "2025-10-27T07:48:37.690442Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where you try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf69d9-9b6a-46f6-bae3-98e44cdbc9f6",
   "metadata": {},
   "source": [
    "### library installation - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa0a97-1317-4ec8-8a4b-6b664ba30b82",
   "metadata": {},
   "source": [
    "### `scikit-learn`, `torchvision` and `PyTorch` library installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3263f7c0-a6c8-45a8-b6b0-24c2a4bc9f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:37.696974Z",
     "iopub.status.busy": "2025-10-27T07:48:37.696509Z",
     "iopub.status.idle": "2025-10-27T07:48:39.957573Z",
     "shell.execute_reply": "2025-10-27T07:48:39.956857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 13.1 ms, total: 23.7 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install scikit-learn==1.7.0 numpy==1.26 matplotlib==3.9.2\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2d7f6-5ec8-4e4e-8334-543c3d3262ef",
   "metadata": {},
   "source": [
    "#### Check if the above libraries installed properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc1a378-2b8e-4d15-bce5-70ba064803d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:39.984834Z",
     "iopub.status.busy": "2025-10-27T07:48:39.984678Z",
     "iopub.status.idle": "2025-10-27T07:48:39.986802Z",
     "shell.execute_reply": "2025-10-27T07:48:39.986488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation failed!\n",
      "--- Error Details ---\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.8.0+cpu (from versions: 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.8.0+cpu\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91221028-16a8-4870-8533-0d18c659dfda",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b5d15a-66e6-44e1-aa4a-2a98dd91f414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:39.988284Z",
     "iopub.status.busy": "2025-10-27T07:48:39.988189Z",
     "iopub.status.idle": "2025-10-27T07:48:39.989861Z",
     "shell.execute_reply": "2025-10-27T07:48:39.989495Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e1df8a-b9c1-4392-8b64-7e45bb01ee1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:39.991007Z",
     "iopub.status.busy": "2025-10-27T07:48:39.990918Z",
     "iopub.status.idle": "2025-10-27T07:48:40.167162Z",
     "shell.execute_reply": "2025-10-27T07:48:40.166789Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9001e171-d261-49ca-9de4-fea4e3f8bf51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:40.168465Z",
     "iopub.status.busy": "2025-10-27T07:48:40.168376Z",
     "iopub.status.idle": "2025-10-27T07:48:41.605382Z",
     "shell.execute_reply": "2025-10-27T07:48:41.604959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 1.06 s, sys: 190 ms, total: 1.25 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- AI LIBRARY IMPORTS ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8efe1e-e72a-455f-9a0d-d136b4e335f0",
   "metadata": {},
   "source": [
    "#### Setting Up Data Extraction Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a56aac8-fb53-4342-8df2-cab372762bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:41.606789Z",
     "iopub.status.busy": "2025-10-27T07:48:41.606681Z",
     "iopub.status.idle": "2025-10-27T07:48:41.608302Z",
     "shell.execute_reply": "2025-10-27T07:48:41.607994Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_dir = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1319577-cdc8-4a38-9365-d06c0866dddf",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Defining dataset URL\n",
    "\n",
    "\n",
    "Let's define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (like S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db02b667-f6b1-47ec-8503-584725d90a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:41.609487Z",
     "iopub.status.busy": "2025-10-27T07:48:41.609416Z",
     "iopub.status.idle": "2025-10-27T07:48:41.610925Z",
     "shell.execute_reply": "2025-10-27T07:48:41.610586Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baf3cb-2259-4a94-a61b-7f4b7ad6e5da",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1. Download and extract data from the cloud using the `skillsnetwork.prepare` method\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats like gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0254a0-c340-4e99-be19-bd8c2f8714d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:41.611920Z",
     "iopub.status.busy": "2025-10-27T07:48:41.611855Z",
     "iopub.status.idle": "2025-10-27T07:48:41.614634Z",
     "shell.execute_reply": "2025-10-27T07:48:41.614251Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc67b95-549e-484f-8c17-914606890cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:41.615580Z",
     "iopub.status.busy": "2025-10-27T07:48:41.615515Z",
     "iopub.status.idle": "2025-10-27T07:48:47.490231Z",
     "shell.execute_reply": "2025-10-27T07:48:47.489811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fc121b4e87471bab1a0264d5656f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e01b58c2f44af794e2cde2ecbedaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (e.g., 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab0f44-9b2a-4dcd-a68b-8a09186fc4d8",
   "metadata": {},
   "source": [
    "## Ensuring repeatability in PyTorch\n",
    "\n",
    "To achieve reproducible results when you train a CNN in PyTorch, you must follow three steps:\n",
    "\n",
    "1.  Define a helper called `set_seed` that seeds every random-number generator and configures cuDNN for deterministic kernels.\n",
    "2.  Call `set_seed()` *once* at the top of your script/notebook to lock in the seed for the main process.\n",
    "3.  Provide a `worker_init_fn` so each `DataLoader` worker starts from a reproducible seed as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d59fcb-546f-4f98-94d2-159d06942d3e",
   "metadata": {},
   "source": [
    "### Define the `set_seed` function\n",
    "What the `set_seed` function does\n",
    "\n",
    "* **Python & NumPy** – Many data-pipeline utilities (shuffling lists, image augmentations) rely on these random-number generators. Seeding them first removes one entire layer of randomness.\n",
    "* **PyTorch CPU / GPU** – `torch.manual_seed` covers every op executed on the CPU, while `torch.cuda.manual_seed_all` applies the same seed to each GPU stream so that multi-GPU jobs stay in sync.\n",
    "* **cuDNN flags** – By default cuDNN picks the fastest convolution algorithm, which can vary run-to-run. Setting `deterministic=True` forces repeatable kernels and turning `benchmark` *off* prevents the auto-tuner from replacing those kernels mid-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc001396-c072-41dd-b6fc-24b9ede061f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.491393Z",
     "iopub.status.busy": "2025-10-27T07:48:47.491304Z",
     "iopub.status.idle": "2025-10-27T07:48:47.493273Z",
     "shell.execute_reply": "2025-10-27T07:48:47.492901Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806d022-ef1f-4e80-8652-74672fd9a0d2",
   "metadata": {},
   "source": [
    "### Call `set_seed()`\n",
    "\n",
    "Running the command *before* you build models, create datasets, or start data-loader workers guarantees that every downstream object inherits the same seed.  If you call it later, some layers or tensors may already have been initialised with non-deterministic values, breaking repeatability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3ae688-4e8c-4548-82d0-931639b00bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.494275Z",
     "iopub.status.busy": "2025-10-27T07:48:47.494204Z",
     "iopub.status.idle": "2025-10-27T07:48:47.499087Z",
     "shell.execute_reply": "2025-10-27T07:48:47.498683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42 — main process is now deterministic.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} — main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35aed1a-a282-491a-8be2-8cd0f824e47e",
   "metadata": {},
   "source": [
    "#### You now know how to fix the seed for reproducibility. Now, let's answer the following question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fef45-c618-424c-be4b-748536b512b8",
   "metadata": {},
   "source": [
    "### Question: Why is random initialization useful for the model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b911580-d39a-4614-87a3-c587bf5bff9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.500091Z",
     "iopub.status.busy": "2025-10-27T07:48:47.500021Z",
     "iopub.status.idle": "2025-10-27T07:48:47.501427Z",
     "shell.execute_reply": "2025-10-27T07:48:47.501072Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626617b4-1bb4-40be-8478-526a5431b39d",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "Random initialization, each neuron starts differently, enabling effective learning and convergence.    \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258a785-5f61-43a9-8acb-7f97b2d9f783",
   "metadata": {},
   "source": [
    "### Define `worker_init_fn` function\n",
    "\n",
    "PyTorch offsets each worker’s seed by default, injecting new randomness. For reproducible results, you want workers to start from **fixed** seeds so every data-augmentation decision (flip, crop, colour-jitter) is repeatable across runs. The `worker_init_fn` function re-seeds Python, NumPy, and PyTorch CPU random-number generators inside **each** worker using a simple deterministic formula (`SEED + worker_id`).  The result will be identical batches, identical gradients, and identical model checkpoints, run after run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7867408d-f4d0-41d2-bf84-b82647f53ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.502370Z",
     "iopub.status.busy": "2025-10-27T07:48:47.502301Z",
     "iopub.status.idle": "2025-10-27T07:48:47.503907Z",
     "shell.execute_reply": "2025-10-27T07:48:47.503541Z"
    }
   },
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id: int) -> None:\n",
    "    \"\"\"Re-seed each DataLoader worker so their RNGs don't collide.\"\"\"\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed) \n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376d6a6-f694-4604-95f5-a7210a0d3149",
   "metadata": {},
   "source": [
    "### Defining `dataset_path`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b7ef7f-dea5-4849-8250-696045816735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.504792Z",
     "iopub.status.busy": "2025-10-27T07:48:47.504717Z",
     "iopub.status.idle": "2025-10-27T07:48:47.506136Z",
     "shell.execute_reply": "2025-10-27T07:48:47.505850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd007fce-9012-4f20-8c53-573a49706ba7",
   "metadata": {},
   "source": [
    "## Defining hyperparameters and device\n",
    "\n",
    "You have to define the key **hyperparameters** that control the model's training process. Hyperparameters are set by the user to configure the learning algorithm.\n",
    "\n",
    "- **`img_size`**: The spatial resolution (height and width) to which all images will be resized. This ensures that the input to the neural network is of a consistent size.\n",
    "- **`batch_size`**: The number of training examples utilized in one iteration (one forward and backward pass). A larger batch size can lead to faster training but requires more memory.\n",
    "- **`lr` (Learning Rate)**: A crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\n",
    "- **`epochs`**: The number of times the learning algorithm will work through the entire training dataset.\n",
    "- **`model_name`**: The name of the model file that will be created after training. This is useful for saving the checkpoint while training.\n",
    "- **`device`**: This line programmatically checks if a CUDA-enabled GPU is available using `torch.cuda.is_available()`. If a GPU is found, the device is set to `\"cuda\"` to leverage hardware acceleration. Otherwise, it defaults to the `\"cpu\"`. This makes the code portable and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e495c63-4f84-43f6-b123-34a91869d092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.506973Z",
     "iopub.status.busy": "2025-10-27T07:48:47.506921Z",
     "iopub.status.idle": "2025-10-27T07:48:47.508605Z",
     "shell.execute_reply": "2025-10-27T07:48:47.508244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used is cpu\n"
     ]
    }
   ],
   "source": [
    "img_size = 64\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "model_name = \"ai_capstone_pytorch_state_dict.pth\"\n",
    "num_classes = 2 #number of classes in the dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device used is {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761cc188-c389-4016-9910-f16eacf44a10",
   "metadata": {},
   "source": [
    "## The data pipeline\n",
    "You have downloaded the dataset and fixed the initial random seed for reproducibility. Now, you can start to build the data pipeline to feed data for training the model.\n",
    "To create the data pipeline for PyTorch, you will:\n",
    "1. Define transformations\n",
    "2. Split the dataset for training and validation\n",
    "3. Create the dataloader to feed the data into the training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee203042-449f-4596-80fe-b6874b36fd17",
   "metadata": {},
   "source": [
    "#### Define transformations\n",
    "Here, you will define a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "This cell constructs the entire pipeline for loading and preparing the image data for the model. It involves defining transformations, splitting the data, and creating data loaders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b8530-bd01-4f65-99ef-e757bed94e14",
   "metadata": {},
   "source": [
    "### Task: Create the training transformation pipeline `train_transform` using the `tranforms.Compose` . \n",
    "You may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size, useful for input vector with fixed dimensions for model training\n",
    "- `transforms.RandomRotation`: For geometrical rotation\n",
    "- `transforms.RandomHorizontalFlip`: For Geometrical horizontal flipping\n",
    "- `transforms.RandomAffine`: For adjusting to a different point-of-view\n",
    "\n",
    "Then, convert the image array to a Tensor using `transforms.ToTensor()`.\n",
    "\n",
    "And finally, normalize the images between [-1,1] using `transforms.Normalize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d04cf8-ab3a-4522-bf2f-ae4b20fd6644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.509586Z",
     "iopub.status.busy": "2025-10-27T07:48:47.509526Z",
     "iopub.status.idle": "2025-10-27T07:48:47.510856Z",
     "shell.execute_reply": "2025-10-27T07:48:47.510520Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba93c70-2986-4aef-9495-af54fa8e32ea",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84a664-dde2-482d-b0e6-a41f0f96eb8f",
   "metadata": {},
   "source": [
    "- **`train_transform`**: This defines a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "- **`val_transform`**: The transformation for the validation set is simpler. It omits the random augmentation steps because you want to evaluate the model's performance on the original, unaltered data.\n",
    "- **`datasets.ImageFolder`**: This PyTorch utility automatically loads an image dataset from a directory where subdirectories are named after their corresponding classes (e.g., `data/agri`, `data/non_agri`).\n",
    "- **`random_split`**: The full dataset is partitioned into training (80%) and validation (20%) sets. This separation is crucial for assessing how well the model generalizes to unseen data.\n",
    "- **`DataLoader`**: These objects wrap the datasets and provide an efficient, iterable way to feed data to the model in batches. `shuffle=True` for the `train_loader` ensures that the model sees the data in a different order each epoch, which helps prevent it from learning the order of the training examples. `worker_init_fn` ensures that **fixed seed** is passed to the `dataloader` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d30a5-881c-4cec-8f08-2bd3743f44fa",
   "metadata": {},
   "source": [
    "### Task: Create the validation transformation pipeline `val_transform`.\n",
    "The validataion dataset is just for validating the preformace of the model and hence, doesn't need to augment the input images. \n",
    "So, you may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size\n",
    "- `transforms.ToTensor()`\n",
    "-  `transforms.Normalize`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd0c591-c3e4-4445-9cde-c2ea9073886d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.511764Z",
     "iopub.status.busy": "2025-10-27T07:48:47.511689Z",
     "iopub.status.idle": "2025-10-27T07:48:47.513050Z",
     "shell.execute_reply": "2025-10-27T07:48:47.512704Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ab0db-a583-4087-9306-d10aa18c0072",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a68de-f66d-4264-97f4-7b70a923156c",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "You have defined the transformation pipelines for the training and validation datasets. \n",
    "\n",
    "Next, you will use the `datasets.ImageFolder` utility to load an image dataset from the root directory `dataset_path`. \n",
    "\n",
    "This root directory contains the subdirectories where each subdirectory corresponds to a class (e.g., `data/agri`, `data/non_agri`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b58ff4a-c2b8-450d-9014-fbf38e4d37a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.513997Z",
     "iopub.status.busy": "2025-10-27T07:48:47.513940Z",
     "iopub.status.idle": "2025-10-27T07:48:47.586761Z",
     "shell.execute_reply": "2025-10-27T07:48:47.586372Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m full_dataset = datasets.ImageFolder(dataset_path, transform=\u001b[43mtrain_transform\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_transform' is not defined"
     ]
    }
   ],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed09a48-567d-40c4-99c4-d694e721cbca",
   "metadata": {},
   "source": [
    "### Dataset split: Train and validation\n",
    "\n",
    "The next step in the data loading pipeline is to split the image dataset for training and validation. \n",
    "\n",
    "You can use `random_split` from `torch.utils.data` class. \n",
    "\n",
    "This method allows you to randomly split the input data based on a pre-defined split ratio for the training and validation datasets. \n",
    "\n",
    "In this case, you can use 80% (0.8) dataset for training and 20% (0.20) for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d76131d-5e1e-466c-b15a-f36af119d8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.587810Z",
     "iopub.status.busy": "2025-10-27T07:48:47.587737Z",
     "iopub.status.idle": "2025-10-27T07:48:47.597142Z",
     "shell.execute_reply": "2025-10-27T07:48:47.596825Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(\u001b[43mfull_dataset\u001b[49m))\n\u001b[32m      2\u001b[39m val_size = \u001b[38;5;28mlen\u001b[39m(full_dataset) - train_size\n\u001b[32m      3\u001b[39m train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
      "\u001b[31mNameError\u001b[39m: name 'full_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9d0af-d75b-49b6-b6d9-9b35dfbe0fa8",
   "metadata": {},
   "source": [
    "### Create training dataLoader\n",
    "\n",
    "Now, you can use the `DataLoader` from `torch.utils.data` class to create a dataset generator for lazy loading of the training dataset.\n",
    "In the input, you define \n",
    "- `train_dataset`: The training image dataset\n",
    "- `batch_size`: The number of images to be loaded in each batch\n",
    "- `shuffle`: Set to *True* to load images from the dataset in random order\n",
    "- `num_workers`: Number of parallel processes used to load the images. This is for optimum utilization of your CPU cores to reduce the image I/O bottleneck\n",
    "- `worker_init_fn`: For function to decide on data augmentation. The default is with *random seed* for better generalization or *fixed seed* for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32cfb6f4-87a1-427a-a2da-0b7fc5ca0178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.598172Z",
     "iopub.status.busy": "2025-10-27T07:48:47.598104Z",
     "iopub.status.idle": "2025-10-27T07:48:47.606941Z",
     "shell.execute_reply": "2025-10-27T07:48:47.606614Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loader = DataLoader(\u001b[43mtrain_dataset\u001b[49m, \n\u001b[32m      2\u001b[39m                           batch_size=batch_size,\n\u001b[32m      3\u001b[39m                           shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m                           num_workers=\u001b[32m4\u001b[39m,\n\u001b[32m      5\u001b[39m                           worker_init_fn=worker_init_fn\n\u001b[32m      6\u001b[39m                          )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          worker_init_fn=worker_init_fn\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de24afb-52db-4b54-a745-463c79e4efb2",
   "metadata": {},
   "source": [
    "### Create validation DataLoader\n",
    " \n",
    "Now that you know how to create the train dataloader, in this step, you will create a validation step dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6de13-04df-4102-987d-22e7e2abcb4f",
   "metadata": {},
   "source": [
    "### Task: create `val_loader` for the validation dataset\n",
    "\n",
    "You have to create the validation dataloader `val_loader` for validation of model in each training step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9000e30-3223-43a5-82a8-c99b8f9f6ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.608000Z",
     "iopub.status.busy": "2025-10-27T07:48:47.607941Z",
     "iopub.status.idle": "2025-10-27T07:48:47.609353Z",
     "shell.execute_reply": "2025-10-27T07:48:47.608992Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8f83e-e4df-4278-82e0-7edecc0ec4fe",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        worker_init_fn=worker_init_fn\n",
    "                       )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2f90dfa-5347-40d6-99f8-f6127566f78a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.610259Z",
     "iopub.status.busy": "2025-10-27T07:48:47.610201Z",
     "iopub.status.idle": "2025-10-27T07:48:47.611745Z",
     "shell.execute_reply": "2025-10-27T07:48:47.611381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Dataloaders. Now creating the model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Created Dataloaders. Now creating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd300c08-8874-49f5-a8da-807a1cb73c9e",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "You will define the Convolutional Neural Network (CNN) architecture and configure the components needed for training.\n",
    "<p></p>\n",
    "\n",
    "\n",
    "<p></p>\n",
    "\n",
    "- **`model = nn.Sequential(...)`**: A sequential container is used to build the model as a linear stack of layers. This is a convenient way to define a straightforward CNN.\n",
    "  - **Convolutional Blocks**: The model consists of several blocks, each containing\n",
    "      - a `Conv2d` layer for feature extraction,\n",
    "      - a `ReLU` activation function,\n",
    "      - a `MaxPool2d` layer to downsample and reduce dimensionality,\n",
    "      - a`BatchNorm2d` to stabilize and accelerate training.    \n",
    "  - **Classifier**: After the convolutional blocks,\n",
    "      - `AdaptiveAvgPool2d` reduces each feature map to a single value, making the model more robust to input size variations.\n",
    "      - `Flatten` converts the 2D feature maps into a 1D vector.\n",
    "      - `Linear` (fully connected) layers then perform the final classification,\n",
    "      - `Dropout` is used as a regularization technique to prevent overfitting.\n",
    "  - **`.to(device)`**: This moves the model's parameters and buffers to the selected device (GPU, if available otherwise CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f942725-642f-40a8-a087-a3b942c591e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.612683Z",
     "iopub.status.busy": "2025-10-27T07:48:47.612605Z",
     "iopub.status.idle": "2025-10-27T07:48:47.661793Z",
     "shell.execute_reply": "2025-10-27T07:48:47.661428Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "model = nn.Sequential(\n",
    "                        # Conv Block 1\n",
    "                        nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "                        nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "                        \n",
    "                        # Conv Block 2-6\n",
    "                        nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "                        nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "                        nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "                        \n",
    "                        # Classifier\n",
    "                        nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "                        nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "                        nn.Linear(2048, num_classes)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4dc64-f5b8-4e69-8717-0a136bb38f49",
   "metadata": {},
   "source": [
    "### Defining the training setup\n",
    "\n",
    "After defining the model, you declare the loss function and the optimizer for backpropagation and learning\n",
    "You also set up the tracking of the history of the model training for loss and accuracy for every step of the model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd27cc-2f50-4999-8345-2ac3cdabe6bf",
   "metadata": {},
   "source": [
    "- The loss function is defined using **`criterion = nn.CrossEntropyLoss()`**\n",
    "    - `CrossEntropyLoss` is specifically designed for multi classs classification problems.\n",
    "<p></p>\n",
    "<p></p>\n",
    "- The optimizer is defined using **`optimizer = optim.Adam(...)`**:\n",
    "    - The Adam optimizer is chosen to update the model's weights. It's an adaptive learning rate method that is computationally efficient and works well in practice.\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- You can **track the history** using `best_loss`, `loss_history` and `acc_history` dictionaries\n",
    "    - `best_loss`: stores the best validation loss achieved so far.\n",
    "    - `loss_history` and `acc_history` dictionaries to log the loss and accuracy history for plotting later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0fbdb5c-dc9d-40a3-87da-f977b0a19dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.662881Z",
     "iopub.status.busy": "2025-10-27T07:48:47.662822Z",
     "iopub.status.idle": "2025-10-27T07:48:47.664759Z",
     "shell.execute_reply": "2025-10-27T07:48:47.664449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model. Now training the model...\n"
     ]
    }
   ],
   "source": [
    "# --- TRAINING SETUP ---\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "best_loss = float('inf')\n",
    "loss_history = {'train': [], 'val': []}\n",
    "acc_history = {'train': [], 'val': []}\n",
    "\n",
    "print(\"Created Model. Now training the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea261a-31f2-4ef6-9acd-f813ab002cd1",
   "metadata": {},
   "source": [
    "## Training and validation\n",
    "\n",
    "Your neural network is now ready for training.\n",
    "\n",
    "Here, you will set up the main logic for how the model learns from the data. The model iterates through the dataset for the specified number of epochs, with each epoch consisting of a training phase and a validation phase.\n",
    "\n",
    "- **Outer Loop (`for epoch in range(epochs)`)**: Controls the number of full passes over the dataset.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "\n",
    "- **In the interest of time, we are training the model for just 3 epochs**.\n",
    "    - Generally, you train a model for many more epochs (atleast 15, usually). The model trained for 20 epochs can be found **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Training Phase**:\n",
    "  - `model.train()`: Sets the model to training mode. This activates layers like Dropout and ensures BatchNorm layers learn from the current batch statistics.\n",
    "  - **Inner Loop (`for images, labels in train_loader`)**: Iterates over batches of training data.\n",
    "  - `optimizer.zero_grad()`: Clears the gradients from the previous iteration before computing new ones.\n",
    "  - `outputs = model(images)`: **Forward Pass**. The input data is passed through the network to get predictions (logits).\n",
    "  - `loss.backward()`: **Backward Pass**. Gradients of the loss with respect to the model's parameters are calculated.\n",
    "  - `optimizer.step()`: The optimizer updates the model's parameters using the computed gradients.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Validation Phase**:\n",
    "  - `model.eval()`: Sets the model to evaluation mode. This deactivates Dropout and makes BatchNorm layers use their learned running statistics.\n",
    "  - `with torch.no_grad()`: Disables gradient calculation, which speeds up computation and reduces memory usage since you are only evaluating, not training.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Model Checkpointing**: After each epoch, the current validation loss is compared to the `best_loss` seen so far. If the current loss is lower, the model's state (`model.state_dict()`) is saved to a file. This ensures that you keep the model version that performed best on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fba5145-817c-4fc7-b7c0-81ae80ab016d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.665729Z",
     "iopub.status.busy": "2025-10-27T07:48:47.665668Z",
     "iopub.status.idle": "2025-10-27T07:48:47.686718Z",
     "shell.execute_reply": "2025-10-27T07:48:47.686302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on : ===cpu=== with batch size: 128 & lr: 0.001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m model.train()\n\u001b[32m      8\u001b[39m train_loss, train_correct, train_total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# for the training metrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[43mtrain_loader\u001b[49m, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)):\n\u001b[32m     10\u001b[39m     images, labels = images.to(device), labels.to(device)  \u001b[38;5;66;03m# labels as integer class indices\u001b[39;00m\n\u001b[32m     11\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    start_time = time.time() # to get the training time for each epoch\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0  # for the training metrics\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "        images, labels = images.to(device), labels.to(device)  # labels as integer class indices\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # outputs are raw logits\n",
    "        loss = criterion(outputs, labels)  # criterion is nn.CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "  \n",
    "    # Synchronize CUDA before stopping timer (if using GPU)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0 #  for the validation metrics\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "  \n",
    "    # Save the best model\n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "    \n",
    "    # Store metrics\n",
    "    loss_history['train'].append(train_loss/len(train_loader))\n",
    "    loss_history['val'].append(val_loss/len(val_loader))\n",
    "    acc_history['train'].append(train_correct/train_total)\n",
    "    acc_history['val'].append(val_correct/val_total)\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {loss_history['train'][-1]:.4f} | Val Loss: {loss_history['val'][-1]:.4f}\")\n",
    "    print(f\"Train Acc: {acc_history['train'][-1]:.4f} | Val Acc: {acc_history['val'][-1]:.4f}\")\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} training completed in {epoch_time:.2f} seconds\\n\") \n",
    "\n",
    "print(\"Trained Model. Now evaluating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82692c08-bb76-45a0-a7cf-f2f1ff782bed",
   "metadata": {},
   "source": [
    "You have successfully trained the model using PyTorch libraries. As you can see, during the model training, each step is easy accessible for fine tuning the model. This gives the user an advantage by giving them control over every hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100582c-4190-42d7-a20c-53399b5a1074",
   "metadata": {},
   "source": [
    "Based on the training cell above, please answer the following questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc53ec-41d7-494b-a9c1-ec5686f68039",
   "metadata": {},
   "source": [
    "### Question: What is `tqdm` used for?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e43c2c31-76c0-426f-b281-0051430d9a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.687825Z",
     "iopub.status.busy": "2025-10-27T07:48:47.687763Z",
     "iopub.status.idle": "2025-10-27T07:48:47.689117Z",
     "shell.execute_reply": "2025-10-27T07:48:47.688802Z"
    }
   },
   "outputs": [],
   "source": [
    "### You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032dcd7-d695-4974-8bf9-d4308dd18be1",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"The tqdm library is used to provide a progress bar to monitor the progress of each epoch.\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713238f-bea3-4017-8451-61ec29157912",
   "metadata": {},
   "source": [
    "### Question: Why are the `train_loss`, `train_correct` and `train_total` set to 0 in every epoch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab0b970f-3bb2-4ce7-9357-d480cb3c19cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.690107Z",
     "iopub.status.busy": "2025-10-27T07:48:47.690049Z",
     "iopub.status.idle": "2025-10-27T07:48:47.691426Z",
     "shell.execute_reply": "2025-10-27T07:48:47.691071Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d181b-d417-44cb-9a09-c93f2b980e52",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"Because they accumulate metrics for that specific epoch only\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442018e1-a3fb-4154-866e-8107ac92a96d",
   "metadata": {},
   "source": [
    "### Question: Why do you need to use `torch.no_grad()` in the validation loop?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "354af83a-a7d8-4a05-a5a4-398acee65276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.692335Z",
     "iopub.status.busy": "2025-10-27T07:48:47.692264Z",
     "iopub.status.idle": "2025-10-27T07:48:47.693640Z",
     "shell.execute_reply": "2025-10-27T07:48:47.693331Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f0da1-0560-4f82-bf08-07d7a5d1ff6f",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"It disables gradient calculation as you do not need gradient calculation for validation\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55c29a-7ad9-4623-9925-b072c69f8a38",
   "metadata": {},
   "source": [
    "### Question: What are two different metrics on which the model can be evaluated for best performance during training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75327d49-19bc-48cf-966e-69a524b0d8cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.694587Z",
     "iopub.status.busy": "2025-10-27T07:48:47.694524Z",
     "iopub.status.idle": "2025-10-27T07:48:47.695890Z",
     "shell.execute_reply": "2025-10-27T07:48:47.695545Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35354f8a-13b6-4a5a-8dbf-376a335b7bac",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"the validation loss and validation accuracy\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf949c-259d-4d29-b932-9f1dd2c21160",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `torch.save` function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model file: `ai_capstone_pytorch_state_dict.pth` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- This model could be used in other labs of this AI capstone course, instead of the model provided at the above link\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the file to your local computer.\n",
    "\n",
    "\n",
    "### The trained model state file (`ai_capstone_pytorch_state_dict.pth` ) in the left pane\n",
    "![Model_PyTorch_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Nar8kA3Qrz3uCmFtFrKI9g/Model-PyTorch-download-screenshot-1-marked.png)\n",
    "\n",
    "### The **download** option\n",
    "![Model_PyTorch_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HKO5ROsE1erbqcE6Kq8ysA/Model-PyTorch-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de6ba1-de4a-4da7-9acf-1db3074b9577",
   "metadata": {},
   "source": [
    "## Visualizing training history\n",
    "\n",
    "Here, you can use `matplotlib` to create plots of the model's accuracy and loss over each epoch. Visualizing these metrics is useful for understanding the training dynamics.\n",
    "\n",
    "\n",
    "Usually, the following two plots are used to track the training history of a model:\n",
    "- **Accuracy Plot**: Shows the training accuracy versus the validation accuracy. A large gap between the two curves can be an indicator of overfitting, where the model performs well on the data it has seen but poorly on new, unseen data.\n",
    "\n",
    "- **Loss Plot**: Shows the training loss versus the validation loss. An ideal plot shows both losses decreasing and converging. If the validation loss starts to increase while the training loss continues to decrease, it's a strong sign of overfitting.\n",
    "\n",
    " \n",
    "These plots provide an intuitive, visual summary of the entire training process and help diagnose potential issues or confirm that the model has trained successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2307bd5-078e-459b-8311-a50850ee06f4",
   "metadata": {},
   "source": [
    "#### Plot the **Model Accuracy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91240273-9b79-4cf7-aa1a-ef0ca751f562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.696913Z",
     "iopub.status.busy": "2025-10-27T07:48:47.696855Z",
     "iopub.status.idle": "2025-10-27T07:48:47.741381Z",
     "shell.execute_reply": "2025-10-27T07:48:47.740987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(acc_history['train'], label='Train Acc')\n",
    "plt.plot(acc_history['val'], label='Val Acc')\n",
    "plt.title('Model Accuracy (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c826e-e012-471a-a608-eb022c5cb0ee",
   "metadata": {},
   "source": [
    "Now, write the code for the loss plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424181b-01e3-4d90-b17d-2414d464bb89",
   "metadata": {},
   "source": [
    "### Task: Plot the *Model Loss* from the training history of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443e96b9-a26c-41d6-b0c8-b3d6999e00ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.742535Z",
     "iopub.status.busy": "2025-10-27T07:48:47.742462Z",
     "iopub.status.idle": "2025-10-27T07:48:47.743896Z",
     "shell.execute_reply": "2025-10-27T07:48:47.743598Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3255dc-d72b-403d-bc88-11fd65576d25",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history['train'], label='Train Loss')\n",
    "plt.plot(loss_history['val'], label='Val Loss')\n",
    "plt.title('Model Loss (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faaafb-1e75-4f93-a570-45252d402ae3",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "This cell comprehensively evaluates the best-performing model saved during the training loop. While accuracy provides a high-level view, these metrics provide a deeper insight into the model's behavior.\n",
    "\n",
    "\n",
    "- **`model.eval()`**: Switches the model to evaluation mode.\n",
    "- **`with torch.no_grad()`**: Disables gradient computation for efficiency.\n",
    "- **Collecting Predictions**: The code iterates through the entire validation set to gather all predictions and their corresponding true labels.\n",
    "\n",
    "- **`accuracy`**: The proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fbdc0-02b8-4ed8-810a-5c5a672688c5",
   "metadata": {},
   "source": [
    "To evaluate the model, you have to get the predictions for the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62e4b4-3783-42a5-a492-6dedb787cee9",
   "metadata": {},
   "source": [
    "### Task: For the images from `val_loader`, get a list of :\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "**1.** all predictions `all_preds`\n",
    " \n",
    "**2.** the ground truth labels `all_labels` \n",
    "\n",
    "For the predictions, you will have to move the data to the CPU using `predictions.cpu()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5871f3e7-8bc4-4644-8326-862e4e72acec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.744934Z",
     "iopub.status.busy": "2025-10-27T07:48:47.744877Z",
     "iopub.status.idle": "2025-10-27T07:48:47.746254Z",
     "shell.execute_reply": "2025-10-27T07:48:47.745889Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494207-619d-409c-b8e9-c75dffd68328",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.numpy())\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514a989-f2a3-403c-9e25-a88b7be75dcd",
   "metadata": {},
   "source": [
    "### Get the accuracy of the plot\n",
    "\n",
    "**`accuracy`** is the proportion of correct predictions out of the total predictions.\n",
    "\n",
    "You can use `accuracy_score` from `sklearn.metrics` class to calculate the **accuracy_score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52618498-4660-4604-ac40-b66017fc3edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.747202Z",
     "iopub.status.busy": "2025-10-27T07:48:47.747146Z",
     "iopub.status.idle": "2025-10-27T07:48:47.756937Z",
     "shell.execute_reply": "2025-10-27T07:48:47.756561Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m accuracy = accuracy_score(\u001b[43mall_labels\u001b[49m, all_preds)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe accuracy of the model is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"The accuracy of the model is: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437db9d2-9730-45ef-8b2c-a9212241d140",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8392969-44ba-42e1-bcca-e84c2c55f2de",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully built, trained, and evaluated a deep learning model for image classification using PyTorch.\n",
    "\n",
    "In this lab, you coded for:\n",
    "- **Data loading pipeline:** Implementing ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** Building a multi-layered CNN incorporating Conv2D layers.\n",
    "- **Model training setup:** Configuring the model’s learning process using the Adam optimizer, BCEWithLogitsLoss, and tracking accuracy and loss metrics.\n",
    "- **Training process:** Executing the training loop by feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** Plotting accuracy and loss curves to analyze learning progress and detect overfitting.\n",
    "- **Model evaluation:** Assessing model performance using accuracy_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283b647-a585-404f-aa8a-2f9b90ec6b6b",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14846807-8144-48d0-8338-9d9caa7b8db8",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-04  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e9d06-3a38-46a0-8b29-ce33958a9efe",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:47.758178Z",
     "iopub.status.busy": "2025-10-27T07:48:47.758117Z",
     "iopub.status.idle": "2025-10-27T07:48:47.763371Z",
     "shell.execute_reply": "2025-10-27T07:48:47.763012Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2044057893.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 2 Lab 2: Implement and Test a PyTorch-Based ClassifierSolutions for all tasks (20 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L2-Implement-and-Test-a-PyTorch-Based-Classifier-v1.ipynb\"\"\"import osimport timeimport numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, random_splitfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score# =============================================================================# TASK 1: Explain the usefulness of random initialization# =============================================================================print(\"=\"*70)print(\"TASK 1: Usefulness of Random Initialization\")print(\"=\"*70)print(\"\"\"Random initialization is crucial in neural networks for several reasons:1. **Breaking Symmetry**: If all weights are initialized to the same value   (e.g., all zeros), all neurons in a layer would compute the same output and   receive the same gradients during backpropagation. This means they would all   update identically, making multiple neurons redundant.2. **Enabling Learning**: Random initialization creates diverse initial states   for neurons, allowing them to learn different features from the data.3. **Gradient Flow**: Proper random initialization (like He initialization for   ReLU activations) helps maintain good gradient flow during backpropagation,   preventing vanishing or exploding gradients.4. **Convergence**: Well-initialized networks converge faster and reach better   local minima compared to poorly initialized networks.**Common Initialization Strategies:**- Xavier/Glorot: For sigmoid/tanh activations- He initialization: For ReLU activations (used in our CNN)- Uniform or Normal distributions with controlled varianceIn our CNN, we use He uniform initialization which is designed to work wellwith ReLU activations by maintaining variance across layers.\"\"\")# =============================================================================# TASK 2: Define train_transform pipeline with augmentation# =============================================================================img_size = 64train_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.RandomRotation(40),                    # Random rotation ±40 degrees    transforms.RandomHorizontalFlip(p=0.5),          # 50% chance horizontal flip    transforms.RandomVerticalFlip(p=0.2),            # 20% chance vertical flip    transforms.RandomAffine(0, shear=0.2),          # Shear transformation    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Color augmentation    transforms.ToTensor(),                            # Convert to tensor    transforms.Normalize(                             # Normalize with ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"\\nTASK 2: Training Transform Pipeline Created\")print(\"  - Resize to 64x64\")print(\"  - RandomRotation(40)\")print(\"  - RandomHorizontalFlip(0.5)\")print(\"  - RandomVerticalFlip(0.2)\")print(\"  - RandomAffine with shear(0.2)\")print(\"  - ColorJitter\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet statistics)\")# =============================================================================# TASK 3: Define val_transform pipeline (no augmentation)# =============================================================================val_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"\\nTASK 3: Validation Transform Pipeline Created\")print(\"  - Resize to 64x64\")print(\"  - ToTensor\")print(\"  - Normalize (same as training)\")# =============================================================================# LOAD DATASET AND CREATE TRAIN/VAL SPLIT# =============================================================================dataset_path = './images_dataSAT'# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)# Split into train (80%) and validation (20%)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42))# Apply validation transform to val datasetval_dataset.dataset.transform = val_transformprint(f\"\\nDataset Split:\")print(f\"  Total samples: {len(full_dataset)}\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")# =============================================================================# TASK 4: Create val_loader for the validation dataset# =============================================================================batch_size = 128# Create training loadertrain_loader = DataLoader(    train_dataset,    batch_size=batch_size,    shuffle=True,    num_workers=0)# Create validation loader (TASK 4 ANSWER)val_loader = DataLoader(    val_dataset,    batch_size=batch_size,    shuffle=False,  # No shuffling for validation    num_workers=0)print(f\"\\nTASK 4: DataLoaders Created\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Validation batches: {len(val_loader)}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# TASK 5: Purpose of tqdm# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 5: Purpose of tqdm\")print(\"=\"*70)print(\"\"\"tqdm (name derived from Arabic \"taqaddum\" meaning \"progress\") is a Pythonlibrary that provides fast, extensible progress bars for loops and iterations.**Purpose in Machine Learning Training:**1. **Visual Feedback**: Shows real-time progress of training/validation loops,   making it easier to monitor long-running processes.2. **Time Estimation**: Provides ETA (Estimated Time of Arrival) and elapsed   time, helping you plan workflow and identify bottlenecks.3. **Iteration Rate**: Shows iterations per second, useful for performance   monitoring and hardware utilization analysis.4. **User Experience**: Makes CLIs more user-friendly by providing interactive   feedback instead of blank terminals.**Example Usage:**   for batch in tqdm(train_loader, desc=\"Training\"):       # Training code here       pass**Output Example:**   Training: 100%|████████████| 45/45 [01:23<00:00,  1.85s/it]This shows: 100% complete, 45/45 batches, 1:23 elapsed, 1.85 seconds per batch.\"\"\")# =============================================================================# DEFINE CNN MODEL# =============================================================================class ConvNet(nn.Module):    def __init__(self, num_classes=2):        super(ConvNet, self).__init__()        # Convolutional feature extractor        self.features = nn.Sequential(            # Block 1: 3 -> 32            nn.Conv2d(3, 32, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(32),            # Block 2: 32 -> 64            nn.Conv2d(32, 64, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(64),            # Block 3: 64 -> 128            nn.Conv2d(64, 128, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(128),            # Block 4: 128 -> 256            nn.Conv2d(128, 256, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(256),            # Block 5: 256 -> 512            nn.Conv2d(256, 512, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(512),            # Block 6: 512 -> 1024            nn.Conv2d(512, 1024, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(1024),        )        # Global pooling and classifier        self.pool = nn.AdaptiveAvgPool2d((1, 1))        self.classifier = nn.Sequential(            nn.Flatten(),            nn.Linear(1024, 2048),            nn.ReLU(inplace=True),            nn.BatchNorm1d(2048),            nn.Dropout(0.4),            nn.Linear(2048, num_classes)        )    def forward(self, x):        x = self.features(x)        x = self.pool(x)        x = self.classifier(x)        return x# Initialize modeldevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')model = ConvNet(num_classes=2).to(device)print(f\"\\nModel initialized on device: {device}\")print(f\"\\nModel Architecture:\")print(model)# =============================================================================# TASK 6: Explain why train_loss, train_correct, train_total are reset every epoch# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 6: Why Reset Metrics Every Epoch\")print(\"=\"*70)print(\"\"\"The variables train_loss, train_correct, and train_total are reset at thebeginning of each epoch for several important reasons:1. **Per-Epoch Statistics**: We want to measure the model's performance   specifically for each epoch, not accumulate across all epochs.2. **Tracking Progress**: Resetting allows us to see how the model improves   from epoch to epoch. If we didn't reset:   - Loss would keep accumulating → meaningless large numbers   - Accuracy would be averaged across all training history   - We couldn't detect if training has plateaued or degraded3. **Meaningful Comparisons**: Each epoch should be evaluated independently   so we can compare:   - Is epoch 5 better than epoch 4?   - Has the model stopped improving?   - Should we stop training (early stopping)?4. **Debugging and Monitoring**: Epoch-wise metrics help identify:   - Overfitting (train accuracy high, val accuracy low)   - Underfitting (both accuracies plateau at low values)   - Training instability (metrics fluctuating wildly)**Example:**   Epoch 1: Loss=0.5, Accuracy=80%   Epoch 2: Loss=0.3, Accuracy=88%  ← Shows clear improvementWithout resetting, these individual epoch insights would be lost.\"\"\")# =============================================================================# TRAINING FUNCTION WITH TQDM# =============================================================================def train_epoch(model, loader, criterion, optimizer, device):    \"\"\"Train for one epoch\"\"\"    model.train()    running_loss = 0.0    running_corrects = 0    total_samples = 0    for images, labels in tqdm(loader, desc=\"Training\"):        images = images.to(device)        labels = labels.to(device)        # Forward pass        optimizer.zero_grad()        outputs = model(images)        loss = criterion(outputs, labels)        # Backward pass        loss.backward()        optimizer.step()        # Statistics        running_loss += loss.item() * images.size(0)        _, preds = torch.max(outputs, 1)        running_corrects += torch.sum(preds == labels).item()        total_samples += images.size(0)    epoch_loss = running_loss / total_samples    epoch_acc = running_corrects / total_samples    return epoch_loss, epoch_acc# =============================================================================# TASK 7: Why use torch.no_grad() in the validation loop?# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 7: Why Use torch.no_grad() in Validation Loop\")print(\"=\"*70)print(\"\"\"torch.no_grad() is a context manager that disables gradient computation.It should be used during validation/testing for several critical reasons:1. **Memory Efficiency**:   - Gradients require significant memory to store intermediate activations   - Without gradients, memory usage can be reduced by 50% or more   - Allows larger batch sizes during inference2. **Computational Speed**:   - Skipping gradient computation makes forward pass faster   - No need to track operations for backpropagation   - Validation runs 2-3x faster3. **Preventing Accidental Updates**:   - Ensures model weights cannot be accidentally modified during validation   - Adds a safety layer against programming errors4. **Correct Model Behavior**:   - Some layers (Dropout, BatchNorm) behave differently in eval mode   - Combined with model.eval(), ensures proper inference behavior**Example:**   with torch.no_grad():       outputs = model(images)       # No gradients computed or storedvs.   outputs = model(images)   # Gradients computed and stored (unnecessary overhead)**Memory Comparison:**   - With gradients: ~4GB VRAM   - Without gradients: ~2GB VRAM   → Allows 2x larger validation batches\"\"\")# =============================================================================# VALIDATION FUNCTION# =============================================================================def validate_epoch(model, loader, criterion, device):    \"\"\"Validate for one epoch\"\"\"    model.eval()    running_loss = 0.0    running_corrects = 0    total_samples = 0    with torch.no_grad():  # TASK 7: No gradient computation        for images, labels in tqdm(loader, desc=\"Validation\"):            images = images.to(device)            labels = labels.to(device)            # Forward pass only            outputs = model(images)            loss = criterion(outputs, labels)            # Statistics            running_loss += loss.item() * images.size(0)            _, preds = torch.max(outputs, 1)            running_corrects += torch.sum(preds == labels).item()            total_samples += images.size(0)    epoch_loss = running_loss / total_samples    epoch_acc = running_corrects / total_samples    return epoch_loss, epoch_acc# =============================================================================# TASK 8: List two metrics used to evaluate training performance# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 8: Two Metrics Used to Evaluate Training Performance\")print(\"=\"*70)print(\"\"\"Two essential metrics for evaluating training performance:1. **LOSS (Cross-Entropy Loss)**:   - Measures how well the model's predictions match the true labels   - Lower loss = better predictions   - Training loss should decrease over epochs   - Validation loss helps detect overfitting   - Formula for cross-entropy: L = -Σ y_true * log(y_pred)2. **ACCURACY**:   - Percentage of correct predictions   - Range: 0% to 100%   - Easy to interpret and communicate   - Training accuracy shows if model is learning   - Validation accuracy shows if model generalizes   - Formula: Accuracy = (Correct Predictions) / (Total Predictions)**Additional Important Metrics:**- Precision: True Positives / (True Positives + False Positives)- Recall: True Positives / (True Positives + False Negatives)- F1-Score: Harmonic mean of precision and recall- ROC-AUC: Area under ROC curve**Why These Two Are Primary:**- Loss guides the optimization (gradient descent minimizes loss)- Accuracy provides interpretable performance measure- Together they give complete picture of model performance\"\"\")# =============================================================================# TRAINING LOOP# =============================================================================# Training configurationnum_epochs = 5learning_rate = 0.001criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=learning_rate)# Storage for metricstrain_losses = []train_accs = []val_losses = []val_accs = []print(f\"\\nStarting training for {num_epochs} epochs...\")print(f\"Device: {device}\")print(f\"Learning rate: {learning_rate}\")print(f\"Batch size: {batch_size}\\n\")best_val_acc = 0.0for epoch in range(num_epochs):    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")    print(\"-\" * 50)    start_time = time.time()    # Train    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)    train_losses.append(train_loss)    train_accs.append(train_acc)    # Validate    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)    val_losses.append(val_loss)    val_accs.append(val_acc)    epoch_time = time.time() - start_time    # Print metrics    print(f\"\\nResults:\")    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")    print(f\"  Time: {epoch_time:.2f}s\")    # Save best model    if val_acc > best_val_acc:        best_val_acc = val_acc        torch.save(model.state_dict(), 'pytorch_cnn_best_model.pth')        print(f\"  *** New best model saved! Val Acc: {val_acc:.4f} ***\")print(\"\\nTraining completed!\")print(f\"Best validation accuracy: {best_val_acc:.4f}\")# =============================================================================# TASK 9: Plot model training loss# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot Loss (TASK 9 ANSWER)axes[0].plot(range(1, num_epochs+1), train_losses, label='Training Loss', marker='o', color='coral')axes[0].plot(range(1, num_epochs+1), val_losses, label='Validation Loss', marker='s', color='dodgerblue')axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Loss')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot Accuracyaxes[1].plot(range(1, num_epochs+1), train_accs, label='Training Accuracy', marker='o')axes[1].plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy', marker='s')axes[1].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Accuracy')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_cnn_training_history.png', dpi=300, bbox_inches='tight')plt.show()print(\"\\nTraining plots saved as 'pytorch_cnn_training_history.png'\")# =============================================================================# TASK 10: Retrieve predictions all_preds and ground truth all_labels from val_loader# =============================================================================# Collect all predictions and labelsall_preds = []all_labels = []model.eval()with torch.no_grad():    for images, labels in tqdm(val_loader, desc=\"Collecting predictions\"):        images = images.to(device)        outputs = model(images)        _, preds = torch.max(outputs, 1)        all_preds.extend(preds.cpu().numpy())        all_labels.extend(labels.numpy())# Convert to numpy arraysall_preds = np.array(all_preds)all_labels = np.array(all_labels)print(f\"\\nTASK 10: Predictions Collected\")print(f\"  Total samples: {len(all_preds)}\")print(f\"  Predictions shape: {all_preds.shape}\")print(f\"  Labels shape: {all_labels.shape}\")# Compute detailed metricsaccuracy = accuracy_score(all_labels, all_preds)precision = precision_score(all_labels, all_preds, average='binary')recall = recall_score(all_labels, all_preds, average='binary')f1 = f1_score(all_labels, all_preds, average='binary')print(f\"\\nDetailed Metrics:\")print(f\"  Accuracy:  {accuracy:.4f}\")print(f\"  Precision: {precision:.4f}\")print(f\"  Recall:    {recall:.4f}\")print(f\"  F1-Score:  {f1:.4f}\")# =============================================================================# BONUS: Confusion Matrix# =============================================================================from sklearn.metrics import confusion_matriximport seaborn as snscm = confusion_matrix(all_labels, all_preds)class_names = ['Non-Agricultural', 'Agricultural']plt.figure(figsize=(8, 6))sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',            xticklabels=class_names, yticklabels=class_names)plt.title('Confusion Matrix - PyTorch CNN', fontsize=14, fontweight='bold')plt.ylabel('True Label')plt.xlabel('Predicted Label')plt.tight_layout()plt.savefig('pytorch_cnn_confusion_matrix.png', dpi=300)plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 2 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Explained random initialization importance\")print(f\"Task 2: Created train_transform with augmentation\")print(f\"Task 3: Created val_transform without augmentation\")print(f\"Task 4: Created val_loader with {len(val_loader)} batches\")print(f\"Task 5: Explained purpose of tqdm for progress tracking\")print(f\"Task 6: Explained why metrics are reset each epoch\")print(f\"Task 7: Explained torch.no_grad() for memory efficiency\")print(f\"Task 8: Listed Loss and Accuracy as primary metrics\")print(f\"Task 9: Plotted training loss curves\")print(f\"Task 10: Collected {len(all_preds)} predictions and labels\")print(f\"\\nFinal Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 2 Lab 2: Implement and Test a PyTorch-Based ClassifierSolutions for all tasks (20 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L2-Implement-and-Test-a-PyTorch-Based-Classifier-v1.ipynb\"\"\"import osimport timeimport numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, random_splitfrom torchvision import datasets, transformsfrom tqdm import tqdmimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score# =============================================================================# TASK 1: Explain the usefulness of random initialization# =============================================================================print(\"=\"*70)print(\"TASK 1: Usefulness of Random Initialization\")print(\"=\"*70)print(\"\"\"Random initialization is crucial in neural networks for several reasons:1. **Breaking Symmetry**: If all weights are initialized to the same value   (e.g., all zeros), all neurons in a layer would compute the same output and   receive the same gradients during backpropagation. This means they would all   update identically, making multiple neurons redundant.2. **Enabling Learning**: Random initialization creates diverse initial states   for neurons, allowing them to learn different features from the data.3. **Gradient Flow**: Proper random initialization (like He initialization for   ReLU activations) helps maintain good gradient flow during backpropagation,   preventing vanishing or exploding gradients.4. **Convergence**: Well-initialized networks converge faster and reach better   local minima compared to poorly initialized networks.**Common Initialization Strategies:**- Xavier/Glorot: For sigmoid/tanh activations- He initialization: For ReLU activations (used in our CNN)- Uniform or Normal distributions with controlled varianceIn our CNN, we use He uniform initialization which is designed to work wellwith ReLU activations by maintaining variance across layers.\"\"\")# =============================================================================# TASK 2: Define train_transform pipeline with augmentation# =============================================================================img_size = 64train_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.RandomRotation(40),                    # Random rotation ±40 degrees    transforms.RandomHorizontalFlip(p=0.5),          # 50% chance horizontal flip    transforms.RandomVerticalFlip(p=0.2),            # 20% chance vertical flip    transforms.RandomAffine(0, shear=0.2),          # Shear transformation    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Color augmentation    transforms.ToTensor(),                            # Convert to tensor    transforms.Normalize(                             # Normalize with ImageNet stats        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"\\nTASK 2: Training Transform Pipeline Created\")print(\"  - Resize to 64x64\")print(\"  - RandomRotation(40)\")print(\"  - RandomHorizontalFlip(0.5)\")print(\"  - RandomVerticalFlip(0.2)\")print(\"  - RandomAffine with shear(0.2)\")print(\"  - ColorJitter\")print(\"  - ToTensor\")print(\"  - Normalize (ImageNet statistics)\")# =============================================================================# TASK 3: Define val_transform pipeline (no augmentation)# =============================================================================val_transform = transforms.Compose([    transforms.Resize((img_size, img_size)),    transforms.ToTensor(),    transforms.Normalize(        mean=[0.485, 0.456, 0.406],        std=[0.229, 0.224, 0.225]    )])print(\"\\nTASK 3: Validation Transform Pipeline Created\")print(\"  - Resize to 64x64\")print(\"  - ToTensor\")print(\"  - Normalize (same as training)\")# =============================================================================# LOAD DATASET AND CREATE TRAIN/VAL SPLIT# =============================================================================dataset_path = './images_dataSAT'# Load full datasetfull_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)# Split into train (80%) and validation (20%)train_size = int(0.8 * len(full_dataset))val_size = len(full_dataset) - train_sizetrain_dataset, val_dataset = random_split(    full_dataset,    [train_size, val_size],    generator=torch.Generator().manual_seed(42))# Apply validation transform to val datasetval_dataset.dataset.transform = val_transformprint(f\"\\nDataset Split:\")print(f\"  Total samples: {len(full_dataset)}\")print(f\"  Training samples: {len(train_dataset)}\")print(f\"  Validation samples: {len(val_dataset)}\")# =============================================================================# TASK 4: Create val_loader for the validation dataset# =============================================================================batch_size = 128# Create training loadertrain_loader = DataLoader(    train_dataset,    batch_size=batch_size,    shuffle=True,    num_workers=0)# Create validation loader (TASK 4 ANSWER)val_loader = DataLoader(    val_dataset,    batch_size=batch_size,    shuffle=False,  # No shuffling for validation    num_workers=0)print(f\"\\nTASK 4: DataLoaders Created\")print(f\"  Train batches: {len(train_loader)}\")print(f\"  Validation batches: {len(val_loader)}\")print(f\"  Batch size: {batch_size}\")# =============================================================================# TASK 5: Purpose of tqdm# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 5: Purpose of tqdm\")print(\"=\"*70)print(\"\"\"tqdm (name derived from Arabic \"taqaddum\" meaning \"progress\") is a Pythonlibrary that provides fast, extensible progress bars for loops and iterations.**Purpose in Machine Learning Training:**1. **Visual Feedback**: Shows real-time progress of training/validation loops,   making it easier to monitor long-running processes.2. **Time Estimation**: Provides ETA (Estimated Time of Arrival) and elapsed   time, helping you plan workflow and identify bottlenecks.3. **Iteration Rate**: Shows iterations per second, useful for performance   monitoring and hardware utilization analysis.4. **User Experience**: Makes CLIs more user-friendly by providing interactive   feedback instead of blank terminals.**Example Usage:**   for batch in tqdm(train_loader, desc=\"Training\"):       # Training code here       pass**Output Example:**   Training: 100%|████████████| 45/45 [01:23<00:00,  1.85s/it]This shows: 100% complete, 45/45 batches, 1:23 elapsed, 1.85 seconds per batch.\"\"\")# =============================================================================# DEFINE CNN MODEL# =============================================================================class ConvNet(nn.Module):    def __init__(self, num_classes=2):        super(ConvNet, self).__init__()        # Convolutional feature extractor        self.features = nn.Sequential(            # Block 1: 3 -> 32            nn.Conv2d(3, 32, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(32),            # Block 2: 32 -> 64            nn.Conv2d(32, 64, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(64),            # Block 3: 64 -> 128            nn.Conv2d(64, 128, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(128),            # Block 4: 128 -> 256            nn.Conv2d(128, 256, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(256),            # Block 5: 256 -> 512            nn.Conv2d(256, 512, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(512),            # Block 6: 512 -> 1024            nn.Conv2d(512, 1024, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(2),            nn.BatchNorm2d(1024),        )        # Global pooling and classifier        self.pool = nn.AdaptiveAvgPool2d((1, 1))        self.classifier = nn.Sequential(            nn.Flatten(),            nn.Linear(1024, 2048),            nn.ReLU(inplace=True),            nn.BatchNorm1d(2048),            nn.Dropout(0.4),            nn.Linear(2048, num_classes)        )    def forward(self, x):        x = self.features(x)        x = self.pool(x)        x = self.classifier(x)        return x# Initialize modeldevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')model = ConvNet(num_classes=2).to(device)print(f\"\\nModel initialized on device: {device}\")print(f\"\\nModel Architecture:\")print(model)# =============================================================================# TASK 6: Explain why train_loss, train_correct, train_total are reset every epoch# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 6: Why Reset Metrics Every Epoch\")print(\"=\"*70)print(\"\"\"The variables train_loss, train_correct, and train_total are reset at thebeginning of each epoch for several important reasons:1. **Per-Epoch Statistics**: We want to measure the model's performance   specifically for each epoch, not accumulate across all epochs.2. **Tracking Progress**: Resetting allows us to see how the model improves   from epoch to epoch. If we didn't reset:   - Loss would keep accumulating → meaningless large numbers   - Accuracy would be averaged across all training history   - We couldn't detect if training has plateaued or degraded3. **Meaningful Comparisons**: Each epoch should be evaluated independently   so we can compare:   - Is epoch 5 better than epoch 4?   - Has the model stopped improving?   - Should we stop training (early stopping)?4. **Debugging and Monitoring**: Epoch-wise metrics help identify:   - Overfitting (train accuracy high, val accuracy low)   - Underfitting (both accuracies plateau at low values)   - Training instability (metrics fluctuating wildly)**Example:**   Epoch 1: Loss=0.5, Accuracy=80%   Epoch 2: Loss=0.3, Accuracy=88%  ← Shows clear improvementWithout resetting, these individual epoch insights would be lost.\"\"\")# =============================================================================# TRAINING FUNCTION WITH TQDM# =============================================================================def train_epoch(model, loader, criterion, optimizer, device):    \"\"\"Train for one epoch\"\"\"    model.train()    running_loss = 0.0    running_corrects = 0    total_samples = 0    for images, labels in tqdm(loader, desc=\"Training\"):        images = images.to(device)        labels = labels.to(device)        # Forward pass        optimizer.zero_grad()        outputs = model(images)        loss = criterion(outputs, labels)        # Backward pass        loss.backward()        optimizer.step()        # Statistics        running_loss += loss.item() * images.size(0)        _, preds = torch.max(outputs, 1)        running_corrects += torch.sum(preds == labels).item()        total_samples += images.size(0)    epoch_loss = running_loss / total_samples    epoch_acc = running_corrects / total_samples    return epoch_loss, epoch_acc# =============================================================================# TASK 7: Why use torch.no_grad() in the validation loop?# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 7: Why Use torch.no_grad() in Validation Loop\")print(\"=\"*70)print(\"\"\"torch.no_grad() is a context manager that disables gradient computation.It should be used during validation/testing for several critical reasons:1. **Memory Efficiency**:   - Gradients require significant memory to store intermediate activations   - Without gradients, memory usage can be reduced by 50% or more   - Allows larger batch sizes during inference2. **Computational Speed**:   - Skipping gradient computation makes forward pass faster   - No need to track operations for backpropagation   - Validation runs 2-3x faster3. **Preventing Accidental Updates**:   - Ensures model weights cannot be accidentally modified during validation   - Adds a safety layer against programming errors4. **Correct Model Behavior**:   - Some layers (Dropout, BatchNorm) behave differently in eval mode   - Combined with model.eval(), ensures proper inference behavior**Example:**   with torch.no_grad():       outputs = model(images)       # No gradients computed or storedvs.   outputs = model(images)   # Gradients computed and stored (unnecessary overhead)**Memory Comparison:**   - With gradients: ~4GB VRAM   - Without gradients: ~2GB VRAM   → Allows 2x larger validation batches\"\"\")# =============================================================================# VALIDATION FUNCTION# =============================================================================def validate_epoch(model, loader, criterion, device):    \"\"\"Validate for one epoch\"\"\"    model.eval()    running_loss = 0.0    running_corrects = 0    total_samples = 0    with torch.no_grad():  # TASK 7: No gradient computation        for images, labels in tqdm(loader, desc=\"Validation\"):            images = images.to(device)            labels = labels.to(device)            # Forward pass only            outputs = model(images)            loss = criterion(outputs, labels)            # Statistics            running_loss += loss.item() * images.size(0)            _, preds = torch.max(outputs, 1)            running_corrects += torch.sum(preds == labels).item()            total_samples += images.size(0)    epoch_loss = running_loss / total_samples    epoch_acc = running_corrects / total_samples    return epoch_loss, epoch_acc# =============================================================================# TASK 8: List two metrics used to evaluate training performance# =============================================================================print(\"\\n\" + \"=\"*70)print(\"TASK 8: Two Metrics Used to Evaluate Training Performance\")print(\"=\"*70)print(\"\"\"Two essential metrics for evaluating training performance:1. **LOSS (Cross-Entropy Loss)**:   - Measures how well the model's predictions match the true labels   - Lower loss = better predictions   - Training loss should decrease over epochs   - Validation loss helps detect overfitting   - Formula for cross-entropy: L = -Σ y_true * log(y_pred)2. **ACCURACY**:   - Percentage of correct predictions   - Range: 0% to 100%   - Easy to interpret and communicate   - Training accuracy shows if model is learning   - Validation accuracy shows if model generalizes   - Formula: Accuracy = (Correct Predictions) / (Total Predictions)**Additional Important Metrics:**- Precision: True Positives / (True Positives + False Positives)- Recall: True Positives / (True Positives + False Negatives)- F1-Score: Harmonic mean of precision and recall- ROC-AUC: Area under ROC curve**Why These Two Are Primary:**- Loss guides the optimization (gradient descent minimizes loss)- Accuracy provides interpretable performance measure- Together they give complete picture of model performance\"\"\")# =============================================================================# TRAINING LOOP# =============================================================================# Training configurationnum_epochs = 5learning_rate = 0.001criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=learning_rate)# Storage for metricstrain_losses = []train_accs = []val_losses = []val_accs = []print(f\"\\nStarting training for {num_epochs} epochs...\")print(f\"Device: {device}\")print(f\"Learning rate: {learning_rate}\")print(f\"Batch size: {batch_size}\\n\")best_val_acc = 0.0for epoch in range(num_epochs):    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")    print(\"-\" * 50)    start_time = time.time()    # Train    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)    train_losses.append(train_loss)    train_accs.append(train_acc)    # Validate    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)    val_losses.append(val_loss)    val_accs.append(val_acc)    epoch_time = time.time() - start_time    # Print metrics    print(f\"\\nResults:\")    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")    print(f\"  Time: {epoch_time:.2f}s\")    # Save best model    if val_acc > best_val_acc:        best_val_acc = val_acc        torch.save(model.state_dict(), 'pytorch_cnn_best_model.pth')        print(f\"  *** New best model saved! Val Acc: {val_acc:.4f} ***\")print(\"\\nTraining completed!\")print(f\"Best validation accuracy: {best_val_acc:.4f}\")# =============================================================================# TASK 9: Plot model training loss# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot Loss (TASK 9 ANSWER)axes[0].plot(range(1, num_epochs+1), train_losses, label='Training Loss', marker='o', color='coral')axes[0].plot(range(1, num_epochs+1), val_losses, label='Validation Loss', marker='s', color='dodgerblue')axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Loss')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot Accuracyaxes[1].plot(range(1, num_epochs+1), train_accs, label='Training Accuracy', marker='o')axes[1].plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy', marker='s')axes[1].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Accuracy')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('pytorch_cnn_training_history.png', dpi=300, bbox_inches='tight')plt.show()print(\"\\nTraining plots saved as 'pytorch_cnn_training_history.png'\")# =============================================================================# TASK 10: Retrieve predictions all_preds and ground truth all_labels from val_loader# =============================================================================# Collect all predictions and labelsall_preds = []all_labels = []model.eval()with torch.no_grad():    for images, labels in tqdm(val_loader, desc=\"Collecting predictions\"):        images = images.to(device)        outputs = model(images)        _, preds = torch.max(outputs, 1)        all_preds.extend(preds.cpu().numpy())        all_labels.extend(labels.numpy())# Convert to numpy arraysall_preds = np.array(all_preds)all_labels = np.array(all_labels)print(f\"\\nTASK 10: Predictions Collected\")print(f\"  Total samples: {len(all_preds)}\")print(f\"  Predictions shape: {all_preds.shape}\")print(f\"  Labels shape: {all_labels.shape}\")# Compute detailed metricsaccuracy = accuracy_score(all_labels, all_preds)precision = precision_score(all_labels, all_preds, average='binary')recall = recall_score(all_labels, all_preds, average='binary')f1 = f1_score(all_labels, all_preds, average='binary')print(f\"\\nDetailed Metrics:\")print(f\"  Accuracy:  {accuracy:.4f}\")print(f\"  Precision: {precision:.4f}\")print(f\"  Recall:    {recall:.4f}\")print(f\"  F1-Score:  {f1:.4f}\")# =============================================================================# BONUS: Confusion Matrix# =============================================================================from sklearn.metrics import confusion_matriximport seaborn as snscm = confusion_matrix(all_labels, all_preds)class_names = ['Non-Agricultural', 'Agricultural']plt.figure(figsize=(8, 6))sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',            xticklabels=class_names, yticklabels=class_names)plt.title('Confusion Matrix - PyTorch CNN', fontsize=14, fontweight='bold')plt.ylabel('True Label')plt.xlabel('Predicted Label')plt.tight_layout()plt.savefig('pytorch_cnn_confusion_matrix.png', dpi=300)plt.show()# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 2 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Explained random initialization importance\")print(f\"Task 2: Created train_transform with augmentation\")print(f\"Task 3: Created val_transform without augmentation\")print(f\"Task 4: Created val_loader with {len(val_loader)} batches\")print(f\"Task 5: Explained purpose of tqdm for progress tracking\")print(f\"Task 6: Explained why metrics are reset each epoch\")print(f\"Task 7: Explained torch.no_grad() for memory efficiency\")print(f\"Task 8: Listed Loss and Accuracy as primary metrics\")print(f\"Task 9: Plotted training loss curves\")print(f\"Task 10: Collected {len(all_preds)} predictions and labels\")print(f\"\\nFinal Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "84f121cb8f96a0ca81ff2351d976d75274c362e809c882a022356476367ff042",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12e2c0a900c14f71b941a302f7f9e3a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15ae8e96e6df40f4a475ad5fbb2c6a95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17d139aa03584dcfa1fa4e38e78a53b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_467cf81cff604efbb426c891c7393024",
       "placeholder": "​",
       "style": "IPY_MODEL_5b79c0d3579a46068ef8806573397237",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 10961.34it/s]"
      }
     },
     "242bda45ed6342bf9296a7b0cd1368a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd706176a10345b199ae7a2959abdeb3",
       "placeholder": "​",
       "style": "IPY_MODEL_12e2c0a900c14f71b941a302f7f9e3a9",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 13689810.53it/s]"
      }
     },
     "28e01b58c2f44af794e2cde2ecbedaa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a47b166f039c40149db5f56f5e0229b2",
        "IPY_MODEL_ebc3ae2d77954fc38869397b3512376c",
        "IPY_MODEL_17d139aa03584dcfa1fa4e38e78a53b0"
       ],
       "layout": "IPY_MODEL_c2875db1b296446f90cd4e976c5a0f40",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2b8c545c373348b4b2618e466a73baba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "320a60b568cd41d88de57522f0ec3997": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81f0a1b5538c4a14855a7772f5c3bf58",
       "placeholder": "​",
       "style": "IPY_MODEL_694876d5d60c45409f005661ea179832",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "35fc121b4e87471bab1a0264d5656f6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_320a60b568cd41d88de57522f0ec3997",
        "IPY_MODEL_b65c82acd35c457fbdce9616faf1639f",
        "IPY_MODEL_242bda45ed6342bf9296a7b0cd1368a1"
       ],
       "layout": "IPY_MODEL_7a22aa1bde4a4bf39097e9f3e0043739",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3aa3c91419434fe88ef8d38f6238aae5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d5f67a47cf245ae9bd40501475c868a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "467cf81cff604efbb426c891c7393024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b79c0d3579a46068ef8806573397237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "694876d5d60c45409f005661ea179832": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "764baaae6e534911baf94c9d0fbfff63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a22aa1bde4a4bf39097e9f3e0043739": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81f0a1b5538c4a14855a7772f5c3bf58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a47b166f039c40149db5f56f5e0229b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_764baaae6e534911baf94c9d0fbfff63",
       "placeholder": "​",
       "style": "IPY_MODEL_2b8c545c373348b4b2618e466a73baba",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "b65c82acd35c457fbdce9616faf1639f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3aa3c91419434fe88ef8d38f6238aae5",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d5f67a47cf245ae9bd40501475c868a",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "c2875db1b296446f90cd4e976c5a0f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd706176a10345b199ae7a2959abdeb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebc3ae2d77954fc38869397b3512376c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_15ae8e96e6df40f4a475ad5fbb2c6a95",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f186e3d85d714110bfca3efe171ddbce",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     },
     "f186e3d85d714110bfca3efe171ddbce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
