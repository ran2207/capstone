{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dff419f-5632-47ba-b342-de1e7a9536e1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3d9bb-7ca0-4497-883f-8ceee0420703",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Train and Evaluate a Keras-Based Classifier </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39708660-b3bd-421a-ba68-5c300fe445fb",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc14f2-0c74-49cf-ade7-020ee372ea87",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "    \n",
    "1. Create a Keras-based convolutional neural network (CNN) model.\n",
    "2. Train the CNN model on agricultural and non-agricultural land dataset.\n",
    "3. Evaluate the performance of the CNN model. \n",
    "    \n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47bc76-d95e-4eef-8f85-ff1c1ea1fbbb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **Keras-based convolutional neural network (CNN)** for image classification, for agricultural images in our case. This lab will cover the following:\n",
    "1. Data preparation\n",
    "2. Model architecture definition\n",
    "3. Training\n",
    "4. Model performance analysis.\n",
    "\n",
    "The goal is to classify satellite images into two categories: \"agricultural\" and \"non-agricultural\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b3315-53ef-4edd-9906-c48cf6d8b285",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Model definition and compilation](#Model-definition-and-compilation)\n",
    "4. [Model training](#Model-training)\n",
    "5. [Download and save the model](#Download-and-save-the-trained-model)\n",
    "6. [Model evaluation and visualization](#Model-evaluation-and-visualization)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d980e7",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b2c08-19fa-4100-a133-3b9ff87a834e",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following cell to install them; it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb28acb-eb30-43ec-ad4d-ed929accf500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:06.615010Z",
     "iopub.status.busy": "2025-10-27T07:48:06.614620Z",
     "iopub.status.idle": "2025-10-27T07:48:06.625578Z",
     "shell.execute_reply": "2025-10-27T07:48:06.623587Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where we try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784459c1-e4cc-4364-a7fc-a0158cf5a9ff",
   "metadata": {},
   "source": [
    "### Library installation - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a973522-25e8-4411-8fc0-f556e5368de7",
   "metadata": {},
   "source": [
    "Next, let’s install the non-AI libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2670034-d505-4c6e-8447-4f2ce3f1c722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:06.629449Z",
     "iopub.status.busy": "2025-10-27T07:48:06.629163Z",
     "iopub.status.idle": "2025-10-27T07:48:08.198480Z",
     "shell.execute_reply": "2025-10-27T07:48:08.197797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.58 ms, sys: 11.8 ms, total: 19.4 ms\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98e265-dd43-42c9-8cd7-ba7e9458ba5e",
   "metadata": {},
   "source": [
    "Now, check if the above libraries are installed properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3b63ba-cdda-411c-8dc0-0f8d2cd528c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:08.200446Z",
     "iopub.status.busy": "2025-10-27T07:48:08.200300Z",
     "iopub.status.idle": "2025-10-27T07:48:08.202949Z",
     "shell.execute_reply": "2025-10-27T07:48:08.202369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b935986-bf16-4b71-96f6-2761f2f3ff08",
   "metadata": {},
   "source": [
    "### `TensorFlow` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743fb43-7101-460c-afc7-b3bacbaf2354",
   "metadata": {},
   "source": [
    "Next, install the `TensorFlow` library using the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6beced9-f72c-4dc0-9c5e-a053095c4d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:08.204368Z",
     "iopub.status.busy": "2025-10-27T07:48:08.204261Z",
     "iopub.status.idle": "2025-10-27T07:48:08.928094Z",
     "shell.execute_reply": "2025-10-27T07:48:08.927416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.19 (from versions: 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.19\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 4.21 ms, sys: 4.85 ms, total: 9.06 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daaf1f-8e40-4bb4-bf84-55e0231e7061",
   "metadata": {},
   "source": [
    "### `scikit-learn` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a370c3-e436-40f9-846f-7c3a0aabfedd",
   "metadata": {},
   "source": [
    "Install the scikit-learn library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a71610f-410e-402c-8fc4-83e487d71ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:08.930261Z",
     "iopub.status.busy": "2025-10-27T07:48:08.930120Z",
     "iopub.status.idle": "2025-10-27T07:48:14.004034Z",
     "shell.execute_reply": "2025-10-27T07:48:14.003523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.7.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (2.3.4)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.16.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading scikit_learn-1.7.0-cp313-cp313-macosx_12_0_arm64.whl (10.6 MB)\r\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/10.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/10.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/10.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/10.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/10.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m8.4/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m9.4/10.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m10.5/10.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.7.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling scikit-learn-1.7.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.7.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed scikit-learn-1.7.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abde24a-e929-4a2f-80d2-4dfc2735d140",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad78903-2d49-410c-900c-eacac5bfeff1",
   "metadata": {},
   "source": [
    "Import the non-AI libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75d5a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.005889Z",
     "iopub.status.busy": "2025-10-27T07:48:14.005757Z",
     "iopub.status.idle": "2025-10-27T07:48:14.007602Z",
     "shell.execute_reply": "2025-10-27T07:48:14.007290Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d385b445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.008976Z",
     "iopub.status.busy": "2025-10-27T07:48:14.008877Z",
     "iopub.status.idle": "2025-10-27T07:48:14.212984Z",
     "shell.execute_reply": "2025-10-27T07:48:14.212494Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c356149",
   "metadata": {},
   "source": [
    "### TensorFlow environment configuration\n",
    "\n",
    "This cell sets environment variables for TensorFlow. \n",
    "- `TF_ENABLE_ONEDNN_OPTS` is set to \"0\" to disable Intel oneDNN optimizations, which can sometimes lead to issues or unwanted behavior on specific hardware configurations.\n",
    "- `TF_CPP_MIN_LOG_LEVEL` is set to \"2,\" instructing TensorFlow to only display warning and error messages from its C++ backend. This reduces verbose output and keeps the console cleaner, focusing on more critical information during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119fe8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.214341Z",
     "iopub.status.busy": "2025-10-27T07:48:14.214238Z",
     "iopub.status.idle": "2025-10-27T07:48:14.215949Z",
     "shell.execute_reply": "2025-10-27T07:48:14.215605Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfe70f",
   "metadata": {},
   "source": [
    "Next, set up the data extraction directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e40eeac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.217028Z",
     "iopub.status.busy": "2025-10-27T07:48:14.216944Z",
     "iopub.status.idle": "2025-10-27T07:48:14.218536Z",
     "shell.execute_reply": "2025-10-27T07:48:14.218136Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace2d9c",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Define the dataset URL\n",
    "\n",
    "\n",
    "We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30f5074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.219521Z",
     "iopub.status.busy": "2025-10-27T07:48:14.219458Z",
     "iopub.status.idle": "2025-10-27T07:48:14.220945Z",
     "shell.execute_reply": "2025-10-27T07:48:14.220602Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4712e2",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e2b732-75ad-4fd1-b892-222e8208a224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.222044Z",
     "iopub.status.busy": "2025-10-27T07:48:14.221966Z",
     "iopub.status.idle": "2025-10-27T07:48:14.224883Z",
     "shell.execute_reply": "2025-10-27T07:48:14.224437Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eea70fc-ca44-474f-950c-b8ffd86ca779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:14.225959Z",
     "iopub.status.busy": "2025-10-27T07:48:14.225872Z",
     "iopub.status.idle": "2025-10-27T07:48:20.550556Z",
     "shell.execute_reply": "2025-10-27T07:48:20.550181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf371c20e55740f1b3a8cc1a94fef3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebb6f1fe3b047299b011b57ccc458f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (for example, 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbf20f",
   "metadata": {},
   "source": [
    "### Import deep learning and ML libraries\n",
    "\n",
    "Here is a brief description of the usage of the **Keras** libraries and methods that will be used:\n",
    "- `Sequential` models are a linear stack of layers.\n",
    "- `Conv2D` and `MaxPooling2D` are fundamental for CNNs, extracting features and reducing dimensionality.\n",
    "- `BatchNormalization` stabilizes training.\n",
    "- `Dense` layers form the classifier.\n",
    "- `Dropout` regularizes to prevent overfitting.\n",
    "- `Adam` is an adaptive learning rate optimizer.\n",
    "- `ImageDataGenerator` automates data loading and augmentation.\n",
    "- `HeUniform` is used for weight initialization.\n",
    "\n",
    "\n",
    "**Scikit-learn** (`sklearn.metrics`) provides the following metrics for model performance assessment: \n",
    "- `classification_report`\n",
    "- `confusion_matrix`\n",
    "- `accuracy_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d220f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:20.551811Z",
     "iopub.status.busy": "2025-10-27T07:48:20.551709Z",
     "iopub.status.idle": "2025-10-27T07:48:32.717645Z",
     "shell.execute_reply": "2025-10-27T07:48:32.716327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported the libraries\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Succesfully imported the libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003882d4-4301-4dde-a0b6-a071533b0766",
   "metadata": {},
   "source": [
    "### Get the processing device\n",
    "Check the availability of GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2908110-98d2-4890-b7f7-5a11f82ad859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.720112Z",
     "iopub.status.busy": "2025-10-27T07:48:32.719789Z",
     "iopub.status.idle": "2025-10-27T07:48:32.723458Z",
     "shell.execute_reply": "2025-10-27T07:48:32.722722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "device = \"gpu\" if gpu_list !=[] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9da38a",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82346e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.725514Z",
     "iopub.status.busy": "2025-10-27T07:48:32.725316Z",
     "iopub.status.idle": "2025-10-27T07:48:32.727821Z",
     "shell.execute_reply": "2025-10-27T07:48:32.727241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc13d4d",
   "metadata": {},
   "source": [
    "### Define the dataset path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f6c8eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.729487Z",
     "iopub.status.busy": "2025-10-27T07:48:32.729335Z",
     "iopub.status.idle": "2025-10-27T07:48:32.731865Z",
     "shell.execute_reply": "2025-10-27T07:48:32.731208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda9646",
   "metadata": {},
   "source": [
    "### Create the dataset file list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084fba0-b7b1-4a5c-baa6-c88a0ff34f6b",
   "metadata": {},
   "source": [
    "Now that we have downloaded the dataset, perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61942ff0-5514-4b3d-8173-861885df05b3",
   "metadata": {},
   "source": [
    "### **Task 1:** Recursively walk through the `dataset_path` using `os.walk` function to create a list **`fnames`** of all image files. \n",
    "Print the total count of files found and displays the first two and last two file paths. \n",
    "\n",
    "Absolute path is captured using `os.path.join(dirname, filename)` and used in `ImageDataGenerator` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2f2b69-ee38-4174-b86f-694d6377a84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.733637Z",
     "iopub.status.busy": "2025-10-27T07:48:32.733518Z",
     "iopub.status.idle": "2025-10-27T07:48:32.735557Z",
     "shell.execute_reply": "2025-10-27T07:48:32.734930Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398d118-7eac-41e4-aaa1-3771afcb82fe",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "\n",
    "fnames = []\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        fnames.append(os.path.join(dirname, filename))\n",
    "print(f\"total files in dataset: {len(fnames)}\")\n",
    "nfname_print=2\n",
    "for f in fnames[:nfname_print]:\n",
    "    print(f)\n",
    "for f in fnames[-nfname_print:]:\n",
    "    print(f)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34fa2a",
   "metadata": {},
   "source": [
    "### Define the model hyperparameters\n",
    "\n",
    "Hyperparameters are configurable values that are set before the training process begins. \n",
    "\n",
    "This cell initializes several key hyperparameters that will govern the training process and the model's input. Here is the list of hyperparameters:\n",
    "\n",
    "1. `img_w` and `img_h` define the width and height for resizing input images.\n",
    "2. `n_channels` defines the number of color channels (3 for RGB).\n",
    "3. `n_epochs` sets the total training iterations over the dataset.\n",
    "4. `batch_size` sets the number of samples processed per batch in the epoch.\n",
    "5. `lr` defines the learning rate for the optimizer.\n",
    "6. `steps_per_epoch` are total number of steps used for training. **None** means the number is calculated automatically.\n",
    "7. `validation_steps` are total number of steps used for validating the model on validation data. **None** means the number is calculated automatically.\n",
    "\n",
    "These hyperparameters are crucial for controlling model performance and resource utilization and significantly influence a model's performance and training efficiency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97eae79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.737161Z",
     "iopub.status.busy": "2025-10-27T07:48:32.737041Z",
     "iopub.status.idle": "2025-10-27T07:48:32.739365Z",
     "shell.execute_reply": "2025-10-27T07:48:32.738793Z"
    }
   },
   "outputs": [],
   "source": [
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "lr = 0.001 # Learning rate\n",
    "n_epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "\n",
    "steps_per_epoch = None\n",
    "validation_steps = None \n",
    "\n",
    "model_name = \"ai_capstone_keras_best_model.model.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba0671",
   "metadata": {},
   "source": [
    "### Configure `ImageDataGenerator` for Augmentation\n",
    "\n",
    "\n",
    "Now, we instantiate the `ImageDataGenerator` with data augmentation parameters:\n",
    "\n",
    "- `rescale=1./255` normalizes pixel values to [0, 1].\n",
    "- `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`, and `zoom_range` define random transformations to apply to images during training, increasing dataset diversity.\n",
    "- `horizontal_flip=True` enables random horizontal mirroring.\n",
    "- `fill_mode='nearest'` specifies how new pixels are filled after transformations.\n",
    "- `validation_split=0.2` reserves 20% of data for validation.\n",
    "\n",
    "\n",
    "This setup boosts model robustness against variations in real-world images. `ImageDataGenerator` performs these transformations on-the-fly, making it efficient for large datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88cc5bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.740803Z",
     "iopub.status.busy": "2025-10-27T07:48:32.740691Z",
     "iopub.status.idle": "2025-10-27T07:48:32.742840Z",
     "shell.execute_reply": "2025-10-27T07:48:32.742338Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2ce28",
   "metadata": {},
   "source": [
    "### Create training and validation data generators\n",
    "\n",
    " `ImageDataGenerator` is used to create `train_generator` and `validation_generator`. \n",
    "`flow_from_directory()` is a convenient method of `ImageDataGenerator` for automatically creating data pipelines from structured image directories.\n",
    " The generator resize images to `(img_w, img_h)` and group them into `batch_size` chunks. `class_mode=\"binary\"` indicates a two-class classification task. \n",
    " \n",
    " The `subset` parameter is used to assign 80% of the data for training and 20% for validation based on the `validation_split`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ff1974-13dc-4fce-842b-041edbb2cfc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.744210Z",
     "iopub.status.busy": "2025-10-27T07:48:32.744106Z",
     "iopub.status.idle": "2025-10-27T07:48:32.819833Z",
     "shell.execute_reply": "2025-10-27T07:48:32.819366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(dataset_path,\n",
    " target_size = (img_w, img_h),\n",
    " batch_size= batch_size,\n",
    " class_mode=\"binary\",\n",
    " subset=\"training\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25805f5-962c-42dc-9d90-cb95065a101e",
   "metadata": {},
   "source": [
    "Here is your next task. We have created the `train_generator`, let's create the `validation_generator`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7b840-63d3-4881-9a65-8f0b9f8d9894",
   "metadata": {},
   "source": [
    "### **Task 2:** Create the `validation_generator` from `dataset_path`.\n",
    "Use `target_size`, and `class_mode` similar to `train_generator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3801e435-d4bf-4630-ab47-24f8c75c6d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.821029Z",
     "iopub.status.busy": "2025-10-27T07:48:32.820963Z",
     "iopub.status.idle": "2025-10-27T07:48:32.822373Z",
     "shell.execute_reply": "2025-10-27T07:48:32.822071Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6cafb-8064-40a1-83c2-c1a370c00976",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = datagen.flow_from_directory(dataset_path,\n",
    "                                                    target_size =(img_w, img_h),\n",
    "                                                    batch_size = batch_size, \n",
    "                                                    class_mode=\"binary\",\n",
    "                                                    subset=\"validation\"\n",
    "                                                    )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897734",
   "metadata": {},
   "source": [
    "## Model definition and compilation\n",
    "\n",
    "### Define the convolutional neural network (CNN) architecture\n",
    "\n",
    "The model architecture is composed of several key components:\n",
    "- **`Sequential`** is a linear stack of layers in Keras.\n",
    "- **Conv2D** layers perform convolution operations, acting as feature detectors.\n",
    "- **MaxPooling2D** reduces the spatial dimensions of the feature maps.\n",
    "-  **BatchNormalization** normalizes layer inputs, stabilizing and accelerating training.\n",
    "-  **GlobalAveragePooling2D** summarizes feature maps into a single vector, reducing parameters.\n",
    "-  **Dense** (fully connected) layers learn complex patterns from these features.\n",
    "-  **Dropout** is a regularization technique that randomly deactivates neurons during training.\n",
    "-  **Sigmoid** activation is used for binary classification, mapping outputs to probabilities.\n",
    "-  **HeUniform** initializer is suitable for ReLU activations.\n",
    "-  **The final output `Dense` layer** uses a `sigmoid` activation for binary classification, outputting a probability between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7197185c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.823423Z",
     "iopub.status.busy": "2025-10-27T07:48:32.823348Z",
     "iopub.status.idle": "2025-10-27T07:48:32.908499Z",
     "shell.execute_reply": "2025-10-27T07:48:32.908008Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    ###\n",
    "                    Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(512, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(1024, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    GlobalAveragePooling2D(),\n",
    "                    \n",
    "                    Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    ###\n",
    "                    Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(1024,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(2048,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    Dense(1 , activation=\"sigmoid\")\n",
    "                    \n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59dda6",
   "metadata": {},
   "source": [
    "### Compile the model and display the summary\n",
    "\n",
    "\n",
    "Here, we compile the model using `model.compile()` with the `Adam` optimizer and `learning_rate` equal to `lr` (0.001). \n",
    "\n",
    "The `loss` function is specified as `\"binary_crossentropy\"`, appropriate for binary classification problems. \n",
    "`accuracy` is set as the performance `metric` to monitor training and evaluation. \n",
    "We print `model.summary()` for a detailed overview of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7a234a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.909688Z",
     "iopub.status.busy": "2025-10-27T07:48:32.909619Z",
     "iopub.status.idle": "2025-10-27T07:48:32.922384Z",
     "shell.execute_reply": "2025-10-27T07:48:32.921941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,277,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,108,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,277,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │    \u001b[38;5;34m13,108,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,352,897</span> (77.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,352,897\u001b[0m (77.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,340,801</span> (77.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,340,801\u001b[0m (77.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span> (47.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,096\u001b[0m (47.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),\n",
    "              loss=loss, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ace3c-0159-44f9-a386-6e2586f0d90c",
   "metadata": {},
   "source": [
    "Answer the question below in the space provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e78c1-121b-4461-a440-2505bf8faa48",
   "metadata": {},
   "source": [
    "## Question: Count the total number of layers in this CNN model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5420f7c-8738-476c-bea7-db19a38a395e",
   "metadata": {},
   "source": [
    "### You can use this cell to type the answer to the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be625e-e99d-454f-a945-4d788df6578f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbffce73-7f2d-4cd2-8ca3-58f5e6480587",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "There are total 38 layers.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0cf5c-d6b5-4889-be90-0aa6c61be7fc",
   "metadata": {},
   "source": [
    "You now know how to create a CNN model using Keras. You can perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc991a22-1b16-4dc8-add3-cb36d194942c",
   "metadata": {},
   "source": [
    "## Task 3: Create and compile a CNN model `test_model` with 4 Conv2D layers and 5 Dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a84fd320-0dd2-4b37-a223-2f3bb37583ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.923691Z",
     "iopub.status.busy": "2025-10-27T07:48:32.923605Z",
     "iopub.status.idle": "2025-10-27T07:48:32.925032Z",
     "shell.execute_reply": "2025-10-27T07:48:32.924714Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd6c52-9b24-4184-a133-f6c78203260b",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "model = Sequential([\n",
    " Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " GlobalAveragePooling2D(),\n",
    "\n",
    " Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(1 , activation=\"sigmoid\")\n",
    " \n",
    " ])\n",
    "\n",
    "# Compile the model to make it ready for training\n",
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7045b4a",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Display the training configuration and hyperparameters\n",
    "\n",
    "Here we print a comprehensive summary of the training configuration and list all critical hyperparameters. This detailed output serves as a quick reference and verification of the experimental setup.\n",
    "Before commencing computationally intensive tasks such as deep learning model training, it's a good practice to log and verify the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f7cb52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:32.926037Z",
     "iopub.status.busy": "2025-10-27T07:48:32.925975Z",
     "iopub.status.idle": "2025-10-27T07:48:33.082246Z",
     "shell.execute_reply": "2025-10-27T07:48:33.081827Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining Hyperparameters:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33m        n_classes (train) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_generator.num_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[33m        n_classes (validation) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mvalidation_generator\u001b[49m.num_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33m        img_w, img_h =\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_w,\u001b[38;5;250m \u001b[39mimg_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33m        n_channels = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m        batch_size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33m        steps_per_epoch = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33m        n_epochs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33m        validation_steps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33m        learning_rate = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'validation_generator' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Training Hyperparameters:\\n\\\n",
    "        n_classes (train) = {train_generator.num_classes},\\n\\\n",
    "        n_classes (validation) = {validation_generator.num_classes},\\n\\\n",
    "        img_w, img_h ={img_w, img_h},\\n\\\n",
    "        n_channels = {n_channels},\\n\\\n",
    "        batch_size = {batch_size},\\n\\\n",
    "        steps_per_epoch = {steps_per_epoch},\\n\\\n",
    "        n_epochs = {n_epochs},\\n\\\n",
    "        validation_steps = {validation_steps},\\n\\\n",
    "        learning_rate = {lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043d82b-6bfb-4fab-bfc2-69c0d943c491",
   "metadata": {},
   "source": [
    "### Save the model checkpoint\n",
    "\n",
    "Now we declare a method to save the **best model** during training. The best model can be defined by either **lowest loss** or **high accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7030d8-a5fa-4c3d-8da7-e32bf78b0f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.083396Z",
     "iopub.status.busy": "2025-10-27T07:48:33.083327Z",
     "iopub.status.idle": "2025-10-27T07:48:33.084947Z",
     "shell.execute_reply": "2025-10-27T07:48:33.084572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_loss',      # or 'val_accuracy'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6d845-24ce-4492-9b7a-e7b96798bb08",
   "metadata": {},
   "source": [
    "The checkpoint of a model can also be based on high accuracy. So, here's your next task.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed3e8b-5ec0-41df-a76c-4239fff66f94",
   "metadata": {},
   "source": [
    "### **Task 4**: Create the checkpoint callback for model with **maximum accuracy**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e356f45b-0c7d-4403-83f9-16a797cd72fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.085886Z",
     "iopub.status.busy": "2025-10-27T07:48:33.085823Z",
     "iopub.status.idle": "2025-10-27T07:48:33.087228Z",
     "shell.execute_reply": "2025-10-27T07:48:33.086899Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156341a-60a0-4cd6-84ae-3bbe81262cda",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ec0c",
   "metadata": {},
   "source": [
    "### Execute model training\n",
    "\n",
    "- `model.fit()` is the primary function for training a Keras model. It controls the entire training loop: iterating over epochs, fetching data batches from generators, performing forward and backward passes, updating weights via the optimizer, and calculating loss and metrics.\n",
    "- `steps_per_epoch` (*if specified*) determines how many batches constitute an \"epoch.\"\n",
    "- `validation_data` and `validation_steps` allow monitoring of the model's generalization ability on a separate dataset, helps in detecting overfitting.\n",
    "- `callbacks` determines how the best model is saved.\n",
    "- The `fit` object stores the model's training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e84af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.088206Z",
     "iopub.status.busy": "2025-10-27T07:48:33.088132Z",
     "iopub.status.idle": "2025-10-27T07:48:33.098344Z",
     "shell.execute_reply": "2025-10-27T07:48:33.098003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on : ===cpu=== with batch size: 128 & lr: 0.001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'validation_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on : ===\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=== with batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m & lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m fit = model.fit(train_generator, \n\u001b[32m      4\u001b[39m                 epochs= n_epochs,\n\u001b[32m      5\u001b[39m                 steps_per_epoch = steps_per_epoch,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m                 validation_data=(\u001b[43mvalidation_generator\u001b[49m),\n\u001b[32m      7\u001b[39m                 validation_steps = validation_steps,\n\u001b[32m      8\u001b[39m                 callbacks=[checkpoint_cb],\n\u001b[32m      9\u001b[39m                 verbose=\u001b[32m1\u001b[39m\n\u001b[32m     10\u001b[39m                )\n",
      "\u001b[31mNameError\u001b[39m: name 'validation_generator' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "fit = model.fit(train_generator, \n",
    "                epochs= n_epochs,\n",
    "                steps_per_epoch = steps_per_epoch,\n",
    "                validation_data=(validation_generator),\n",
    "                validation_steps = validation_steps,\n",
    "                callbacks=[checkpoint_cb],\n",
    "                verbose=1\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11207c3c-3d98-4294-8f59-4e4eb1456b26",
   "metadata": {},
   "source": [
    "## Download and save the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5d799-0dcc-4fd1-a586-18196feeb756",
   "metadata": {},
   "source": [
    "After the training is completed, you will see `ai_capstone_keras_best_model.model.keras` in the left pane\n",
    "\n",
    "**However**, for your convenience, I have saved a model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**.\n",
    "\n",
    "\n",
    "This is the Keras AI model created by training on the provided dataset for agricultural and non-agricultural land dartaset. This model can now be used for infering un-classified images with the dimensions similar to the training images. \n",
    "\n",
    "- You can also download the your trained model file: `ai_capstone_keras_best_model.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clickinng \"Download\".\n",
    "- You could use this model for the other labs of this capstone project course.\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the model to your local computer.\n",
    "\n",
    "\n",
    "### The trained model file (`ai_capstone_keras_best_model.model.keras` ) in the left pane\n",
    "![Model_Keras_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/NM4wJ1o8G3f0Gv3Ic_cHOQ/Model-Keras-download-screenshot-1-marked.png)\n",
    "\n",
    "\n",
    "### The **download** option\n",
    "![Model_Keras_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/y4ubxvX6OHSWP9KvB-fzHQ/Model-Keras-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095e5f6-4687-4cb4-9263-4a081f133021",
   "metadata": {},
   "source": [
    "## Model evaluation and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7988c04",
   "metadata": {},
   "source": [
    "### Perform a comprehensive model evaluation\n",
    "\n",
    "Here, you will perform a detailed evaluation of the trained model on the validation dataset. You would calculate the necessary prediction `steps` based on the validation data and `batch_size`. Then, you will obtain the true class labels (`y_true`) and generate the model's predictions (`y_pred`) on the validation set. The predicted probabilities are converted to binary class labels using a 0.5 threshold. Finally, you will print the overall `accuracy_score`, to get a  quantitative assessment of the model's performance on unseen data.\n",
    "\n",
    "Model evaluation metrics are essential for understanding a model's generalization ability. `y_true` represents the actual labels, while `y_pred` are the model's predicted labels. For binary classification, probabilities are converted to class labels by thresholding. The **accuracy score** is the proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40c7fe5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.099516Z",
     "iopub.status.busy": "2025-10-27T07:48:33.099454Z",
     "iopub.status.idle": "2025-10-27T07:48:33.110183Z",
     "shell.execute_reply": "2025-10-27T07:48:33.109790Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m steps = \u001b[38;5;28mint\u001b[39m(np.ceil(\u001b[43mvalidation_generator\u001b[49m.samples / validation_generator.batch_size))\n\u001b[32m      2\u001b[39m batch_size = \u001b[38;5;28mint\u001b[39m(validation_generator.batch_size)\n\u001b[32m      4\u001b[39m all_preds = []\n",
      "\u001b[31mNameError\u001b[39m: name 'validation_generator' is not defined"
     ]
    }
   ],
   "source": [
    "steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\n",
    "batch_size = int(validation_generator.batch_size)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for step in range(steps):\n",
    "    # Get one batch data\n",
    "    images, labels = next(validation_generator)\n",
    "    preds = model.predict(images)\n",
    "    preds = (preds > 0.5).astype(int).flatten() \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc11a-0228-47dc-ac46-521cd739e1b5",
   "metadata": {},
   "source": [
    "### Visualize the training history (accuracy and loss)\n",
    "\n",
    "\n",
    "This cell generates two plots to visualize the model's training performance, one for accuracy and one for loss, across epochs. \n",
    "- **Accuracy** measures the proportion of correct predictions. \n",
    "- **Loss** quantifies the error between predictions and true labels. \n",
    "- Using these metrics, we can check the model for **overfitting** or **underfitting**. \n",
    "- `fit.history` attribute stores these metrics for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3e85bc7-ce80-4780-a1ee-522aae09bbb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.111205Z",
     "iopub.status.busy": "2025-10-27T07:48:33.111140Z",
     "iopub.status.idle": "2025-10-27T07:48:33.161683Z",
     "shell.execute_reply": "2025-10-27T07:48:33.161261Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m fig, axs = plt.subplots(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plot Accuracy on the first subplot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m axs.plot(\u001b[43mfit\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTraining Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m axs.plot(fit.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mValidation Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m axs.set_title(\u001b[33m'\u001b[39m\u001b[33mModel Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig, axs = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd999097-1580-43b6-99c5-6ba956cabdbd",
   "metadata": {},
   "source": [
    "Plot the model loss in the task below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ba209-5b62-4686-8353-ec3c6d3f39bd",
   "metadata": {},
   "source": [
    "### **Task 5:** Plot the graph for **training loss** and **validation loss** for the model `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06249605-0d3c-4a36-b53b-9b07924a9e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.162740Z",
     "iopub.status.busy": "2025-10-27T07:48:33.162677Z",
     "iopub.status.idle": "2025-10-27T07:48:33.164168Z",
     "shell.execute_reply": "2025-10-27T07:48:33.163793Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143eeb9-e305-4a39-9598-2bdaf84115c4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(8, 6))\n",
    "\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1a7d-3166-4ded-b9d2-9c46a759e3d6",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce0319-3189-4db3-aeb7-d8ef19734030",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully bulit, trained, and evaluated a deep learning model using Keras for image classification.\n",
    "\n",
    "- **Robust data handling:** We implemented a robust data acquisition strategy, featuring a primary method and a crucial fallback for reliable data downloading and extraction.\n",
    "- **Reproducibility:** We used fixed random seeds ensures your results are consistent and verifiable across multiple runs.\n",
    "- **Data generators:** We learnt about ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** We built a multi-layered CNN, incorporating Conv2D, MaxPooling2D, BatchNormalization, Dropout, and Dense layers for effective feature learning and classification.\n",
    "- **Model compilation:** We configured the model's learning process with an Adam optimizer, binary_crossentropy loss, and accuracy metric.\n",
    "- **Training process:** We executed the training loop, feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** We plotted the accuracy and loss plots for understandinbg the model's learning progress and identify overfitting.\n",
    "- **Model evaluation:** Finally, we use accuracy_score for a quantitative assessment of your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807370f5-c80c-4498-9955-6e15aad4bdbc",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e156172-bf48-4869-b01b-4dda220188a0",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "```\n",
    "```|---|---|---|---|\n",
    "```\n",
    "```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n",
    "```\n",
    "```| 2025-06-30  | 2.0  | Sangeeta |  ID review |\n",
    "```\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1fc1b-0bbd-45e2-9a6a-f06b7b0ced04",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1199644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T07:48:33.165334Z",
     "iopub.status.busy": "2025-10-27T07:48:33.165259Z",
     "iopub.status.idle": "2025-10-27T07:48:33.168998Z",
     "shell.execute_reply": "2025-10-27T07:48:33.168595Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3443369381.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Module 2 Lab 1: Train and Evaluate a Keras-Based ClassifierSolutions for all tasks (12 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L1-Train-and-Evaluate-a-Keras-Based-Classifier-v1.ipynb\"\"\"import osimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.keras import layers, modelsfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpointfrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.initializers import HeUniform# =============================================================================# TASK 1: Walk through dataset_path to create list fnames of all image files# =============================================================================dataset_path = './images_dataSAT'# Walk through the directory treefnames = []for root, dirs, files in os.walk(dataset_path):    for file in files:        if file.endswith(('.jpg', '.jpeg', '.png')):            full_path = os.path.join(root, file)            fnames.append(full_path)print(f\"Total image files found: {len(fnames)}\")print(f\"\\nFirst 5 files:\")for fname in fnames[:5]:    print(f\"  {fname}\")# Alternative one-liner approachfnames_alt = [os.path.join(root, file)              for root, dirs, files in os.walk(dataset_path)              for file in files if file.endswith(('.jpg', '.jpeg', '.png'))]print(f\"\\nVerification: {len(fnames_alt)} files found using alternative method\")# =============================================================================# TASK 2: Create validation_generator with appropriate settings# =============================================================================img_w, img_h = 64, 64batch_size = 128# Define ImageDataGenerator with augmentation and validation splitdatagen = ImageDataGenerator(    rescale=1./255,    rotation_range=40,    width_shift_range=0.2,    height_shift_range=0.2,    shear_range=0.2,    zoom_range=0.2,    horizontal_flip=True,    fill_mode='nearest',    validation_split=0.2  # 20% for validation)# Create training generatortrain_generator = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode='categorical',  # For 2 classes with softmax    subset='training',    shuffle=True)# Create validation generator (TASK 2 ANSWER)validation_generator = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode='categorical',    subset='validation',    shuffle=False  # No shuffling for validation)print(f\"\\nTraining samples: {train_generator.samples}\")print(f\"Validation samples: {validation_generator.samples}\")print(f\"Number of classes: {train_generator.num_classes}\")print(f\"Class indices: {train_generator.class_indices}\")# =============================================================================# TASK 3: Count the total number of CNN model layers# =============================================================================# First, let's build the model (needed for Task 4 anyway)# Then we'll count the layers# CNN Architecture: 4 Conv2D blocks + 5 Dense layersmodel = models.Sequential([    # First Conv2D block    layers.Conv2D(32, (5, 5), activation='relu', kernel_initializer=HeUniform(),                  input_shape=(img_w, img_h, 3), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Second Conv2D block    layers.Conv2D(64, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Third Conv2D block    layers.Conv2D(128, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Fourth Conv2D block    layers.Conv2D(256, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Flatten and Dense layers    layers.Flatten(),    # First Dense layer    layers.Dense(512, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.4),    layers.BatchNormalization(),    # Second Dense layer    layers.Dense(256, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.3),    layers.BatchNormalization(),    # Third Dense layer    layers.Dense(128, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.2),    # Fourth Dense layer    layers.Dense(64, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.2),    # Fifth Dense layer (output layer)    layers.Dense(train_generator.num_classes, activation='softmax')])# TASK 3 ANSWER: Count total layerstotal_layers = len(model.layers)print(f\"\\nTASK 3: Total number of CNN model layers: {total_layers}\")# Display model architectureprint(\"\\nModel Architecture:\")model.summary()# Verify layer countsconv_layers = len([layer for layer in model.layers if isinstance(layer, layers.Conv2D)])dense_layers = len([layer for layer in model.layers if isinstance(layer, layers.Dense)])print(f\"\\nVerification:\")print(f\"  Conv2D layers: {conv_layers} (Expected: 4)\")print(f\"  Dense layers: {dense_layers} (Expected: 5)\")# =============================================================================# TASK 4: Create and compile CNN with 4 Conv2D and 5 Dense layers# (Already done above, but showing compilation here)# =============================================================================# Compile the modelmodel.compile(    optimizer=Adam(learning_rate=0.001),    loss='categorical_crossentropy',    metrics=['accuracy'])print(\"\\nModel compiled successfully!\")print(f\"Optimizer: Adam (lr=0.001)\")print(f\"Loss: categorical_crossentropy\")print(f\"Metrics: accuracy\")# =============================================================================# TASK 5: Define checkpoint callback with max accuracy monitoring# =============================================================================# Define model checkpoint to save best model based on validation accuracycheckpoint_path = 'keras_cnn_best_model.keras'checkpoint_callback = ModelCheckpoint(    filepath=checkpoint_path,    monitor='val_accuracy',      # Monitor validation accuracy    mode='max',                   # Save when validation accuracy increases    save_best_only=True,         # Only save when val_accuracy improves    verbose=1,                    # Print message when saving    save_weights_only=False      # Save complete model)print(f\"\\nCheckpoint callback created:\")print(f\"  Filepath: {checkpoint_path}\")print(f\"  Monitor: val_accuracy\")print(f\"  Mode: max\")print(f\"  Save best only: True\")# =============================================================================# TRAINING THE MODEL# =============================================================================# Train the model (reduced epochs for demonstration)epochs = 5  # Increase to 15-20 for full trainingprint(f\"\\nTraining model for {epochs} epochs...\")print(f\"This may take 20-30 minutes on CPU...\")history = model.fit(    train_generator,    epochs=epochs,    validation_data=validation_generator,    callbacks=[checkpoint_callback],    verbose=1)print(\"\\nTraining completed!\")# =============================================================================# TASK 6: Plot training and validation loss# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot accuracyaxes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Accuracy')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot loss (TASK 6 ANSWER)axes[1].plot(history.history['loss'], label='Training Loss', marker='o', color='coral')axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s', color='dodgerblue')axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Loss')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('keras_cnn_training_history.png', dpi=300, bbox_inches='tight')plt.show()print(\"\\nTraining history plots saved as 'keras_cnn_training_history.png'\")# =============================================================================# BONUS: Evaluate model on validation set# =============================================================================print(\"\\nEvaluating model on validation set...\")val_loss, val_accuracy = model.evaluate(validation_generator, verbose=1)print(f\"\\nFinal Validation Results:\")print(f\"  Loss: {val_loss:.4f}\")print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")# =============================================================================# BONUS: Make predictions and show confusion matrix# =============================================================================from sklearn.metrics import classification_report, confusion_matriximport seaborn as sns# Reset generatorvalidation_generator.reset()# Get predictionspredictions = model.predict(validation_generator, verbose=1)predicted_classes = np.argmax(predictions, axis=1)true_classes = validation_generator.classesclass_labels = list(validation_generator.class_indices.keys())# Confusion matrixcm = confusion_matrix(true_classes, predicted_classes)plt.figure(figsize=(8, 6))sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',            xticklabels=class_labels, yticklabels=class_labels)plt.title('Confusion Matrix - Keras CNN', fontsize=14, fontweight='bold')plt.ylabel('True Label')plt.xlabel('Predicted Label')plt.tight_layout()plt.savefig('keras_cnn_confusion_matrix.png', dpi=300)plt.show()# Classification reportprint(\"\\nClassification Report:\")print(classification_report(true_classes, predicted_classes,                          target_names=class_labels, digits=4))# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 1 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Created fnames list with {len(fnames)} image files\")print(f\"Task 2: Created validation_generator with {validation_generator.samples} samples\")print(f\"Task 3: Total CNN layers = {total_layers}\")print(f\"Task 4: Built CNN with {conv_layers} Conv2D + {dense_layers} Dense layers\")print(f\"Task 5: Created checkpoint callback monitoring val_accuracy (max)\")print(f\"Task 6: Plotted training and validation loss curves\")print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")print(\"=\"*70)\u001b[39m\n                                                                                                                                                                                                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module 2 Lab 1: Train and Evaluate a Keras-Based ClassifierSolutions for all tasks (12 points total)Copy these code blocks into the corresponding cells in the notebook:Lab-M2L1-Train-and-Evaluate-a-Keras-Based-Classifier-v1.ipynb\"\"\"import osimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.keras import layers, modelsfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpointfrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.initializers import HeUniform# =============================================================================# TASK 1: Walk through dataset_path to create list fnames of all image files# =============================================================================dataset_path = './images_dataSAT'# Walk through the directory treefnames = []for root, dirs, files in os.walk(dataset_path):    for file in files:        if file.endswith(('.jpg', '.jpeg', '.png')):            full_path = os.path.join(root, file)            fnames.append(full_path)print(f\"Total image files found: {len(fnames)}\")print(f\"\\nFirst 5 files:\")for fname in fnames[:5]:    print(f\"  {fname}\")# Alternative one-liner approachfnames_alt = [os.path.join(root, file)              for root, dirs, files in os.walk(dataset_path)              for file in files if file.endswith(('.jpg', '.jpeg', '.png'))]print(f\"\\nVerification: {len(fnames_alt)} files found using alternative method\")# =============================================================================# TASK 2: Create validation_generator with appropriate settings# =============================================================================img_w, img_h = 64, 64batch_size = 128# Define ImageDataGenerator with augmentation and validation splitdatagen = ImageDataGenerator(    rescale=1./255,    rotation_range=40,    width_shift_range=0.2,    height_shift_range=0.2,    shear_range=0.2,    zoom_range=0.2,    horizontal_flip=True,    fill_mode='nearest',    validation_split=0.2  # 20% for validation)# Create training generatortrain_generator = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode='categorical',  # For 2 classes with softmax    subset='training',    shuffle=True)# Create validation generator (TASK 2 ANSWER)validation_generator = datagen.flow_from_directory(    dataset_path,    target_size=(img_w, img_h),    batch_size=batch_size,    class_mode='categorical',    subset='validation',    shuffle=False  # No shuffling for validation)print(f\"\\nTraining samples: {train_generator.samples}\")print(f\"Validation samples: {validation_generator.samples}\")print(f\"Number of classes: {train_generator.num_classes}\")print(f\"Class indices: {train_generator.class_indices}\")# =============================================================================# TASK 3: Count the total number of CNN model layers# =============================================================================# First, let's build the model (needed for Task 4 anyway)# Then we'll count the layers# CNN Architecture: 4 Conv2D blocks + 5 Dense layersmodel = models.Sequential([    # First Conv2D block    layers.Conv2D(32, (5, 5), activation='relu', kernel_initializer=HeUniform(),                  input_shape=(img_w, img_h, 3), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Second Conv2D block    layers.Conv2D(64, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Third Conv2D block    layers.Conv2D(128, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Fourth Conv2D block    layers.Conv2D(256, (5, 5), activation='relu', kernel_initializer=HeUniform(), padding='same'),    layers.MaxPooling2D((2, 2)),    layers.BatchNormalization(),    # Flatten and Dense layers    layers.Flatten(),    # First Dense layer    layers.Dense(512, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.4),    layers.BatchNormalization(),    # Second Dense layer    layers.Dense(256, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.3),    layers.BatchNormalization(),    # Third Dense layer    layers.Dense(128, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.2),    # Fourth Dense layer    layers.Dense(64, activation='relu', kernel_initializer=HeUniform()),    layers.Dropout(0.2),    # Fifth Dense layer (output layer)    layers.Dense(train_generator.num_classes, activation='softmax')])# TASK 3 ANSWER: Count total layerstotal_layers = len(model.layers)print(f\"\\nTASK 3: Total number of CNN model layers: {total_layers}\")# Display model architectureprint(\"\\nModel Architecture:\")model.summary()# Verify layer countsconv_layers = len([layer for layer in model.layers if isinstance(layer, layers.Conv2D)])dense_layers = len([layer for layer in model.layers if isinstance(layer, layers.Dense)])print(f\"\\nVerification:\")print(f\"  Conv2D layers: {conv_layers} (Expected: 4)\")print(f\"  Dense layers: {dense_layers} (Expected: 5)\")# =============================================================================# TASK 4: Create and compile CNN with 4 Conv2D and 5 Dense layers# (Already done above, but showing compilation here)# =============================================================================# Compile the modelmodel.compile(    optimizer=Adam(learning_rate=0.001),    loss='categorical_crossentropy',    metrics=['accuracy'])print(\"\\nModel compiled successfully!\")print(f\"Optimizer: Adam (lr=0.001)\")print(f\"Loss: categorical_crossentropy\")print(f\"Metrics: accuracy\")# =============================================================================# TASK 5: Define checkpoint callback with max accuracy monitoring# =============================================================================# Define model checkpoint to save best model based on validation accuracycheckpoint_path = 'keras_cnn_best_model.keras'checkpoint_callback = ModelCheckpoint(    filepath=checkpoint_path,    monitor='val_accuracy',      # Monitor validation accuracy    mode='max',                   # Save when validation accuracy increases    save_best_only=True,         # Only save when val_accuracy improves    verbose=1,                    # Print message when saving    save_weights_only=False      # Save complete model)print(f\"\\nCheckpoint callback created:\")print(f\"  Filepath: {checkpoint_path}\")print(f\"  Monitor: val_accuracy\")print(f\"  Mode: max\")print(f\"  Save best only: True\")# =============================================================================# TRAINING THE MODEL# =============================================================================# Train the model (reduced epochs for demonstration)epochs = 5  # Increase to 15-20 for full trainingprint(f\"\\nTraining model for {epochs} epochs...\")print(f\"This may take 20-30 minutes on CPU...\")history = model.fit(    train_generator,    epochs=epochs,    validation_data=validation_generator,    callbacks=[checkpoint_callback],    verbose=1)print(\"\\nTraining completed!\")# =============================================================================# TASK 6: Plot training and validation loss# =============================================================================fig, axes = plt.subplots(1, 2, figsize=(14, 5))# Plot accuracyaxes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')axes[0].set_xlabel('Epoch')axes[0].set_ylabel('Accuracy')axes[0].legend()axes[0].grid(True, alpha=0.3)# Plot loss (TASK 6 ANSWER)axes[1].plot(history.history['loss'], label='Training Loss', marker='o', color='coral')axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s', color='dodgerblue')axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')axes[1].set_xlabel('Epoch')axes[1].set_ylabel('Loss')axes[1].legend()axes[1].grid(True, alpha=0.3)plt.tight_layout()plt.savefig('keras_cnn_training_history.png', dpi=300, bbox_inches='tight')plt.show()print(\"\\nTraining history plots saved as 'keras_cnn_training_history.png'\")# =============================================================================# BONUS: Evaluate model on validation set# =============================================================================print(\"\\nEvaluating model on validation set...\")val_loss, val_accuracy = model.evaluate(validation_generator, verbose=1)print(f\"\\nFinal Validation Results:\")print(f\"  Loss: {val_loss:.4f}\")print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")# =============================================================================# BONUS: Make predictions and show confusion matrix# =============================================================================from sklearn.metrics import classification_report, confusion_matriximport seaborn as sns# Reset generatorvalidation_generator.reset()# Get predictionspredictions = model.predict(validation_generator, verbose=1)predicted_classes = np.argmax(predictions, axis=1)true_classes = validation_generator.classesclass_labels = list(validation_generator.class_indices.keys())# Confusion matrixcm = confusion_matrix(true_classes, predicted_classes)plt.figure(figsize=(8, 6))sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',            xticklabels=class_labels, yticklabels=class_labels)plt.title('Confusion Matrix - Keras CNN', fontsize=14, fontweight='bold')plt.ylabel('True Label')plt.xlabel('Predicted Label')plt.tight_layout()plt.savefig('keras_cnn_confusion_matrix.png', dpi=300)plt.show()# Classification reportprint(\"\\nClassification Report:\")print(classification_report(true_classes, predicted_classes,                          target_names=class_labels, digits=4))# =============================================================================# SUMMARY OF SOLUTIONS# =============================================================================print(\"\\n\" + \"=\"*70)print(\"SUMMARY: Module 2 Lab 1 - All Tasks Completed\")print(\"=\"*70)print(f\"Task 1: Created fnames list with {len(fnames)} image files\")print(f\"Task 2: Created validation_generator with {validation_generator.samples} samples\")print(f\"Task 3: Total CNN layers = {total_layers}\")print(f\"Task 4: Built CNN with {conv_layers} Conv2D + {dense_layers} Dense layers\")print(f\"Task 5: Created checkpoint callback monitoring val_accuracy (max)\")print(f\"Task 6: Plotted training and validation loss curves\")print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "prev_pub_hash": "fd4028941667565cdca5d946adb49321584044cacb8975f859bc4f258d5b52fa",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "208c1bdeb3d047659202ff8fc3b34786": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a6bf3677c084b60b6ed1527c414cee3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "376ade8ce528489eaa977d21b6cfc213": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "58e6676c931d4d75ab3ebe39212a0da3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6e26d58179fb457abaa57faa9c6d2174": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b2398a1e125a42bfbf77435d19929c57",
       "placeholder": "​",
       "style": "IPY_MODEL_b3bfe435f628405bb1a330db4b10f322",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading images-dataSAT.tar: 100%"
      }
     },
     "6e5f2c13864a4b7081607a684e16f2d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ebb6f1fe3b047299b011b57ccc458f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bbeace62d92040db986032ec0d532534",
        "IPY_MODEL_f58d1b8b2a8845b994114e8b5b111fc5",
        "IPY_MODEL_ee24ffbd04ff4795904d6e1c9383949c"
       ],
       "layout": "IPY_MODEL_b4a817c4b9284fd89e26122b1745a6d3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "74cae356298a4961b2f3d9f3c95190b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76a5ffcfb2c644448a7a69b16784fbae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7db139193860494096f54c929c712618": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a9fe301b12348e1a47d1ab6ab0b0cbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_74cae356298a4961b2f3d9f3c95190b8",
       "max": 20243456.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_76a5ffcfb2c644448a7a69b16784fbae",
       "tabbable": null,
       "tooltip": null,
       "value": 20243456.0
      }
     },
     "9a047560cf69474692b5e15020036f54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6e5f2c13864a4b7081607a684e16f2d0",
       "placeholder": "​",
       "style": "IPY_MODEL_376ade8ce528489eaa977d21b6cfc213",
       "tabbable": null,
       "tooltip": null,
       "value": " 20243456/20243456 [00:03&lt;00:00, 12700231.02it/s]"
      }
     },
     "b2398a1e125a42bfbf77435d19929c57": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3bfe435f628405bb1a330db4b10f322": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b4a817c4b9284fd89e26122b1745a6d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8d09914481247a9ad40b1b641115712": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba440c9d852b4bb4bd792025de1be3e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bbeace62d92040db986032ec0d532534": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_208c1bdeb3d047659202ff8fc3b34786",
       "placeholder": "​",
       "style": "IPY_MODEL_ba440c9d852b4bb4bd792025de1be3e8",
       "tabbable": null,
       "tooltip": null,
       "value": "Extracting images-dataSAT.tar: 100%"
      }
     },
     "c299460a848c47d78e4f4b813e51e30c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cf371c20e55740f1b3a8cc1a94fef3b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6e26d58179fb457abaa57faa9c6d2174",
        "IPY_MODEL_8a9fe301b12348e1a47d1ab6ab0b0cbf",
        "IPY_MODEL_9a047560cf69474692b5e15020036f54"
       ],
       "layout": "IPY_MODEL_7db139193860494096f54c929c712618",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ee24ffbd04ff4795904d6e1c9383949c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a6bf3677c084b60b6ed1527c414cee3",
       "placeholder": "​",
       "style": "IPY_MODEL_c299460a848c47d78e4f4b813e51e30c",
       "tabbable": null,
       "tooltip": null,
       "value": " 6003/6003 [00:00&lt;00:00, 11397.76it/s]"
      }
     },
     "f58d1b8b2a8845b994114e8b5b111fc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8d09914481247a9ad40b1b641115712",
       "max": 6003.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_58e6676c931d4d75ab3ebe39212a0da3",
       "tabbable": null,
       "tooltip": null,
       "value": 6003.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
